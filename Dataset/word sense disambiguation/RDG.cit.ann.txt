R09	Although sense distribution data derived from SemCor can be more accurate than such information derived automatically (McCarthy et al. , 2004), in a given domain there will be words for which the SemCor frequency distributions are inappropriate or unavailable. ::: H05-1053_157:192

	Our final partial tagger is a re-implementation of the algorithm developed by Yarowsky (1992). ::: J01-3001_304:520

M03	14 Dagan, Itai, and Schwall (1991) and Dagan and Itai (1994) propose a similar method, but instead of a parallel corpus use two monolingual corpora and a bilingual dictionary. ::: J98-1001_284:526

M57	Various corpus-based approaches to word sense disambiguation have been proposed (Bruce and Wiebe 1994; Charniak 1993; Dagan and Itai 1994; Fujii et al. 1996; Hearst 1991; Karov and Edelman 1996; Kurohashi and Nagao 1994; Li, Szpakowicz, and Matwin 1995; Ng and Lee 1996; Niwa and Nitta 1994; Sch~itze 1992; Uramoto 1994b; Yarowsky 1995). ::: J98-4002_11:389

	The first five of these seven features were also used in (Ng and Lee, 1996). ::: W97-0323_87:132

	In the results reported in (Bruce and Wiebe, 1994), they used a test set of 600 randomly selected sentences from the 2369 sentences. ::: P96-1006_109:198

	The accuracy figures of LEXAS as reported in (Ng and Lee, 1996) are reproduced in the third row of Table 1. ::: W97-0323_97:132

	However, the feature value pruning method of (Ng and Lee, 1996) only selects surrounding words and local collocations as feature values if they are indicative of some sense class as measured by conditional probability (See (Ng and Lee, 1996) for details). ::: W97-0323_99:132

M02,M25	The work of (Miller et al. , 1994; Leacock et al. , 1993; Yarowsky, 1992) used only the unordered set of surrounding words to perform WSD, and they used statistical classifiers, neural networks, or IR-based techniques. ::: P96-1006_171:198

M02	A supervised algorithm based on this property is given in (Yarowsky, 1994). ::: P95-1026_27:260

E05	The results of disambiguation strategies reported for pseudo words and the like are consistently above 95% overall accuracy, far higher than those reported for disambiguating three or more senses of polysemous words (Wilks et al. 1993; Leacock, Towell, and Voorhees 1993). ::: J98-1006_225:505

	In each of our 100 random trials, the accuracy of LEXAS is always higher than the accuracy of 78% reported in (Bruce and Wiebe, 1994). ::: P96-1006_116:198

	A probabilistic classifier is used in (Bruce and Wiebe, 1994). ::: P96-1006_175:198

R01	One of the first large scale hand tagging efforts is reported in (Miller et al. , 1993), where a subset of the Brown corpus was tagged with WordNet July 2002, pp. ::: W02-0817_22:149

M38	Prior research has shown that using Support Vector Machines (SVM) as the learning algorithm for WSD achieves good results (Lee and Ng, 2002). ::: P07-1005_39:177

	For other related work, see, for example, (Brown et al. 1991; Dagan and Itai 1994; Pedersen and Bruce 1997; Schutze 1998; Kikui 1999; Mihalcea and Moldovan 1999). ::: P02-1044_29:149

	The feature value pruning method of (Ng and Lee, 1996) is intended to keep only feature values deemed important for classification. ::: W97-0323_105:132

R04	This corpus was first reported in (Ng and Lee, 1996), and it contains about 192,800 sense-tagged word occurrences of 191 most frequently occurring and ambiguous words of English. ::: W97-0323_70:132

	In addition to the 11 collocations used in similar work (Lee and Ng, 2002), we also used C3,1, C3,2, C2,3, C1,3. ::: N07-1026_101:191

M17	In Yarowsky's experiment (Yarowsky, 1995), an average of 3936 examples were used to disambiguate between two senses. ::: P97-1009_14:195

E07,P08,R16	Recent estimations of the inter-annotator agreement when using the WordNet inventory report figures of 72.5% agreement in the preparation of the English all-words test set at Senseval-3 (Snyder and Palmer, 2004) and 67.3% on the Open Mind Word Expert annotation exercise (Chklovski and Mihalcea, 2002). ::: W07-2006_47:118

M02	Previous studies e.g., (Lee and Ng, 2002; Florian and Yarowsky, 2002), have applied supervised learning techniques to WSD with success. ::: W06-2911_9:209

M02	In several recent proposals (Hearst, 1991; Bruce and Wiebe, 1994; Leacock, Towwell, and Voorhees, 1996; Ng and Lee, 1996; Yarowsky, 1992; Yarowsky, 1994), statistical and machine learning techniques were used to extract classifiers from hand-tagged corpus. ::: P97-1009_7:195

R15	Like Cowie, Guthrie, and Guthrie (1992), we shall give results at both levels, but it is worth pointing out that the targets of, say, work using translation equivalents (e.g. , Brown et al. 1991; Gale, Church, and Yarowsky 1992c; and see Section 2.3) and Roget categories (Yarowsky 1992; Masterman 1957) correspond broadly to the wider, homograph, distinctions. ::: J01-3001_28:520

M01,M02,M57	The approach combines statistical and knowledge-based methods, but unlike many recent corpus-based approaches to sense disambiguation (arowsky, 1993; Bruce and Wiebe, 1994; Miller et al. , 1994), it takes as its starting point the assumption that sense annotated training text is not available. ::: W97-0209_9:195

M17	Many of the systems mentioned in Table 2 including Yarowsky (1992) do not currently take advantage of the prior probabilities of the senses, so they would be at a disadvantage relative to the baseline if one of the senses had a very high prior, as is the case for the test word issue. ::: P92-1032_137:229

R04	One exception is the sense-tagged data set used in (Bruce and Wiebe, 1994), which has been made available in the public domain by Bruce and Wiebe. ::: P96-1006_100:198

M32,M31	A variety of classifiers have been employed for this task (see Mooney [1996] and Ide and Veronis [1998] for overviews), the most popular being decision lists (Yarowsky 1994, 1995) and naive Bayesian classifiers (Pedersen 2000; Ng 1997; Pedersen and Bruce 1998; Mooney 1996; Cucerzan and Yarowsky 2002). ::: J04-1003_304:472

R01	The work of (Miller et al. , 1994) is the only prior work we know of which attempted to evaluate WSD on a large data set and using the refined sense distinction of WORDNET. ::: P96-1006_183:198

V02,P08,R09,P04	The algorithm was also evaluated on two other data sets, SENSEVAL-3 English all-words data (Snyder and Palmer, 2004) and a subset of SemCor (Miller et al. , 1993), although only fine-grained sense evaluations could be conducted on these test sets. ::: H05-1052_132:160

	This is not surprising, given the observation in (Leacock et al. , 1993) that widely divergent sense-disambiguation algorithms tend to perform roughly the same given the same evidence. ::: P94-1013_205:225

	In order to evaluate the relative contribution of the knowledge sources, including (1) POS and mor43 WSD research Accuracy Black (1988) 72% Zernik (1990) 70% Yarowsky (1992) 72% Bruce & Wiebe (1994) 79% LEXhS (1996) 89% Table 3: Comparison with previous results Knowledge Source POS & morpho surrounding words collocations verb-object Mean Accuracy 77.2% 62.0% 80.2% 43.5% Std Dev 1.44% 1.82% 1.55% 1.79% Table 4: Relative Contribution of Knowledge Sources phological form; (2) unordered set of surrounding words; (3) local collocations; and (4) verb to the left (verb-object syntactic relation), we conducted 4 separate runs of 100 random trials each. ::: P96-1006_121:198

M13,P04	Most recently, Yarowsky used an unsupervised learning procedure to perform WSD (Yarowsky, 1995), although this is only tested on disambiguating words into binary, coarse sense distinction. ::: P96-1006_188:198

M32	The decision-list algorithm used here (Yarowsky, 1994) identifies other collocations that reliably partition the seed training data, ranked by the purity of the distribution. ::: P95-1026_117:260

R09	Also (Miller et al. , 1994) tagging semantically SemCor by hand, measure an error rate around 10% for polysemous words. ::: P97-1007_150:168

M01,R04,M03,M20	They roughly fall into three categories according to what is used for supervision in learning process: (1) using external resources, e.g., thesaurus or lexicons, to disambiguate word senses or automatically generate sense-tagged corpus, (Lesk, 1986; Lin, 1997; McCarthy et al. , 2004; Seo et al. , 2004; Yarowsky, 1992), (2) exploiting the differences between mapping of words to senses in different languages by the use of bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al. , 1991; Dagan and Itai, 1994; Diab and Resnik, 2002; Li and Li, 2004; Ng et al. , 2003), (3) bootstrapping sense tagged seed examples to overcome the bottleneck of acquisition of large sense-tagged data (Hearst, 1991; Karov and Edelman, 1998; Mihalcea, 2004; Park et al. , 2000; Yarowsky, 1995). ::: P05-1049_11:199

	In a comparative paper in this volume \[Leacock et al. 1993\], all three methods under investigation used words in wide context as a pool of evidence independent of relative position. ::: H93-1052_12:195

M57	Various corpus-based approaches to word sense disambiguation have been proposed (Bruce and Wiebe 1994; Charniak 1993; Dagan and Itai 1994; Fujii et al. 1996; Hearst 1991; Karov and Edelman 1996; Kurohashi and Nagao 1994; Li, Szpakowicz, and Matwin 1995; Ng and Lee 1996; Niwa and Nitta 1994; Sch~itze 1992; Uramoto 1994b; Yarowsky 1995). ::: J98-4002_11:389

M38,M21	SVM) (Lee and Ng, 2002) or semi-supervised methods (ex. ::: H05-1114_39:189

R01	Also, the Cognitive Science Laboratory at Princeton has undertaken the hand-tagging of 1,000 words from the Brown Corpus with WordNet senses (Miller et al. 1993) (so far, 200,000 words are available via ftp), and hand-tagging of 25 verbs in a small segment of the Wall Street Journal (12,925 sentences), is also underway (Wiebe et al. 1997). ::: J98-1001_265:526

M02	In contrast, LEXAS uses supervised learning from tagged sentences, which is also the approach taken by most recent work on WSD, including (Bruce and Wiebe, 1994; Miller et al. , 1994; Leacock et al. , 1993; Yarowsky, 1994; Yarowsky, 1993; Yarowsky, 1992). ::: P96-1006_170:198

M13	The best results reported for an unsupervised sense disambiguation method are those of Yarowsky (1992), who uses evidence from a wider context (a window of 100 surrounding words) to build up a co-occurrence model using classes from Roget's thesaurus. ::: W97-0209_99:195

	Compared to (Lee and Ng, 2002), the only difference is that for each training and testing case, we have additional LK LDA features, since there are L bag-of-words and each has a topic distribution represented by K values. ::: D07-1108_101:175

	To evaluate the performance of LEXAS, we conducted two tests, one on a common data set used in (Bruce and Wiebe, 1994), and another on a larger data set that we separately collected. ::: P96-1006_96:198

	Corpus-based Statistical Sense Identification Work on automatic sense identification from the 1950s onward has been well summarized by Hirst (1987) and Dagan and Itai (1994). ::: J98-1006_32:505

R04,R09	Most researchers working on word sense disambiguation (WSD) use manually sense tagged data such as SemCor (Miller et al. , 1993) to train statistical classifiers, but also use the information in SemCor on the overall sense distribution for each word as a backoff model. ::: H05-1053_9:192

	For each ambiguous word in ELS task of SENSEVAL-3, we used three types of features to capture contextual information: part-of-speech of neighboring words with position information, unordered single words in topical context, and local collocations (as same as the feature set used in (Lee and Ng, 2002) except that we did not use syntactic relations). ::: H05-1114_97:189

	In contrast, approaches to WSD attempt to take advantage of many different sources of information (e.g. see (McRoy, 1992; Ng and Lee, 1996; Bruce and Wiebe, 1994)); it seems possible to obtain benefit from sources ranging from local collocational clues (Yarowsky, 1993) to membership in semantically or topically related word classes (arowsky, 1992; Resnik, 1993) to consistency of word usages within a discourse (Gale et al. , 1992); and disambignation seems highly lexically sensitive, in effect requiring specialized disamhignators for each polysemous word. ::: W97-0213_59:184

M02	In contrast, LEXAS uses supervised learning from tagged sentences, which is also the approach taken by most recent work on WSD, including (Bruce and Wiebe, 1994; Miller et al. , 1994; Leacock et al. , 1993; Yarowsky, 1994; Yarowsky, 1993; Yarowsky, 1992). ::: P96-1006_170:198

R09	We used a subset of the SemCor (Miller et al. , 1994) to evaluate our algorithm. ::: P97-1009_137:195

M01,R04,M03,M20	They roughly fall into three categories according to what is used for supervision in learning process: (1) using external resources, e.g., thesaurus or lexicons, to disambiguate word senses or automatically generate sense-tagged corpus, (Lesk, 1986; Lin, 1997; McCarthy et al. , 2004; Seo et al. , 2004; Yarowsky, 1992), (2) exploiting the differences between mapping of words to senses in different languages by the use of bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al. , 1991; Dagan and Itai, 1994; Diab and Resnik, 2002; Li and Li, 2004; Ng et al. , 2003), (3) bootstrapping sense tagged seed examples to overcome the bottleneck of acquisition of large sense-tagged data (Hearst, 1991; Karov and Edelman, 1998; Mihalcea, 2004; Park et al. , 2000; Yarowsky, 1995). ::: P05-1049_11:199

	Bayes (Soft Tag) 68.9 SVM-Topic 66.0 SVM baseline 65.2 NB baseline 63.4 ASO(best configuration)(Ando, 2006) 68.1 Classifier Combination(Florian, 2002) 66.5 Polynomial KPCA(Wu et al. , 2004) 65.8 SVM(Lee and Ng, 2002) 65.4 Senseval-2 Best System 64.2 Table 4: Results (best configuration) compared to previous best systems on Senseval-2 English lexical sample task. ::: D07-1108_134:175

M02	In several recent proposals (Hearst, 1991; Bruce and Wiebe, 1994; Leacock, Towwell, and Voorhees, 1996; Ng and Lee, 1996; Yarowsky, 1992; Yarowsky, 1994), statistical and machine learning techniques were used to extract classifiers from hand-tagged corpus. ::: P97-1009_7:195

P08	To simulate this classifier on SENSEVAL English all-words tasks data (Edmonds and Cotton, 2001; Snyder and Palmer, 2004), we mapped the fine grained senses from official answer keys to their respective beginners. ::: P05-1005_29:185

M04,M03	Regardless of whether it takes the form of dictionaries (Lesk 1986; Guthrie et al. 1991; Dagan, Itai, and Schwall 1991; Karov and Edelman 1996), thesauri (Yarowsky 1992; Walker and Amsler 1986), bilingual corpora (Brown et al. 1991; Church and Gale 1991), or hand-labeled training sets (Hearst 1991; Leacock, Towell, and Voorhees 1993; Niwa and Nitta 1994; Bruce and Wiebe 1994), providing information for sense definitions can be a considerable burden. ::: J98-1004_17:646

M25,M32,M36	Many different learning approaches have been used, including neural networks (Leacock et al. , 1993), probabilistic algorithms (Bruce and Wiebe, 1994; Gale et al. , 1992a; Gale et al. , 1995; Leacock et al. , 1993; Yarowsky, 1992), decision lists (Yarowsky, 1994), exemplar-based learning algorithms (Cardie, 1993; Ng and Lee, 1996), etc. In particular, Mooney (1996) evaluated seven state-of-the-art machine learning algorithms on a common data set for disambiguating six senses of the word "line". ::: W97-0323_6:132

	Although the work of (Yarowsky, i994) can be applied to WSD, the results reported in (Yarowsky, 1994) only dealt with accent restoration, which is a much simpler problem. ::: P96-1006_181:198

M04,M03	Regardless of whether it takes the form of dictionaries (Lesk 1986; Guthrie et al. 1991; Dagan, Itai, and Schwall 1991; Karov and Edelman 1996), thesauri (Yarowsky 1992; Walker and Amsler 1986), bilingual corpora (Brown et al. 1991; Church and Gale 1991), or hand-labeled training sets (Hearst 1991; Leacock, Towell, and Voorhees 1993; Niwa and Nitta 1994; Bruce and Wiebe 1994), providing information for sense definitions can be a considerable burden. ::: J98-1004_17:646

R15	Pragmatic information was used by Yarowsky (1992) whose approach relied upon statistical models of categories from Roget's Thesaurus (Chapman, 1977), a resource that had been used in much earlier approaches to WSD such as Masterman (1957). ::: J01-3001_48:520

M04,M03	Regardless of whether it takes the form of dictionaries (Lesk 1986; Guthrie et al. 1991; Dagan, Itai, and Schwall 1991; Karov and Edelman 1996), thesauri (Yarowsky 1992; Walker and Amsler 1986), bilingual corpora (Brown et al. 1991; Church and Gale 1991), or hand-labeled training sets (Hearst 1991; Leacock, Towell, and Voorhees 1993; Niwa and Nitta 1994; Bruce and Wiebe 1994), providing information for sense definitions can be a considerable burden. ::: J98-1004_17:646

P08,V02,R01	In the English all-words task of SENSEVAL-2 and SENSEVAL3 (Palmer et al., 2001; Snyder and Palmer, 2004), no training data was provided and systems must tag all the content words (noun, verb, adjective, and adverb) in running English texts with their correct WordNet senses. ::: D08-1105_15:155

	In contrast, approaches to WSD attempt to take advantage of many different sources of information (e.g. see (McRoy, 1992; Ng and Lee, 1996; Bruce and Wiebe, 1994)); it seems possible to obtain benefit from sources ranging from local collocational clues (Yarowsky, 1993) to membership in semantically or topically related word classes (arowsky, 1992; Resnik, 1993) to consistency of word usages within a discourse (Gale et al. , 1992); and disambignation seems highly lexically sensitive, in effect requiring specialized disamhignators for each polysemous word. ::: W97-0213_59:184

M02	For the experiments reported in this paper, we follow the supervised learning approach of (Lee and Ng, 2002), by training an individual classifier for each word using the knowledge sources of local collocations, parts-of-speech (POS), and surrounding words. ::: D08-1105_53:155

	Some researchers have concentrated on producing WSD systems that base results on a limited number of words, for example Yarowsky (1995) and Schtitze (1992) who quoted results for 12 words, and a second group, including Leacock, Towell, and Voorhees (1993) and Bruce and Wiebe (1994), who gave results for just one, namely interest. ::: J01-3001_18:520

M02,M21,M13	Many methods have been proposed to deal with this problem, including supervised learning algorithms (Leacock et al. , 1998), semi-supervised learning algorithms (Yarowsky, 1995), and unsupervised learning algorithms (Schutze, 1998). ::: P05-1049_6:199

	For other related work, see, for example, (Brown et al. 1991; Dagan and Itai 1994; Pedersen and Bruce 1997; Schutze 1998; Kikui 1999; Mihalcea and Moldovan 1999). ::: P02-1044_29:149

	Note that the results of MB-D here cannot be directly compared with those in (Yarowsky, 1995), mainly because the data used are different. ::: P02-1044_131:149

M02	In contrast, LEXAS uses supervised learning from tagged sentences, which is also the approach taken by most recent work on WSD, including (Bruce and Wiebe, 1994; Miller et al. , 1994; Leacock et al. , 1993; Yarowsky, 1994; Yarowsky, 1993; Yarowsky, 1992). ::: P96-1006_170:198

	Features We adopt the feature design used by Lee and Ng (2002), which consists of the following four types: (1) Local context: D2-grams of nearby words (position sensitive); (2) Global context: all the words (excluding stopwords) in the given context (position-insensitive; a bag of words); (3) POS: parts-of-speech D2-grams of nearby words; (4) Syn79 tactic relations: syntactic information obtained from parser output. ::: W06-2911_74:209

M02,M01	Some of them have been fully tested in real size texts (e.g. statistical methods (Yarowsky, 1992), (Yarowsky, 1994), (Miller and Teibel, 1991), knowledge based methods (Sussna, 1993), (Agirre and Rigau, 1996), or mixed methods (Richardson et al. , 1994), (Resnik, 1995)). ::: P97-1007_138:168

M04,M03	Regardless of whether it takes the form of dictionaries (Lesk 1986; Guthrie et al. 1991; Dagan, Itai, and Schwall 1991; Karov and Edelman 1996), thesauri (Yarowsky 1992; Walker and Amsler 1986), bilingual corpora (Brown et al. 1991; Church and Gale 1991), or hand-labeled training sets (Hearst 1991; Leacock, Towell, and Voorhees 1993; Niwa and Nitta 1994; Bruce and Wiebe 1994), providing information for sense definitions can be a considerable burden. ::: J98-1004_17:646

M02	In contrast, LEXAS uses supervised learning from tagged sentences, which is also the approach taken by most recent work on WSD, including (Bruce and Wiebe, 1994; Miller et al. , 1994; Leacock et al. , 1993; Yarowsky, 1994; Yarowsky, 1993; Yarowsky, 1992). ::: P96-1006_170:198

	Yarowsky (1992) proposes a method that completely avoids manual tagging of the training corpus. ::: J94-4003_577:730

M36	On the other hand, our past work on WSD (Ng and Lee, 1996) used an exemplar-based (or nearest neighbor) learning approach. ::: W97-0323_12:132

M02	In contrast, LEXAS uses supervised learning from tagged sentences, which is also the approach taken by most recent work on WSD, including (Bruce and Wiebe, 1994; Miller et al. , 1994; Leacock et al. , 1993; Yarowsky, 1994; Yarowsky, 1993; Yarowsky, 1992). ::: P96-1006_170:198

M36,M31	The accuracy achieved by our improved exemplar-based classifier is comparable to the accuracy on the same data set obtained by the Naive-Bayes algorithm, which was reported in (Mooney, 1996) to have the highest disambiguation accuracy among seven state-of-the-art machine learning algorithms. ::: W97-0323_4:132

	However, the work of (Black, 1988; Zernik, 1990; Yarowsky, 1992) were not based on the present set of sentences, so the comparison is only suggestive. ::: P96-1006_118:198

	Local collocations have been found to be the single most informative set of features for WSD (Ng and Lee, 1996). ::: W97-0323_79:132

M57	Various corpus-based approaches to word sense disambiguation have been proposed (Bruce and Wiebe 1994; Charniak 1993; Dagan and Itai 1994; Fujii et al. 1996; Hearst 1991; Karov and Edelman 1996; Kurohashi and Nagao 1994; Li, Szpakowicz, and Matwin 1995; Ng and Lee 1996; Niwa and Nitta 1994; Sch~itze 1992; Uramoto 1994b; Yarowsky 1995). ::: J98-4002_11:389

	Note that the accuracy figures of PEBLS with k = 1 are 1.0% and 1.6% higher than the accuracy figures of (Ng and Lee, 1996) in the third row, also with k = 1. ::: W97-0323_104:132

R04,R09	Among the existing sense-tagged corpora, the SEMCOR corpus (Miller et al., 1994) is one of the most widely used. ::: D08-1105_30:155

M41	It also adopts the heuristics of one sense per discourse (Gale et al. 1992b) to further classify unclassified sentences. ::: P02-1044_27:149

M38	For parts-of-speech, we use 7 features: P3, P2, P1, P0, P1, P2, P3, where P0 is the POS of w, and Pi (Pi) is the POS of the ith token to the left (right) of w. For surrounding words, we consider all unigrams (single words) in the surrounding context of w. These words can be in a different sentence from w. For our experiments reported in this paper, we use support vector machines (SVM) as our learning algorithm, which was shown to achieve good WSD performance in (Lee and Ng, 2002; Chan et al., 2007b). ::: D08-1105_55:155

M25,M32,M36	Many different learning approaches have been used, including neural networks (Leacock et al. , 1993), probabilistic algorithms (Bruce and Wiebe, 1994; Gale et al. , 1992a; Gale et al. , 1995; Leacock et al. , 1993; Yarowsky, 1992), decision lists (Yarowsky, 1994), exemplar-based learning algorithms (Cardie, 1993; Ng and Lee, 1996), etc. In particular, Mooney (1996) evaluated seven state-of-the-art machine learning algorithms on a common data set for disambiguating six senses of the word "line". ::: W97-0323_6:132

	In contrast, approaches to WSD attempt to take advantage of many different sources of information (e.g. see (McRoy, 1992; Ng and Lee, 1996; Bruce and Wiebe, 1994)); it seems possible to obtain benefit from sources ranging from local collocational clues (Yarowsky, 1993) to membership in semantically or topically related word classes (arowsky, 1992; Resnik, 1993) to consistency of word usages within a discourse (Gale et al. , 1992); and disambignation seems highly lexically sensitive, in effect requiring specialized disamhignators for each polysemous word. ::: W97-0213_59:184

M02,R04	Since then, supervised learning from sense-tagged corpora has since been used by several researchers: Zernik (1990, 1991), Hearst (1991), Leacock, Towell, and Voorhees (1993), Gale, Church, and Yarowsky (1992d, 1993), Bruce and Wiebe (1994), Miller et al. ::: J98-1001_259:526

M02,M25	The work of (Miller et al. , 1994; Leacock et al. , 1993; Yarowsky, 1992) used only the unordered set of surrounding words to perform WSD, and they used statistical classifiers, neural networks, or IR-based techniques. ::: P96-1006_171:198

V02	FY02 (Florian and Yarowsky, 2002), WSC04 (Wu et al. , 2004), LN02 (Lee and Ng, 2002) the unseen Senseval-3 test sets). ::: W06-2911_145:209

	Yogi = sw (w,) (5) wiEDef The salient word vector (swv) for a word contains a saliency weight (Yarowsky, 1992) for each of the 25 semantic tags of WordNet. ::: P97-1007_75:168

	Training on WordNet's Monosemous Relatives The approach we have used is related to that of Yarowsky (1992) in that training materials are collected using a knowledge base, but it differs in other respects, notably in the selection of training and testing materials, the choice of a knowledge base, and use of both topical and local classifiers. ::: J98-1006_305:505

	SThe second and third column correspond to the train and test sets used by (Ng and Lee, 1996; Ng, 1997a) Regarding the portability of the systems, very disappointing results are obtained. ::: W00-1322_123:173

M03	Several studies have used parallel texts for WSD (e.g. , Gale et al. , 1993; Dagan et al. , 1991; Dagan and Itai, 1994) as well as to define semantic properties of and relations among lexemes (Dyvik, 1998). ::: W02-0808_10:130

	In fact, Yarowsky (1992) falls below the baseline for one of the twelve words (issue), although perhaps, we needn't be too concerned about this one deviation. ::: P92-1032_123:229

M17	Yarowsky (1995) proposes a method for word sense disambiguation, which is based on Monolingual Bootstrapping. ::: P02-1044_24:149

R03	Stevenson and Wilks (2001) propose a somewhat related technique to handle WSD, based on integrating LDOCE classes with simulated annealing. ::: C04-1133_34:182

	In order to evaluate the relative contribution of the knowledge sources, including (1) POS and mor43 WSD research Accuracy Black (1988) 72% Zernik (1990) 70% Yarowsky (1992) 72% Bruce & Wiebe (1994) 79% LEXhS (1996) 89% Table 3: Comparison with previous results Knowledge Source POS & morpho surrounding words collocations verb-object Mean Accuracy 77.2% 62.0% 80.2% 43.5% Std Dev 1.44% 1.82% 1.55% 1.79% Table 4: Relative Contribution of Knowledge Sources phological form; (2) unordered set of surrounding words; (3) local collocations; and (4) verb to the left (verb-object syntactic relation), we conducted 4 separate runs of 100 random trials each. ::: P96-1006_121:198

R09	The number attached to a node C is the probability P(C) that a randomly selected noun refers to an instance of C. The probabilities are estimated by the frequency of concepts in SemCor (Miller et al. , 1994), a sense-tagged subset of the Brown corpus. ::: P97-1009_91:195

	These include a Bayesian classifier (Gale, Church, and Yarowsky 1993) and a distance 589 Computational Linguistics Volume 20, Number 4 metric between vectors (Schiitze 1993), both inspired from methods in information retrieval; the use of the flip-flop algorithm for ordering possible informants about the preferred sense, trying to maximize the mutual information between the informant and the ambiguous word (Brown et al. 1991); and the use of confidence intervals to establish the degree of confidence in a certain preference, combined with a constraint propagation algorithm (the current paper). ::: J94-4003_626:730

M02	In several recent proposals (Hearst, 1991; Bruce and Wiebe, 1994; Leacock, Towwell, and Voorhees, 1996; Ng and Lee, 1996; Yarowsky, 1992; Yarowsky, 1994), statistical and machine learning techniques were used to extract classifiers from hand-tagged corpus. ::: P97-1009_7:195

	Distance from a target word is used for this purpose and it is calculated by the assumption that the target words in the context window have the same sense (Yarowsky, 1995). ::: C02-1097_74:173

	This parameter is also used by (Ng and Lee, 1996). ::: W02-1006_54:163

M01,M02	Some of them have been fully tested in real size texts (e.g. statistical methods (Yarowsky, 1992), (Yarowsky, 1994), (Miller and Teibel, 1991), knowledge based methods (Sussna, 1993), (Agirre and Rigau, 1996), or mixed methods (Richardson et al. , 1994), (Resnik, 1995)). ::: P97-1007_138:168

M02	Training of WSD Classifier Much research has been done on the best supervised learning approach for WSD (Florian and Yarowsky, 2002; Lee and Ng, 2002; Mihalcea and Moldovan, 2001; Yarowsky et al. , 2001). ::: P03-1058_56:160

P08,V02,R09	The algorithm was also evaluated on two other data sets, SENSEVAL-3 English all-words data (Snyder and Palmer, 2004) and a subset of SemCor (Miller et al. , 1993), although only fine-grained sense evaluations could be conducted on these test sets. ::: H05-1052_132:160

	Also, given the relative importance of the various knowledge sources as reported in (Ng and Lee, 1996), it may be possible to improve disambignation performance by introducing feature weighting. ::: W97-0323_127:132

	The features used in these systems usually include local features, such as part-of-speech (POS) of neighboring words, local collocations, syntactic patterns and global features such as single words in the surrounding context (bag-of-words) (Lee and Ng, 2002). ::: D07-1108_15:175

	Similarly, in our case, each noun is represented by a vector comprising 4 Different types of application of hand-crafted thesauri to word sense disambiguation have been proposed, for example, by Yarowsky (1992). ::: J98-4002_109:389

M25,M32,M36	Many different learning approaches have been used, including neural networks (Leacock et al. , 1993), probabilistic algorithms (Bruce and Wiebe, 1994; Gale et al. , 1992a; Gale et al. , 1995; Leacock et al. , 1993; Yarowsky, 1992), decision lists (Yarowsky, 1994), exemplar-based learning algorithms (Cardie, 1993; Ng and Lee, 1996), etc. In particular, Mooney (1996) evaluated seven state-of-the-art machine learning algorithms on a common data set for disambiguating six senses of the word "line". ::: W97-0323_6:132

	This implementation is exactly the one proposed in (Yarowsky 1995), and we will denote it as MB-D hereafter. ::: P02-1044_87:149

	This was expanded upon by (Gale et al. , 1992), and in a class-based variant by (Yarowsky, 1992). ::: P94-1013_57:225

M41	The well-known observation that words rarely exhibit more than one sense per discourse (Yarowsky, 1995) implies that features closely associated with a particular sense have a low probability of appearing in the same document as features associated with another sense. ::: N10-1088_65:204

	Table 1 shows the performance of Yarowsky (1992) on twelve words which have been previously discussed in the literature. ::: P92-1032_80:229

	Previous uses of this model include language modeling(Lau et al. , 1993), machine translation(Berger et al. , 1996), prepositional phrase attachment(Ratnaparkhi et al. , 1994), and word morphology(Della Pietra et al. , 1995). ::: W96-0213_12:123

M31,M32,M34,M30,M25	They include those using Nave Bayes (Gale et al. 1992a), Decision List (Yarowsky 1994), Nearest Neighbor (Ng and Lee 1996), Transformation Based Learning (Mangu and Brill 1997), Neural Network (Towell and 1 In this paper, we take English-Chinese translation as example; it is a relatively easy process, however, to extend the discussions to translations between other language pairs. ::: P02-1044_17:149

M03,R15	Like Cowie, Guthrie, and Guthrie (1992), we shall give results at both levels, but it is worth pointing out that the targets of, say, work using translation equivalents (e.g. , Brown et al. 1991; Gale, Church, and Yarowsky 1992c; and see Section 2.3) and Roget categories (Yarowsky 1992; Masterman 1957) correspond broadly to the wider, homograph, distinctions. ::: J01-3001_28:520

M02	However, following the work of Yarowsky (1992), Yarowsky (1995), many supervised WSD systems use minimal information about syntactic structures, for the most part restricting the notion of context to topical and local features. ::: C04-1133_24:182

R04	By using a larger value of k, the number of nearest neighbors to use for determining the class of a test example, and through 10-fold cross validation to automatically determine the best k, we have obtained improved disambiguation accuracy on a large sense-tagged corpus first used in (Ng and Lee, 1996). ::: W97-0323_3:132

	Experiment 2: Yarowskys Words We also conducted translation on seven of the twelve English words studied in (Yarowsky, 1995). ::: P02-1044_119:149

M04,M57	They range from dictionary-based approaches that rely on definitions (V~ronis and Ide 1990; Wilks et al. 1993) to corpus-based approaches that use only word cooccurrence frequencies extracted from large textual corpora (Sch~itze 1995; Dagan and Itai 1994). ::: J98-1006_9:505

	In line with existing work (Lee and Ng, 2002; Niu et al. , 2005), we use the following features: Integer 7: seven features consisting of the POS of the previous three words, the POS of the next three words, and the POS of the word itself. ::: N07-1026_97:191

	LEXAS achieves a mean accuracy of 87.4% on this data set, which is higher than the accuracy of 78% reported in (Bruce and Wiebe, 1994). ::: P96-1006_15:198

M57,M02,M21,M13,M01	Many corpus based statistical methods have been proposed to solve this problem, including supervised learning algorithms (Leacock et al. , 1998; Towel and Voorheest, 1998), weakly supervised learning algorithms (Dagan and Itai, 1994; Li and Li, 2004; Mihalcea, 2004; Niu et al. , 2005; Park et al. , 2000; Yarowsky, 1995), unsupervised learning algorithms (or word sense discrimination) (Pedersen and Bruce, 1997; Schutze, 1998), and knowledge based algorithms (Lesk, 1986; McCarthy et al. , 2004). ::: H05-1114_9:189

M03,M02	Although there is some hope from using aligned bilingual corpora as training data for supervised algorithms (Brown et al. , 1991), this approach suffers from both the limited availability of such corpora, and the frequent failure of bilingual translation differences to model monolingual sense differences. ::: P95-1026_230:260

	Following the approach of Dagan and Itai (1994), we use the log of the ratio of the probabilities ln(pl/p2) for this purpose. ::: J98-1006_182:505

M03	A similar observation underlies the use of parallel bilingual corpora for sense disambiguation (Brown et al. 1991; Gale, Church, and Yarowsky 1992). ::: J94-4003_71:730

M13	Unsupervised learning is introduced primarily to deal with the problem, but with limited success (Snyder and Palmer, 2004). ::: D07-1108_31:175

E09	The default strategy of picking the most frequent sense has been advocgted as the baseline performance for evaluating WSD programs (Gale et al. , 1992b; Miller et al. , 1994). ::: W97-0323_90:132

M03,R15	Like Cowie, Guthrie, and Guthrie (1992), we shall give results at both levels, but it is worth pointing out that the targets of, say, work using translation equivalents (e.g. , Brown et al. 1991; Gale, Church, and Yarowsky 1992c; and see Section 2.3) and Roget categories (Yarowsky 1992; Masterman 1957) correspond broadly to the wider, homograph, distinctions. ::: J01-3001_28:520

R01	This should not be too surprising, as it is widely believed that sense tagging using the full set of refined senses found in a large dictionary like WORDNET involve making subtle human judgments (Wilks et al. , 1990; Bruce and Wiebe, 1994), such that there are many genuine cases where two humans will not agree fully on the best sense assignments. ::: P96-1006_146:198

	Syntactic Relations We adopt the same syntactic relations as (Lee and Ng, 2002). ::: D07-1108_56:175

	In addition, Yarowsky (1993) used homophones (e.g. , cellar~seller) and Yarowsky (1994) created homographs by stripping accents from French and Spanish words. ::: J98-1006_223:505

M13	Yarowsky (Yarowsky, 1995) proposed an unsupervised method that used heuristics to obtain seed classifications and expanded the results to the other parts of the corpus, thus avoided the need to hand-annotate any examples. ::: P97-1009_8:195

M38	Our single-task baseline performance is almost the same as LN02 (Lee and Ng, 2002), which uses SVM. ::: W06-2911_147:209

M01,M02	Some of them have been fully tested in real size texts (e.g. statistical methods (Yarowsky, 1992), (Yarowsky, 1994), (Miller and Teibel, 1991), knowledge based methods (Sussna, 1993), (Agirre and Rigau, 1996), or mixed methods (Richardson et al. , 1994), (Resnik, 1995)). ::: P97-1007_138:168

	External information such as the discourse or domain dependency of each word sense (Guthrie et al. 1991; Nasukawa 1993; Yarowsky 1995) is expected to lead to system improvement. ::: J98-4002_180:389

	Yarowsky (1992) used a thesaurus to collect training materials. ::: J98-1006_226:505

M57	Learning to Disambiguate Word Senses Several recent research projects have taken a corpus-based approach to lexical disambiguation (Brown, Della-Pietra, Della-Pietra, & Mercer, 1991; Gale, Church, & Yarowsky, 1992b; Leacock et al. , 1993b; Lehman, 1994). ::: W96-0208_48:170

	The common data set was the interest corpus constructed by Bruce and Wiebe (1994) consisting of 2,639 sentences from the Wall Street Journal, each containing an occurrence of the noun interest. ::: J01-3001_114:520

M01,R04,M03,M20	They roughly fall into three categories according to what is used for supervision in learning process: (1) using external resources, e.g., thesaurus or lexicons, to disambiguate word senses or automatically generate sense-tagged corpus, (Lesk, 1986; Lin, 1997; McCarthy et al. , 2004; Seo et al. , 2004; Yarowsky, 1992), (2) exploiting the differences between mapping of words to senses in different languages by the use of bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al. , 1991; Dagan and Itai, 1994; Diab and Resnik, 2002; Li and Li, 2004; Ng et al. , 2003), (3) bootstrapping sense tagged seed examples to overcome the bottleneck of acquisition of large sense-tagged data (Hearst, 1991; Karov and Edelman, 1998; Mihalcea, 2004; Park et al. , 2000; Yarowsky, 1995). ::: P05-1049_11:199

V02,P08,R01	State-of-the-art systems attained a disambiguation accuracy around 65% in the Senseval-3 all-words task (Snyder and Palmer, 2004), where WordNet (Fellbaum, 1998) was adopted as a reference sense inventory. ::: W07-2006_7:118

	Fortunately, we have found in (Gale et al. , 1992) that the agreement rate can be very high (96.8%), which is well above the baseline, under very different experimental conditions. ::: P92-1032_175:229

M02,R04	Since then, supervised learning from sense-tagged corpora has since been used by several researchers: Zernik (1990, 1991), Hearst (1991), Leacock, Towell, and Voorhees (1993), Gale, Church, and Yarowsky (1992d, 1993), Bruce and Wiebe (1994), Miller et al. ::: J98-1001_259:526

	Both test sets are identical to the ones reported in (Ng and Lee, 1996). ::: W97-0323_77:132

M03	Dagan and Itai (1994) have proposed a method using co-occurrence statistics in independent monolingual corpora of two languages to guide lexical choice in machine translation. ::: P95-1026_235:260

M13,M32	Yarowsky (1995) dealt with this problem largely by producing an unsupervised learning algorithm that generates probabilistic decision list models of word senses from seed collocates. ::: J01-3001_100:520

M41,M20	It has been shown that one sense per discourse property can improve the performance of bootstrapping algorithm (Li and Li, 2004; Yarowsky, 1995). ::: P05-1049_194:199

	This view is reflected also in their elaborate method for target word selection (Brown et al. 1991), in which better estimates of translation probabilities are achieved as a result of word sense disambiguation. ::: J94-4003_696:730

	Ng and Lee (1996) explored an approach to WSD in which a word is assigned the sense of the most similar example already seen. ::: J01-3001_103:520

M31	Gale, Church and Yarowsky (Gale et al. , 1992a; Gale et al. , 1995; Yarowsky, 1992) have also successfully used the Naive-Bayes algorithm (and several extensions and variations) for word sense disambiguation. ::: W97-0323_11:132

M02	However, following the work of Yarowsky (1992), Yarowsky (1995), many supervised WSD systems use minimal information about syntactic structures, for the most part restricting the notion of context to topical and local features. ::: C04-1133_24:182

M57	Iterating between these two 1 Note that these problems are associated with corpus-based approaches in general, and have been identified by a number of researchers (Engelson and Dagan 1996; Lewis and Gale 1994; Uramoto 1994a; Yarowsky 1995). ::: J98-4002_34:389

M55	More important is information beyond selectional preference, notably the wider context utilized by Yarowsky (1992). ::: W97-0209_112:195

	(Miller et al. , 1994), we compare 44 Test set BC50 WSJ6 Sense 1 40.5% 44.8% Most Frequent LEXAS 47.1% 54.0% 63.7% 68.6% Table 5: Evaluation on a Large Data Set a subset of the occurrences that overlap. ::: P96-1006_144:198

M34	Note that the nearest neighbor algorithm tested in (Mooney, 1996) uses Hamming distance as the distance metric between two symbolic feature values. ::: W97-0323_38:132

	Our past work (Ng and Lee, 1996) suggests that multiple sources of knowledge are indeed useful for WSD. ::: W97-0323_123:132

	In addition to the results reported by Yarowsky (1994) and Mooney and Califf (1995), it provides evidence for the utility of this representation for natural-language problems. ::: W96-0208_151:170

R19,M55	Stevenson and Wilks (2001) presented a classifier combination framework where 3 disambiguation methods (simulated annealing, subject codes and selectional restrictions) were combined using the TiMBL memory-based approach (Daelemans et al. , 1999). ::: W02-1004_15:156

M25	The work of (Miller et al. , 1994; Leacock et al. , 1993; Yarowsky, 1992) used only the unordered set of surrounding words to perform WSD, and they used statistical classifiers, neural networks, or IR-based techniques. ::: P96-1006_171:198

M01,R04,M03,M20	They roughly fall into three categories according to what is used for supervision in learning process: (1) using external resources, e.g., thesaurus or lexicons, to disambiguate word senses or automatically generate sense-tagged corpus, (Lesk, 1986; Lin, 1997; McCarthy et al. , 2004; Seo et al. , 2004; Yarowsky, 1992), (2) exploiting the differences between mapping of words to senses in different languages by the use of bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al. , 1991; Dagan and Itai, 1994; Diab and Resnik, 2002; Li and Li, 2004; Ng et al. , 2003), (3) bootstrapping sense tagged seed examples to overcome the bottleneck of acquisition of large sense-tagged data (Hearst, 1991; Karov and Edelman, 1998; Mihalcea, 2004; Park et al. , 2000; Yarowsky, 1995). ::: P05-1049_11:199

M38,V02	For SVM, we did not perform feature selection on SENSEVAL-3 data since feature selection deteriorates its performance (Lee and Ng, 2002). ::: P05-1049_92:199

	To represent this knowledge source of local collocations, we extracted 11 features corresponding to the following collocations: a52 a4a14a13a59a53a58a4a14a13, a52 a13a59a53a58a13, a52 a4a7a12a57a53a58a4a7a12, a52 a12a57a53a12, a52 a4a7a12a57a53a58a4a14a13, a52 a4a14a13a59a53a58a13, a52 a13a59a53a12, a52 a4a7a6a57a53a58a4a14a13, a52 a4a7a12a57a53a58a13, a52 a4a14a13a59a53a12, and a52 a13a59a53a6 . This set of 11 features is the union of the collocation features used in Ng and Lee (1996) and Ng (1997). ::: W02-1006_59:163

	To evaluate our WSD program, named LEXAS (LEXical Ambiguity-resolving _System), we tested it on a common data set involving the noun "interest" used by Bruce and Wiebe (Bruce and Wiebe, 1994). ::: P96-1006_14:198

M36,M31	The accuracy achieved by our improved exemplar-based classifier is comparable to the accuracy on the same data set obtained by the Naive-Bayes algorithm, which was reported in (Mooney, 1996) to have the highest disambiguation accuracy among seven stateof-the-art machine learning algorithms. ::: W97-0323_20:132

	Proceedings of the Conference on Empirical Methods in Natural learning algorithms (Mooney, 1996; Pedersen and Bruce, 1997) tend to base their comparison on only one word or at most a dozen words. ::: W02-1006_26:163

	Ng and Lee (1996) reported the relative contribution of different knowledge sources, but on only one word interest. ::: W02-1006_33:163

	We believe that the chances for such a high accuracy in a coarse-grained sense classifier is better, for several reasons: previously reported good performance for coarse grained systems (Yarowsky, 1992) better availability of data, due to the possibility of reusing data created for different words. ::: P05-1005_38:185

M02	In several recent proposals (Hearst, 1991; Bruce and Wiebe, 1994; Leacock, Towwell, and Voorhees, 1996; Ng and Lee, 1996; Yarowsky, 1992; Yarowsky, 1994), statistical and machine learning techniques were used to extract classifiers from hand-tagged corpus. ::: P97-1009_7:195

	However, all possible feature values (collocated words) are used, without employing the feature value pruning method used in (Ng and Lee, 1996). ::: W97-0323_103:132

	We also include verb-object syntactic relation as a feature, which is not used in (Yarowsky, 1994). ::: P96-1006_180:198

	Some researchers have concentrated on producing WSD systems that base results on a limited number of words, for example Yarowsky (1995) and Schtitze (1992) who quoted results for 12 words, and a second group, including Leacock, Towell, and Voorhees (1993) and Bruce and Wiebe (1994), who gave results for just one, namely interest. ::: J01-3001_18:520

	The high accuracy of the LEXAS system (Ng and Lee, 1996) is due in part to the use of large corpora. ::: W02-0817_30:149

M25,M32,M36	Many different learning approaches have been used, including neural networks (Leacock et al. , 1993), probabilistic algorithms (Bruce and Wiebe, 1994; Gale et al. , 1992a; Gale et al. , 1995; Leacock et al. , 1993; Yarowsky, 1992), decision lists (Yarowsky, 1994), exemplar-based learning algorithms (Cardie, 1993; Ng and Lee, 1996), etc. In particular, Mooney (1996) evaluated seven state-of-the-art machine learning algorithms on a common data set for disambiguating six senses of the word "line". ::: W97-0323_6:132

	In fact, both Black (1988) and Yarowsky (1992) report 72% performance on this very same word. ::: P92-1032_103:229

M04,R01	Most WSD systems have relied on hand-labeled training examples (Leroy and Rindflesch, 2004; Joshi et al., 2005; Mohammad and Pedersen, 2004) or on dictionary glosses (Lesk, 1986; Stevenson and Wilks, 2001) or the WordNet hierarchy (BoydGraber et al., 2007) to help make disambiguation choices. ::: N10-1088_34:204

M31	Despite its simplicity, Naive Bayes is claimed to obtain state-of-theart accuracy on supervised WSD in many papers (Mooney, 1996; Ng, 1997a; Leacock et al. , 1998). ::: W00-1322_43:173

R15	Yarowsky (1992) inputs a 100-word context surrounding a polysemous word and scores each of the 1042 Roget Categories by: 1-\[ Pr(wlRoget Categoryi) w in context The program can also be run in a mode where it takes unrestricted text as input and tags each word with its most likely Roget Category. ::: P92-1032_65:229

R15	Yarowsky (1992) used Rogets Thesaurus categories as classes for word senses. ::: P05-1005_62:185

	This corpus was collected by Ng and colleagues (Ng and Lee, 1996) and it is available from the Linguistic Data Consortium (LDC) 5. ::: W00-1322_76:173

R15	Roget's International Thesaurus, which was put into machine-tractable form in the 1950s and has been used in a variety of applications including machine translation (Masterman 1957), information retrieval (Sparck-Jones 1964, 1986), and content analysis (Sedelow and Sedelow \[1969\], see also Sedelow and Sedelow \[1986, 1992\]), also supplies an explicit concept hierarchy consisting of up to eight increasingly refined levels/Typically, each occurrence of the same word under different categories of the thesaurus represents different senses of that word; i.e., the categories correspond roughly to word senses (Yarowsky 1992). ::: J98-1001_166:526

R09,V02	For training and testing, we used publicly available data sets, namely SEMCOR corpus (Miller et al. , 1993) and SENSEVAL English all-words task data. ::: P05-1005_66:185

M02,M31	Achieving higher precision in supervised word sense disambiguation (WSD) tasks without resorting to ad hoc voting or similar ensemble techniques has become somewhat daunting in recent years, given the challenging benchmarks set by nave Bayes models (e.g. , Mooney (1996), Chodorow et al. ::: P04-1081_5:119

	This way of creating classified data is similar to that in (Yarowsky, 1995). ::: P02-1044_99:149

	Yarowsky (1994), building on his earlier work, designed a classifier that looks at words within :kk positions from the target; lemma forms are obtained through morphological analysis; and a coarse part-of-speech assignment is performed by dictionary lookup. ::: J98-1006_58:505

M25,M32,M36	Many different learning approaches have been used, including neural networks (Leacock et al. , 1993), probabilistic algorithms (Bruce and Wiebe, 1994; Gale et al. , 1992a; Gale et al. , 1995; Leacock et al. , 1993; Yarowsky, 1992), decision lists (Yarowsky, 1994), exemplar-based learning algorithms (Cardie, 1993; Ng and Lee, 1996), etc. In particular, Mooney (1996) evaluated seven state-of-the-art machine learning algorithms on a common data set for disambiguating six senses of the word "line". ::: W97-0323_6:132

	In order to compare our results with those reported in Yarowsky (1992), we trained and tested on the same two senses of the noun duty that Yarowsky had tested ('obligation' and 'tax'). ::: J98-1006_347:505

M01	Stevenson and Wilks (2001) investigated the interaction of knowledge sources, such as part-of-speech, dictionary definition, subject codes, etc. on WSD. ::: W02-1006_34:163

R15	I This estimation method is similar to that used by Yarowsky (1992) for Roget's thesaurus categories, and works for similar reasons. ::: W97-0209_35:195

	The method we use to extract the keywords for each sense is based on the work of Ng and Lee (1996). ::: W04-0827_57:122

V02,P08,E09,R09	Indeed, only 5 out of the 26 systems in the recent SENSEVAL-3 English all words task (Snyder and Palmer, 2004) outperformed the heuristic of choosing the most frequent sense as derived from SemCor (which would give 61.5% precision and recall1). ::: H05-1053_11:192

M57,M02,M21,M13,M01		Many corpus based statistical methods have been proposed to solve this problem, including supervised learning algorithms (Leacock et al. , 1998; Towel and Voorheest, 1998), weakly supervised learning algorithms (Dagan and Itai, 1994; Li and Li, 2004; Mihalcea, 2004; Niu et al. , 2005; Park et al. , 2000; Yarowsky, 1995), unsupervised learning algorithms (or word sense discrimination) (Pedersen and Bruce, 1997; Schutze, 1998), and knowledge based algorithms (Lesk, 1986; McCarthy et al. , 2004). ::: H05-1114_9:189

	More recent work \[Brown et al. 1991\]\[Hearst 1991\] has utilized a set of discrete local questions (such as word-to-the-right) in the development of statistical decision procedures. ::: H93-1052_9:195

M31,M32,M34,M30,M25	They include those using Nave Bayes (Gale et al. 1992a), Decision List (Yarowsky 1994), Nearest Neighbor (Ng and Lee 1996), Transformation Based Learning (Mangu and Brill 1997), Neural Network (Towell and 1 In this paper, we take English-Chinese translation as example; it is a relatively easy process, however, to extend the discussions to translations between other language pairs. ::: P02-1044_17:149

	This consisted of 192,800 occurrences of the 121 nouns and 70 verbs that are "the most frequently occurring and ambiguous words in English" (Ng and Lee 1996, 44). ::: J01-3001_119:520

M36	Previous work on memory-based WSD includes work from Ng and Lee (1996), Veenstra et al. ::: W04-0827_18:122

M32	A number of effective concept-learning systems have employed decision lists (Clark 84 Niblett, 1989; Quinlan, 1993; Mooney & Califf, 1995) and they have already been successfully applied to lexical disambiguation (Yarowsky, 1994). ::: W96-0208_91:170

	The work of (Yarowsky, 1994) is perhaps the most similar to our present work. ::: P96-1006_177:198

	Yarowsky has proposed an algorithm that requires as little user input as one seed word per sense to start the training process (Yarowsky 1995). ::: J98-1004_427:646

R04,R01	Test and training materials were derived from the Brown corpus of American English, all of which has been parsed and manually verified by the Penn T~eebank project (Marcus et al. , 1993) and parts of which have been manually sense-tagged by the WordNet group (Miller et al. , 1993). ::: W97-0209_55:195

	Dagan and Itai (1994) also describe a way to make the threshold dynamic so that it adjusts for the amount of evidence used to estimate pl and p2. ::: J98-1006_185:505

M31,M32,M34,M30,M25	They include those using Nave Bayes (Gale et al. 1992a), Decision List (Yarowsky 1994), Nearest Neighbor (Ng and Lee 1996), Transformation Based Learning (Mangu and Brill 1997), Neural Network (Towell and 1 In this paper, we take English-Chinese translation as example; it is a relatively easy process, however, to extend the discussions to translations between other language pairs. ::: P02-1044_17:149

	Previous work on using the unordered set of surrounding words have used a much larger window, such as the 100-word window of (Yarowsky, 1992), and the 2-sentence context of (Leacock et al. , 1993). ::: P96-1006_126:198

R01,R09	The senses in WordNet are ordered according to the frequency data in the manually tagged resource SemCor (Miller et al. , 1993). ::: P04-1036_10:201

	Previous work on using the unordered set of surrounding words have used a much larger window, such as the 100-word window of (Yarowsky, 1992), and the 2-sentence context of (Leacock et al. , 1993). ::: P96-1006_126:198

M38,M20,M02,M21	Comparison between SVM, Bootstrapping and LP For WSD, SVM is one of the state of the art supervised learning algorithms (Mihalcea et al. , 2004), while bootstrapping is one of the state of the art semi-supervised learning algorithms (Li and Li, 2004; Yarowsky, 1995). ::: P05-1049_56:199

	Local Collocations Collocation Ci,j refers to the ordered sequence of tokens (words or punctuations) surrounding w. The starting and ending position of the sequence are denoted i and j respectively, where a negative value refers to the token position prior to w. We adopt the same 11 collocation features as (Lee and Ng, 2002), namely C1,1, C1,1, C2,2, C2,2, C2,1, C1,1, C1,2, C3,1, C2,1, C1,2, and C1,3. ::: D07-1108_52:175

	Our implemented WSD classifier uses the knowledge sources of local collocations, parts-of-speech (POS), and surrounding words, following the successful approach of (Lee and Ng, 2002). ::: P07-1005_41:177

R03	The next significant hand tagging task was reported in (Bruce and Wiebe, 1994), where 2,476 usages of interest were manually assigned with sense tags from the Longman Dictionary of Contemporary English (LDOCE). ::: W02-0817_28:149

	Compared to previous results on fine-grained evaluation exercises (Edmonds and Kilgarriff, 2002; Snyder and Palmer, 2004), the systems??results are much higher. ::: W07-2006_65:118

R09	Notice that our calculation of the MFS is based on the frequencies in the SemCor corpus (Miller et al. , 1993), as we exploit WordNet sense rankings. ::: W07-2006_58:118

	Gale et al. and Yarowsky use words that appear within 50 words in each 14 The reader is referred to some of these recent papers for thorough surveys of work on sense disambiguation (Hearst 1991; Gale, Church, and Yarowsky 1992a; Yarowsky 1992). ::: J94-4003_522:730

	(Yarowsky, 1995) reports a success rate of 96% disambiguating twelve words with two clear sense distinctions each one). ::: P97-1007_140:168

M01,M57,R04	The approach combines statistical and knowledge-based methods, but unlike many recent corpus-based approaches to sense disambiguation (arowsky, 1993; Bruce and Wiebe, 1994; Miller et al. , 1994), it takes as its starting point the assumption that sense annotated training text is not available. ::: W97-0209_9:195

M31	We believe that this result is significant, in light of the fact that Naive-Bayes has been found to give the best performance for WSD among seven state-ofthe-art machine learning algorithms (Mooney, 1996). ::: W97-0323_118:132

M41	Furthermore, it is not possible to apply the powerful "one sense per discourse" property (Yarowsky, 1995) because there is no discourse in dictionaries. ::: P97-1007_144:168

R03,R15	These differences between the structures of LDOCE and Roget meant that we had to adapt the original algorithm reported in Yarowsky (1992). ::: J01-3001_310:520

	A second type of information is provided by words that occur in the global context of the ambiguous word (Gale, Church, and Yarowsky 1992b, 1993; Yarowsky 1992; Sch6tze 1992). ::: J94-4003_521:730

R15	Recent WSD work has exploited topical context: Yarowsky (1992) uses a 100-word window, both to derive classes of related words and as context surrounding the polysemous target, in his experiments using Roget's Thesaurus (see Section 2.3.2). ::: J98-1001_385:526

	The already described set of attributes contains those attributes used in (Ng and Lee, 1996), with the exception of the morphology of the target word and the verb-object syntactic relation. ::: W00-1322_91:173

M25,M32,M36	Many different learning approaches have been used, including neural networks (Leacock et al. , 1993), probabilistic algorithms (Bruce and Wiebe, 1994; Gale et al. , 1992a; Gale et al. , 1995; Leacock et al. , 1993; Yarowsky, 1992), decision lists (Yarowsky, 1994), exemplar-based learning algorithms (Cardie, 1993; Ng and Lee, 1996), etc. In particular, Mooney (1996) evaluated seven state-of-the-art machine learning algorithms on a common data set for disambiguating six senses of the word "line". ::: W97-0323_6:132

M01,R04,M03,M20	They roughly fall into three categories according to what is used for supervision in learning process: (1) using external resources, e.g., thesaurus or lexicons, to disambiguate word senses or automatically generate sense-tagged corpus, (Lesk, 1986; Lin, 1997; McCarthy et al. , 2004; Seo et al. , 2004; Yarowsky, 1992), (2) exploiting the differences between mapping of words to senses in different languages by the use of bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al. , 1991; Dagan and Itai, 1994; Diab and Resnik, 2002; Li and Li, 2004; Ng et al. , 2003), (3) bootstrapping sense tagged seed examples to overcome the bottleneck of acquisition of large sense-tagged data (Hearst, 1991; Karov and Edelman, 1998; Mihalcea, 2004; Park et al. , 2000; Yarowsky, 1995). ::: P05-1049_11:199

R09,R01	We are therefore using (1) SemCor (Miller et al. , 1993) a balanced, semantically annotated dataset, with all content words manually tagged by trained lexicographers to learn a seAs sociation for Computational Linguistics for the Semantic Analysis of Text, Barcelona, Spain, July 2004 SENSEVAL-3: Third International Workshop on the Evaluation of Systems mantic language model for the words seen in the training corpus; and (2) information drawn from WordNet (Miller, 1995), to derive semantic generalizations for those words that did not appear in the annotated corpus. ::: W04-0838_19:94

R04	By using 10-fold cross validation (Kohavi and 208 John, 1995) on the training set to automatically determine the best k to use, we have obtained improved disambiguation accuracy on a large sense tagged corpus first used in (Ng and Lee, 1996). ::: W97-0323_19:132

	In this paper, we used the WSD program reported in (Lee and Ng, 2002). ::: P03-1058_57:160

M57	Various corpus-based approaches to word sense disambiguation have been proposed (Bruce and Wiebe 1994; Charniak 1993; Dagan and Itai 1994; Fujii et al. 1996; Hearst 1991; Karov and Edelman 1996; Kurohashi and Nagao 1994; Li, Szpakowicz, and Matwin 1995; Ng and Lee 1996; Niwa and Nitta 1994; Sch~itze 1992; Uramoto 1994b; Yarowsky 1995). ::: J98-4002_11:389

M22	Classifier combination has been extensively studied in the last decade, and has been shown to be successful in improving the performance of diverse NLP applications, including POS tagging (Brill and Wu, 1998; van Halteren et al. , 2001), base noun phrase chunking (Sang et al. , 2000), parsing (Henderson and Brill, 1999) and word sense disambiguation (Kilgarriff and Rosenzweig, 2000; Stevenson and Wilks, 2001). ::: W02-1004_5:156

	Further details may be found in Yarowsky (1992). ::: J01-3001_324:520

M31	When more than one verb sense is selected for any given method (or none of them remains, for the rule-based method), the system simply selects the verb sense that appears most frequently in the database, s In the experiment, we conducted sixfold cross-validation, that is, we divided the training/test data into six equal parts, and conducted six trials in which a different 7 A number of experimental results have shown the effectiveness of the Naive-Bayes method for word sense disambiguation (Gale, Church, and Yarowsky 1993; Leacock, Towell, and Voorhees 1993; Mooney 1996; Ng 1997; Pedersen, Bruce, and Wiebe 1997). ::: J98-4002_154:389

	Yarowsky (1995) has proposed automatically augmenting a small set of experimenter-supplied seed collocations (e.g. , manufacturing plant and plant life for two different senses of the noun plant) into a much larger set of training materials. ::: J98-1006_246:505

	Bruce and Wiebe also performed a separate test by using a subset of the "interest" data set with only 4 senses (sense 1, 4, 5, and 6), so as to compare their results with previous work on WSD (Black, 1988; Zernik, 1990; Yarowsky, 1992), which were tested on 4 senses of the noun "interest". ::: P96-1006_117:198

M38	In the SVM (Vapnik, 1995) approach, we first form a training and a testing file using all standard features for each sense following (Lee and Ng, 2002) (one classifier per sense). ::: D07-1108_98:175

M57,M02,M21,M13,M01	Many corpus based statistical methods have been proposed to solve this problem, including supervised learning algorithms (Leacock et al. , 1998; Towel and Voorheest, 1998), weakly supervised learning algorithms (Dagan and Itai, 1994; Li and Li, 2004; Mihalcea, 2004; Niu et al. , 2005; Park et al. , 2000; Yarowsky, 1995), unsupervised learning algorithms (or word sense discrimination) (Pedersen and Bruce, 1997; Schutze, 1998), and knowledge based algorithms (Lesk, 1986; McCarthy et al. , 2004). ::: H05-1114_9:189

R15	Yarowsky (1992) derives classes of words by starting with words in common categories in Roget's (4th edition). ::: J98-1001_174:526

	The work of (Bruce and Wiebe, 1994) used parts of speech (POS) and morphological form, in addition to surrounding words. ::: P96-1006_172:198

M03	Many WSD studies have incorporated this cross-lingual evidence idea and have successfully applied bilingual WSD classifiers (Gale and Church, 1993; Ng et al., 2003; Diab and Resnik, 2002) or systems that use a combination of existing WordNets with multilingual evidence (Tufis et al., 2004). ::: W11-1006_39:147

	Yarowsky (1992) reports that his algorithm achieves 92% correct disambiguation, which is nearly 13% higher than achieved in our implementation. ::: J01-3001_478:520

M31	Section 4 presents the disambiguation accuracy of PEBLS and Naive-Bayes on the large corpus of (Ng and Lee, 1996). ::: W97-0323_24:132

M20	Yarowsky (1995) proposes a method for word sense (translation) disambiguation that is based on a bootstrapping technique, which we refer to here as Monolingual Bootstrapping (MB). ::: P02-1044_9:149

M02	Currently, machine learning methods (Yarowsky 1995; Rigau, Atserias, and Agirre 1997) and combinations of classifiers (McRoy 1992) have been popular. ::: J01-3001_15:520

M02,V02,M31,M33,M38	Supervised feature selection improves the performance of an examplar based learning algorithm over SENSEVAL2 data (Mihalcea, 2002), Naive Bayes and decision tree over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002), but feature selection does not improve SVM and Adaboost over SENSEVAL-1 and SENSEVAL-2 data (Lee and Ng, 2002) for word sense disambiguation. ::: H05-1114_14:189

M57	Despite making such an assumption, this proves to be among the most accurate techniques in comparative studies of corpus-based word sense disambiguation methodologies (e.g. , (Leacock et al. , 1993), (Mooney, 1996), (Ng and Lee, 1996), (Pealersen and Bruce, 1997)). ::: A00-2009_10:165

	The line data was studied again by (Mooney, 1996), where seven different machine learning methodologies are compared. ::: A00-2009_106:165

M31	A similar finding has emerged in word sense disambiguation, where a number of comparative studies have all reported that no method achieves significantly greater accuracy than the Naive Bayesian classifier (e.g. , (Leacock et al. , 1993), (Mooney, 1996), (Ng and Lee, 1996), (Pedersen and Bruce, 1997)). ::: A00-2009_122:165

	For example, (Ng and Lee, 1996) report that local collocations alone achieve 80% accuracy disambiguating interest, while their full set of features result in 87%. ::: A00-2009_130:165

	(Ng and Lee, 1996)), shallow lexical features such as co-occurrences and collocations prove to be stronger contributors to accuracy than do deeper, linguistically motivated features such as part-of-speech and verb-object relationships. ::: A00-2009_13:165

	The line data was created by (Leacock et al. , 1993) by tagging every occurrence of line in the ACL/DCI Wall Street Journal corpus and the American Printing House for the Blind corpus with one of six possible WordNet senses. ::: A00-2009_49:165

	This data has since been used in studies by (Mooney, 1996), (Towell and Voorhees, 1998), and (Leacock et al. , 1998). ::: A00-2009_51:165

R03	The interest data was created by (Bruce and Wiebe, 1994) by tagging all occurrences of interest in the ACL/DCI Wall Street Journal corpus with senses from the Longman Dictionary of Contemporary English. ::: A00-2009_54:165

	This data set was subsequently used for word sense disambiguation experiments by (Ng and Lee, 1996), (Pedersen et al. , 1997), and (Pedersen and Bruce, 1997). ::: A00-2009_55:165

	For example, in this work five-fold cross validation is employed to assess accuracy while (Ng and Lee, 1996) train and test using 100 randomly sampled sets of data. ::: A00-2009_81:165

	The interest data was first studied by (Bruce and Wiebe, 1994). ::: A00-2009_84:165

	The line data was first studied by (Leacock et al. , 1993). ::: A00-2009_99:165

M02,M13	Statistical techniques, both supervised learning from tagged corpora (Yarowsky, 1992), (Ng and Lee, 1.996), and unsupervised learning (Yarowsky, 1995), (Resnik, 1997), have been investigated. ::: C00-1023_15:203

M20,M55	The model can be seen as a bootstrapping learning process tbr disambiguation, where the information gained from one part (selectional preference) is used to improve tile other (disambiguation) and vice versa, reminiscent of the work by Riloff and Jones (1.999) and Yarowsky (1995). ::: C00-1023_196:203

R09	In Table 1, the accuracy of disambiguating 135 adjective noun pairs Dora the br-a01 file of the semantically tagged corlms SemCor (Miller et al. , 1993) is compared to the baseline, wlfich was calculated by using the first WordNet sense of the adjective. ::: C00-1023_79:203

E03	Results and evaluation We used two measurements, applicability and precision (Dagan and Itai 1994), to evaluate the performance of our method. ::: C02-1058_105:160

M13,M04	A variety of unsupervised WSD methods, which use a machine readable dictionary or thesaurus in addition to a corpus, have also been proposed (Yarowsky 1992; Yarowsky 1995; Karov and Edelman 1998). ::: C02-1058_14:160

	Total S1={tank, , , } 571664 S2={tank, } 8 23 5 36 Total 65 24 11 100 [Note] S1: a large container for storing liquid or gas S2: an enclosed heavily armed, armored vehicle (e) Polysemous word title (applicability=92.0%; precision=81.5%) Results Correct sense S1 S2S3 S4 ? Total S1={title, , , } 43100246 S2={title, , , , } 62601538 S3={title, , , } 11011 4 S4={title, } 3306012 Total 533108810 [Note] S1: a word or name given to a person to be used before his/her name as a sign rank, profession, etc. S2: a name given to a book, play, etc. S3: the legal right to own something S4: the position of being the winner of an sports competition (f) Polysemous word trial (applicability=92.0%; precision=92.4%) Results Correct sense S1 S2S3 S4 S5 ? Total S1={trial,,, } 623000570 S2={trial,, ,,, } 4230002 29 S3={trial, } 000001 1 S4={trial,, } 000000 0 S5={trial,,, } 000000 0 Total 626000810 [Note] S1: a legal process in which a court examines a case S2: a process of testing to determine quality, value, usefulness, etc. S3: a sports competition that tests a players ability S4: annoying thing or person S5: difficulties and troubles (Dagan and Itai 1994; Kikui 1998), where instances of co-occurrence in a first-language text are aligned with co-occurrences statistically extracted from the second language corpus. ::: C02-1058_142:160

M29	Under the maximum entropy framework (Berger et al. , 1996), evidence from different features can be combined with no assumptions of feature independence. ::: C02-1143_14:130

	We used a rich feature space based on lemmas, part-of-speech (POS) tags and a variety of positional and syntactic relationship soft he target word capturing both immediate local context and wider context.These feature types have been widely used in WSD algorithms (see Lee and Ng (2002) for an evaluation of their effectiveness).Their use is illustrated on a sample English sentence for the target word sense in Table 1. ::: C08-1009_134:223

	A similar idea, proposed by Yarowsky (1992), is to use a thesaurus and acquire informative contexts from words in the same category as the target. ::: C08-1009_60:223

M46,M13	Lins (1998) information-theoretic similarity measure is commonly used in lexicon acquisition tasks and has demonstrated good performance in unsupervised WSD (McCarthy et al., 2004). ::: C08-1009_81:223

	First, it has 67 been shown to perform well in the related task of predominant sense detection (McCarthy et al., 2004). ::: C08-1009_98:223

M01,M56,M51,M50	Knowledge based approaches to WSD such as Lesks algorithm (Lesk, 1986), Walkers algorithm (Walker and Amsler, 1986), Conceptual Density (Agirre and Rigau, 1996) and PageRank (Mihalcea, 2005) are less demanding in terms of resources but fail to deliver good results. ::: C10-1063_30:177

M02,M38,M34	Supervised approaches like SVM (Lee et al., 2004) and k-NN (Ng and Lee, 1996), on the other hand, give better accuracies, but the requirement of large annotated corpora renders them unsuitable for resource scarce languages. ::: C10-1063_31:177

M02,M13,R04	Previous works have shown that supervised approaches to Word Sense Disambiguation which rely on sense annotated corpora (Ng and Lee, 1996; Lee et al., 2004) outperform unsupervised (Veronis, 2004) and knowledge based approaches (Mihalcea, 2005). ::: C10-1063_7:177

V02,R09	The sentences that we use from the GWS dataset were originally extracted from the English SENSEVAL-3 lexical sample task (Mihalcea et al., 2004) (hereafter SE-3) and SemCor (Miller et al., 1993). ::: D09-1046_59:247

M31	Naive Bayes is particularly useful when relatively small amounts of training CSF instances are available (Zhang, 2004), and achieves good results when compared to other classifiers for the WSD task (Mooney, 1996), which might explain our results. ::: D10-1032_214:334

R09,P08,V01,V02	We prefer SemCor to all-words datasets available in Senseval-3 (Snyder and Palmer, 2004) or SemEval-2007 (Pradhan et al., 2007), since it includes many more documents than either set (350 versus 3) and therefore allowing more reliable results. ::: D11-1051_195:250

V01,V02,M50	Evaluation framework and results In this paper we will use two datasets for comparing graph-based WSD methods, namely, the Senseval-2 (S2AW) and Senseval-3 (S3AW) all words datasets (Snyder and Palmer, 2004; Palmer et al., 2001), which are both labeled with WordNet 1.7 tags. ::: E09-1005_107:217

M52	Typically, some semantic similarity metric is used for calculating the relatedness among senses (Lesk, 1986; McCarthy et al., 2004). ::: E09-1005_11:217

	The differences between methods are not statistically significant, which is a common problem on this relatively small datasets (Snyder and Palmer, 2004; Palmer et al., 2001). ::: E09-1005_121:217

E09,M02	Table 1 also shows the most frequent sense (MFS), as well as the best supervised systems (Snyder and Palmer, 2004; Palmer et al., 2001) that participated in each competition (SMUaw and GAMBL, respectively). ::: E09-1005_128:217

M44	Although alternatives like simulated annealing (Cowie et al., 1992) and conceptual density (Agirre and Rigau, 1996) were tried, most of past knowledge based WSD was done in a suboptimal word-by-word process, i.e., disambiguating words one at a time. ::: E09-1005_13:217

M02	Supervised WSD systems are the best performing in public evaluations (Palmer et al., 2001; Snyder and Palmer, 2004; Pradhan et al., 2007) but they need large amounts of hand-tagged data, which is typically very expensive to build. ::: E09-1005_7:217

	However, we cannot directly apply Elesk as it was used in (McCarthy et al., 2004) to nd the most likely sense in the set of word senses dened in each inventory following the approach of McCarthy et al. ::: I08-1073_115:164

	This nding is in line with previous results (Snyder and Palmer, 2004). ::: I08-1073_14:164

M56	We contrast an implementation of lesk (Lesk, 1986) which uses only dictionary denitions with the Jiang-Conrath measure (jcn) (Jiang and Conrath, 1997) which uses manually produced hyponym links and was used previously for this purpose on English datasets (McCarthy et al., 2004). ::: I08-1073_33:164

P01	Previous research in inducing sense rankings from an untagged corpus (McCarthy et al., 2004), and inducing selectional preferences at the word level (for other applications) (Erk, 2007) will provide the starting point for research in this direction. ::: I08-2105_121:147

M20	Yarowsky (1995) has proposed a bootstrapping method for word sense disambiguation. ::: J04-1001_11:334

	This implementation is exactly the one proposed in Yarowsky (1995). ::: J04-1001_214:334

	We viewed the seed word as a classified sentence, following a similar proposal in Yarowsky (1995). ::: J04-1001_226:334

	Experiment 2: Yarowskys Words We also conducted translation on seven of the twelve English words studied in Yarowsky (1995). ::: J04-1001_252:334

	Note that the results of MB-D here cannot be directly compared with those in Yarowsky (1995), because the data used are different. ::: J04-1001_272:334

M31,M32,M34,M30,M25,M22	They include those using naive Bayes (Gale, Church, and Yarowsky 1992a), decision lists (Yarowsky 1994), nearest neighbor (Ng and Lee 1996), transformation-based learning (Mangu and Brill 1997), neural networks (Towell and Voorhees 1998), Winnow (Golding and Roth 1999), boosting (Escudero, Marquez, and Rigau 2000), and naive Bayesian ensemble (Pedersen 2000). ::: J04-1001_41:334

M32	Let us first consider the use of decision lists, as proposed in Yarowsky (1994). ::: J04-1001_55:334

M20,M17	Yarowsky (1995) proposed such a method for word sense disambiguation, which we refer to as monolingual bootstrapping. ::: J04-1001_72:334

M41	After line 17, we can employ the one-sense-per-discourse heuristic to further classify unclassified data, as proposed in Yarowsky (1995). ::: J04-1001_86:334

	In this article, we present an in-depth study of a method for automatically acquiring predominant senses for words from raw text (McCarthy et al. 2004a). ::: J07-4005_17:587

R01	We use WordNet (Fellbaum 1998) because this is widely used, is publicly available, and has plenty of gold-standard evaluation data available (Miller et al. 1993; Cotton et al. 2001; Preiss and Yarowsky 2001; Mihalcea and Edmonds 2004). ::: J07-4005_20:587

R09	We extend previously reported work in a number of different directions: a114 We evaluate the method on all parts of speech (PoS) on SemCor (Miller et al. 1993). ::: J07-4005_23:587

R09,V02	Previous experiments (McCarthy et al. 2004c) evaluated only nouns on SemCor, or all PoS but only on the Senseval-2 (Cotton et al. 2001) and Senseval-3 (Mihalcea and Edmonds 2004) data. ::: J07-4005_24:587

	From previous work (McCarthy et al. 2004b), the value of k has a minimal effect on finding the predominant sense; however, we will continue experimentation with this in the future for using our ranking score for estimating probability distributions of senses, because a sufficiently large value of k will be needed to include neighbors for rarer senses. ::: J07-4005_248:587

R01,R04	WordNet is widely used for research in WSD because it is publicly available and there are a number of associated sense-tagged corpora (Miller et al. 1993; Cotton et al. 2001; Preiss and Yarowsky 2001; Mihalcea and Edmonds 2004) available for testing purposes. ::: J07-4005_256:587

M56	However, we did not use jcn for verbs because in previous experiments (McCarthy et al. 2004c) the lesk measure outperformed jcn because the structure of the hyponym hierarchy is very shallow for verbs and the measure is therefore considerably less informative for verbs than it is for nouns. ::: J07-4005_278:587

	In the third experiment, we revisit some previous work on noun senses and domain (McCarthy et al. 2004a) using corpora of news text about sports and finance. ::: J07-4005_319:587

R09	We have previously demonstrated that the method produces intuitive domain-specific models for nouns (McCarthy et al. 2004a), and that these can be more accurate than first senses derived from SemCor for words salient to a domain (Koeling, McCarthy, and Carroll 2005). ::: J07-4005_478:587

M02	The most accurate word sense disambiguation (WSD) systems use supervised machine learning approaches (Stevenson and Wilks 2001), trained on text which has been sense tagged by hand. ::: J07-4005_54:587

	We presented a number of evaluations investigating various facets of a previously proposed method for automatically acquiring this information (McCarthy et al. 2004a). ::: J07-4005_559:587

V01,V02	But in the English all-words tasks in Senseval-2 and Senseval-3 (Snyder and Palmer 2004), systems that did not make use of hand-tagged data (in some form or other) performed substantially worse than those that did. ::: J07-4005_59:587

P08,E09	The performance of WSD systems, at least for all-words tasks, seems to have plateaued at a level just above the first sense heuristic (Snyder and Palmer 2004). ::: J07-4005_65:587

	This figure is the arithmetic mean of two published estimates (Snyder and Palmer 2004), the difference being due to the treatment of multiwords. ::: J07-4005_79:587

R03,R01	The sense ordering in LDOCE is based on lexicographer intuition, whereas in WordNet the senses are ordered according to their frequency in SemCor (Miller et al. 1993). ::: J07-4005_83:587

M57	Word-sense disambiguation, a problem that once seemed out of reach for systems without a great deal of handcrafted linguistic and world knowledge, can now in some cases be done with high accuracy when all information is derived automatically from corpora (Brown, Lai, and Mercer 1991; Yarowsky 1992; Gale, Church, and Yarowsky 1992; Bruce and Wiebe 1994). ::: J95-4004_12:404

	At each training-set size, a new copy of the network is trained under each of the following conditions: (1) using SULU, (2) using SULU but supplying only the labeled training examples to synthesize, (3) standard network training, (4) using a re-implementation of an algorithm proposed by Yarowsky (1995), and (5) using standard network training but with all training examples labeled to establish an upper bound. ::: J98-1005_336:530

M15,M20,M03,M14	This includes the automatic generation of sense-tagged data using monosemous relatives (Leacock et al. , 1998; Mihalcea and Moldovan, 1999; Agirre and Martinez, 2004), automatically bootstrapped disambiguation patterns (Yarowsky, 1995; Mihalcea, 2002), parallel texts as a way to point out word senses bearing different translations in a second language (Diab and Resnik, 2002; Ng et al. , 2003; Diab, 2004), and the use of volunteer contributions over the Web (Chklovski and Mihalcea, 2002). ::: N07-1025_12:153

M03	Starting with a collection of parallel texts, sense annotations were generated either for one word at a time (Ng et al. , 2003; Diab, 2004), or for all words in unrestricted text (Diab and Resnik, 2002), and in both cases the systems trained on these data were found to be competitive with other word sense disambiguation systems. ::: N07-1025_135:153

V02	This method, initially proposed by (Yarowsky, 1995), was successfully evaluated in the context of the SENSEVAL framework (Mihalcea, 2002). ::: N07-1025_137:153

M01,M57,M02	Among the various knowledge-based (Lesk, 1986; Galley and McKeown, 2003; Navigli and Velardi, 2005) and data-driven (Yarowsky, 1995; Ng and Lee, 1996; Pedersen, 2001) word sense disambiguation methods that have been proposed to date, supervised systems have been constantly observed as leading to the highest performance. ::: N07-1025_8:153

V02	This feature set is similar to the one used by (Ng and Lee, 1996), as well as by a number of state-ofthe-art word sense disambiguation systems participating in the SENSEVAL-2 and SENSEVAL-3 evaluations. ::: N07-1025_82:153

M31	The features are integrated in a Naive Bayes classifier, which was selected mainly for its performance in previous work showing that it can lead to a state-of-the-art disambiguation system given the features we consider (Lee and Ng, 2002). ::: N07-1025_83:153

	In the following testing phase, a word is classified into senses (Mihalcea, 2002) (Ng and Lee, 1996). ::: N09-1004_25:224

M02,M20	To overcome the knowledge acquisition bottleneck problem suffered by supervised methods, these methods make use of a small annotated corpus as seed data in a bootstrapping process (Hearst, 1991) (Yarowsky, 1995). ::: N09-1004_29:224

	Disambiguation of a limited number of words is not hard, and necessary context information can be carefully collected and hand-crafted to achieve high disambiguation accuracy as shown in (Yarowsky, 1995). ::: N09-1004_35:224

M03	For example, the use of parallel corpora for sense tagging can help with word sense disambiguation (Brown et al. , 1991; Dagan, 1991; Dagan and Itai, 1994; Ide, 2000; Resnik and Yarowsky, 1999). ::: P04-1037_11:196

M02,M13	Supervised approaches which make use of a small hand-labeled training set (Bruce and Wiebe, 1994; Yarowsky, 1993) typically outperform unsupervised approaches (Agirre et al. , 2000; Litkowski, 2000; Lin, 2000; Resnik, 1997; Yarowsky, 1992; Yarowsky, 1995), but tend to be tuned to a speci c corpus and are constrained by scarcity of labeled data. ::: P04-1037_9:196

	Two more recent investigations are by Yarowsky, (Yarowsky, 1995), and later, Mihalcea, (Mihalcea, 2002). ::: P04-1039_29:192

M38	(Lee and Ng, 2002) explored the relative contribution of different knowledge sources and learning algorithms to WSD; they used Support Vector Machines (SVM) and included local collocations and syntactic relations, and also found that adding syntactic features improved accuracy. ::: P05-1006_135:146

	This is in comparison to the 61.1% accuracy achieved by (Lee and Ng, 2002), which has been the best published result on this corpus. ::: P05-1006_64:146

R01,V02,R16,P08	Recent estimations of the inter-annotator agreement when using the WordNet inventory report figures of 72.5% agreement in the preparation of the English all-words test set at Senseval-3 (Snyder and Palmer, 2004) and 67.3% on the Open Mind Word Expert annotation exercise (Chklovski and Mihalcea, 2002). ::: P06-1014_10:205

P08,V02,P02	Experiments on Senseval-3 As a first experiment, we assessed the effect of the automatic sense clustering on the English allwords task at Senseval-3 (Snyder and Palmer, 2004). ::: P06-1014_144:205

M01,M57	WSD approaches can be classified as (a) knowledge-based approaches, which make use of linguistic knowledge, manually coded or extracted from lexical resources (Agirre and Rigau, 1996; Lesk 1986); (b) corpus-based approaches, which make use of shallow knowledge automatically acquired from corpus and statistical or machine learning algorithms to induce disambiguation models (Yarowsky, 1995; Schtze 1998); and (c) hybrid approaches, which mix characteristics from the two other approaches to automatically acquire disambiguation models from corpus supported by linguistic knowledge (Ng and Lee 1996; Stevenson and Wilks, 2001). ::: P07-1006_34:188

	In (McCarthy et al. , 2004), a method was presented to determine the predominant sense of a word in a corpus. ::: P07-1007_154:190

M02	However, in (Chan and Ng, 2005), we showed that in a supervised setting where one has access to some annotated training data, the EMbased method in section 5 estimates the sense priors more effectively than the method described in (McCarthy et al. , 2004). ::: P07-1007_155:190

R07	DSO corpus (Ng and Lee, 1996) contains 192,800 annotated examples for 121 nouns and 70 verbs, drawn from BC and WSJ. ::: P07-1007_36:190

R09	Among the few currently available manually sense-annotated corpora for WSD, the SEMCOR (SC) corpus (Miller et al. , 1994) is the most widely used. ::: P07-1007_41:190

	These knowledge sources were effectively used to build a state-of-the-art WSD program in one of our prior work (Lee and Ng, 2002). ::: P07-1007_66:190

R09	The optimal threshold and maximum search depth are estimated by maximizing Degrees F1 on a development set of 1,000 randomly chosen noun instances from the SemCor corpus (Miller et al., 1993). ::: P10-1154_169:225

M02,R04	Related work Earlier approaches to WSD which encoded information from variety of knowledge sources can be classified as follows: Supervised approaches: Most of the supervised systems (Yarowsky and Florian, 2002; 16 Lee and Ng, 2002; Martnez et al., 2002; Stevenson and Wilks, 2001) rely on the sense tagged data. ::: P10-3003_113:140

M02	Supervised approaches like (Yarowsky and Florian, 2002; Lee and Ng, 2002; Martnez et al., 2002; Stevenson and Wilks, 2001) used collective information from various knowledge sources to perform disambiguation. ::: P10-3003_15:140

M41	For instance, to the extent one sense per discourse (Gale et al., 1992) holds true, higher utility can be returned to the solutions which favour same sense to all the occurrences of a word in a given discourse. ::: P10-3003_74:140

M13,M01	To circumvent this problem, unsupervised and knowledge based approaches (Lesk, 1986; Walker and Amsler, 1986; Agirre and Rigau, 1996; McCarthy et al., 2004; Mihalcea, 2005) have been proposed as an alternative but they have failed to deliver good accuracies. ::: P11-1057_11:151

M21	Semi-supervised approaches (Yarowsky, 1995) which use a small amount of annotated data and a large amount of untagged data have shown promise albeit for a limited set of target words. ::: P11-1057_12:151

M20	Bootstrapping for Word Sense Disambiguation was first discussed in (Yarowsky, 1995). ::: P11-1057_43:151

P08,M03	The failure of monolingual approaches (Ng and Lee, 1996; Lee et al., 2004; Lesk, 1986; Walker and Amsler, 1986; Agirre and Rigau, 1996; McCarthy et al., 2004; Mihalcea, 2005) to deliver high accuracies for all-words WSD at low costs created interest in bilingual approaches which aim at reducing the annotation effort. ::: P11-1057_48:151

M02	The high cost of collecting sense annotated data for supervised approaches (Ng and Lee, 1996; Lee et al., 2004) has always remained a matter of concern for some of the resource deprived languages of the world. ::: P11-1057_9:151

	A data recovery task In the second evaluation, the estimation method had to distinguish between members of two sets of 8It should be emphasized that the TWS method uses only a monolingual target corpus, and not a bilingual corpus as in other methods ((Brown et al. , 1991; Gale et al. , 1992)). ::: P93-1022_143:178

R03	The senses marked with * are used in Yarowsky (1992) but no corresponding sense is found in LDOCE. ::: P95-1025_142:187

R03	The sense marked with ** is defined in LDOCE but not used in Yarowsky (1992). ::: P95-1025_144:187

R03	For some of the words, more than one sense listed in LDOCE corresponds to a sense as used in Yarowsky (1992). ::: P95-1025_148:187

R01	On the other hand, the thesaurus-based method of Yarowsky (1992) may suffer from loss of information (since it is semi-class-based) as well as data sparseness (since H Classes used in Resnik (1992) are based on the WordNet taxonomy while classes of Brown et al. ::: P95-1025_162:187

M57,R04,M03	Previous corpus-based sense disambiguation methods require substantial amounts of sense-tagged training data (Kelly and Stone, 1975; Black, 1988 and Hearst, 1991) or aligned bilingual corpora (Brown et al. , 1991; Dagan, 1991 and Gale et al. 1992). ::: P95-1025_4:187

	Like the thesaurus based approach of Yarowsky (1992), our approach relies on the dilution of this noise by their distribution through all the 1792 defining concepts. ::: P95-1025_45:187

R04	Yarowsky (1992) introduces a thesaurus-based approach to statistical sense disambiguation which works on monolingual corpora without the need for sense-tagged training data. ::: P95-1025_5:187

	Our system is tested on the twelve words discussed in Yarowsky (1992) and previous publications on sense disambiguation. ::: P95-1025_90:187

	Numerically, the result is not as good as the 92% as reported in Yarowsky (1992). ::: P95-1025_93:187

	In this work, we shall adopt the methodology first explicitly noted in connection with WSD by (McRoy, 1992), and more recently (Ng and Lee, 1996), namely that of bringing together a number of partial sources of information about a phenomenon and combining them in a principled manner. ::: P98-2228_27:113

	We chose this strategy since there is good evidence (Gale et al. , 1992) that nouns are best disambiguated by broad contextual considerations, while other parts of speech are resolved by more local factors. ::: P98-2228_58:113

M32	Decision lists have already been successfully applied to lexical ambiguity resolution by (Yarowsky, 1995) where they perfromed well. ::: P98-2228_67:113

P08	First, researchers are divided between a general method (that attempts to apply WSD to all the content words of texts, the option taken in this paper) and one that is applied only to a small trial selection of texts words (for example (Schiitze, 1992) (Yarowsky, 1995)). ::: P98-2228_8:113

M02	WSD that use information gathered from training on a corpus that has already been semantically disambiguated (supervised training methods) (Gale et al. , 1992), (Ng and Lee, 1996); 3. ::: P99-1020_10:160

M13	WSD that use information gathered from raw corpora (unsupervised training methods) (Yarowsky, 1995) (Resnik, 1997). ::: P99-1020_11:160

	There are also hybrid methods that combine several sources of knowledge such as lexicon information, heuristics, collocations and others (McRoy, 1992) (Bruce and Wiebe, 1994) (Ng and Lee, 1996) (Rigau et al. , 1997). ::: P99-1020_12:160

	Some of the best results were reported in (Yarowsky, 1995) who uses a large training corpus. ::: P99-1020_135:160

M04	WSD that make use of the information provided by machine readable dictionaries (Cowie et al. , 1992), (Miller et al. , 1994), (Agirre and Rigau, 1995), (Li et al. , 1995), (McRoy, 1992); 2. ::: P99-1020_9:160

M02,M21,M20	To overcome the knowledge acquisition bottleneck suffered in supervised methods, semi-supervised methods make use of a small annotated corpus as seed data in a bootstrapping process (Hearst, 1991) (Yarowsky, 1995). ::: S10-1089_132:142

P04,R01,M04,R13,R03,R07,R09	Since the papers were published, word sense disambiguation has moved to deal with fine grained sense distinctions from widely recognized semantic lexical resources; ontologies like Sensus, Cyc, EDR, WordNet, EuroWordNet, etc. or machine-readable dictionaries like OALDC, Webster's, LDOCE, etc. This is due, in part, to the availability of public hand-tagged material, e.g. SemCor (Miller et al. , 1993) and the DSO collection (Ng & Lee, 1996). ::: W00-1326_15:234

R07	Resources used The DSO collection (Ng and Lee, 1996) focuses on 191 frequent and polysemous words (nouns and verbs), and contains around 1,000 sentences per word. ::: W00-1326_37:234

	These collocations have been used in other word sense disambiguation research and are also referred to as features (Gale et al. , 1993; Ng & Lee, 1996; Escudero et al. , 2000). ::: W00-1326_72:234

	They have been successfully applied to accent restoration, word" sense disambiguation 209 and homograph disambiguation (Yarowsky, 1994; 1995; 1996). ::: W00-1326_79:234

	Previous experiments (Ng and Lee, 1996) have explored the relative contribution of different knowledge sources to WSD and have concluded that collocational information is more important than syntactic information. ::: W02-0813_20:111

	Under the maximum entropy framework (Berger et al. , 1996), evidence from different features can be combined with no assumptions of feature independence. ::: W02-0813_24:111

	Unconstrained CL corresponds exactly to a conditional maximum entropy model (Berger et al. , 1996; Lafferty et al. , 2001). ::: W02-1002_65:249

M21	In another line of research, (Yarowsky, 1995) and (Blum and Mitchell, 1998) have shown that it is possible to reduce the need for supervision with the help of large amounts of unannotated data. ::: W02-1304_77:125

D01	This task is closely related to both named entity recognition (NER), which traditionally assigns nouns to a small number of categories and word sense disambiguation (Agirre and 1http://class.inrialpes.fr/ Rigau, 1996; Yarowsky, 1995), where the sense for a word is chosen from a much larger inventory of word senses. ::: W06-0505_13:181

M21,M03	The information for semi-supervised sense disambiguation is usually obtained from bilingual corpora (e.g. parallel corpora or untagged monolingual corpora in two languages) (Brown et al. , 1991; Dagan and Itai, 1994), or sense-tagged seed examples (Yarowsky, 1995). ::: W06-1649_12:177

V02	We used three types of features to capture the information in all the contextual sentences of target words in SENSEVAL-3 data for all the four algorithms: part-of-speech of neighboring words with position information, words in topical context without position information (after removing stop words), and local collocations (as same as the feature set used in (Lee and Ng, 2002) except that we did not use syntactic relations). ::: W06-1649_130:177

M57,R04,M02,M21	Many corpus based methods have been proposed to deal with the sense disambiguation problem when given de nition for each possible sense of a target word or a tagged corpus with the instances of each possible sense, e.g., supervised sense disambiguation (Leacock et al. , 1998), and semi-supervised sense disambiguation (Yarowsky, 1995). ::: W06-1649_9:177

	Let mwi ??MW be the multiword identified by majority vote for item i. Let MWsys be the subset of T for which there is a multiword response from the system and mwsysi be a multiword specified by the system for item i. detection P =summation text mwsysi?MWsys 1 if mwi exists at i |MWsys| (9) detection R =summation text mwsysi?MW 1 if mwi exists at i |MW| (10) identification P = summation text mwsysi?MWsys 1 if mwsysi = mwi |MWsys| (11) 50 identification R = summation text mwsysi?MW 1 if mwsysi = mwi |MW| (12) 3.1 Baselines We produced baselines using WordNet 2.1 (Miller et al. , 1993a) and a number of distributional similarity measures. ::: W07-2009_70:171

R04,R09,M21	Most systems did not use sense tagged data for disambiguation though MELB did use SemCor (Miller et al. , 1993b) for filtering infrequent synonyms and UNT used a semi-supervised word sense disambiguation combined with a host of other techniques, including machine translation engines. ::: W07-2009_87:171

R09,M03,R07	Hence, besides gathering examples from the widely used SEMCOR corpus, we also gathered training examples from 6 English-Chinese parallel corpora and the DSO corpus (Ng and Lee, 1996). ::: W07-2054_15:92

M38,M02	For both systems, we performed supervised word sense disambiguation based on the approach of (Lee and Ng, 2002) and using Support Vector Machines (SVM) as our learning algorithm. ::: W07-2054_17:92

R09,M03,R07	We gathered training examples from parallel corpora, SEMCOR (Miller et al. , 1994), and the DSO corpus. ::: W07-2054_23:92

M03	Research in (Ng et al. , 2003; Chan and Ng, 2005) has shown that examples gathered from parallel texts are useful for WSD. ::: W07-2054_26:92

	To gather examples from these parallel corpora, we followed the approach in (Ng et al. , 2003). ::: W07-2054_28:92

R09,R04	SEMCOR corpus (Miller et al. , 1994) is one of the few currently available, manually sense annotated corpora for WSD. ::: W07-2054_36:92

R09,R07	SEMCOR, the DSO corpus (Ng and Lee, 1996) also contains manually annotated examples for WSD. ::: W07-2054_39:92

M38	Following the approach of (Lee and Ng, 2002), we train an SVM classifier for each word using the knowledge sources of local collocations, parts-ofspeech (POS), and surrounding words. ::: W07-2054_51:92

M41,A07	We investigated the one sense per discourse hypothesis (Gale et al., 1992b) in the context of machine translation. ::: W09-2404_137:142

M03	In monolingual WSD, word alignments in parallel corpora have been successfully used as learning evidence (Resnik and Yarowsky, 1999; Diab and Resnik, 2002; Ng et al., 2003). ::: W09-2404_15:142

M13	Yarowsky (1995) successfully used this observation as an approximate annotation technique in an unsupervised WSD model. ::: W09-2404_30:142

M03,P08	Moreover, some studies present multilingual WSD systems that attain state-of-the-art performance in all-words disambiguation (Ng et al., 2003). ::: W09-2413_33:105

	(Lesk, 1986) or (Agirre and Rigau, 1996), (Basili et al., 2004) methods) and then use it to rank the alternative senses according to the incoming context. ::: W10-2304_18:230

M03	It has thus been widely adopted in works on multilingual WSD and WSD in MT, where senses are derived from parallel data (Diab, 2003; Ide, 1999; Ide et al., 2002; Ng et al., 2003; Chan et al., 2007; Carpuat and Wu, 2007). ::: W11-2203_27:226

M36	The first is the LBXAS algorithm which uses an exemplar-based learning framework s;mil~to the case-based reasoning foundation of Kenmore (Ng, 1997; Ng and Lee, 1996). ::: W97-0108_165:196

	Ng and Lee (1996) found that train/rig sets of 1000-1500 e~mples per word are necessary for sense dJ-~mhiguation of one highly ambiguous word. ::: W97-0108_191:196

M57,M02	Corpus-based word sense disambignation algorjthm~ such as (Ng and Lee, 1996; Bruce and Wiebe, 1994; Yarowsky, 1994) relied on supervised learning fzom annotated corpora. ::: W97-0108_49:196

M13,M04,R01	Attempts to alleviate this tagbottleneck i~lude tmotstr~ias (Te~ ot ill,, 1996; Hearst, 1991) and unsupervised algorith~ (Yarowsky, 199s) Dictionary-based approaches rely on linguistic knowledge sources such as ma~l~i,~e-readable dictionaries (Luk, 1995; Veronis and Ide, 1990) and WordNet (Agirre and Rigau, 1996; Resnik, 1995) and e0(ploit these for word sense disaznbiguation. ::: W97-0108_51:196

M13,M02	Unsupervised algorit~m~ such as (Yarowsky, 1995) have reported good accuracy that rivals that of supervised algorithms. ::: W97-0108_54:196

R01	In Section 3, I examine the size of the training corpus on the accuracy of WSD, using a corpus of 192,800 occurrences of 191 words hand tagged with WORDNET senses (Ng and Lee, 1996). ::: W97-0201_16:114

A07	The work of (Dagan and Itai, 1994) has also successfully used WSD to improve the accuracy of machine translation. ::: W97-0201_23:114

	This is also the task addressed by other WSD research such as (Bruce and Wiebe, 1994; Miller et al. , 1994). ::: W97-0201_27:114

P04,R01	When the task is to resolve word senses to the fine-grain distinction of WORDNET senses, the accuracy figures achieved are generally not very high (Miller et al. , 1994; Ng and Lee, 1996). ::: W97-0201_28:114

P04	Similarly, if the task is to distinguish between binary, coarse sense distinction, then current WSD techniques can achieve very high accuracy (in excess of 96% when tested on a dozen words in (Yarowsky, 1995)). ::: W97-0201_31:114

R04,M36,E09	Using the sense-tagged corpus of 192,800 word occurrences reported in (Ng and Lee, 1996), I examine the effect of the number of training examples on the accuracy of an exemplar-based classifier versus the base-line, most-frequent-sense classitier. ::: W97-0201_3:114

R01	This is in contrast to disambiguating word senses to the refined senses of WoRDNET, where for instance, the average number of senses per noun is 7.8 and the average number of senses per verb is 12.0 for the set of 191 most ambiguous words investigated in (Ng and Lee, 1996). ::: W97-0201_33:114

	A number of past research work on WSD, such as (Leacock et al. , 1993; Bruce and Wiebe, 1994; Mooney, 1996), were tested on a small number of words like "line" and "interest". ::: W97-0201_48:114

	Similarly, (Yarowsky, 1995) tested his WSD algorithm on a dozen words. ::: W97-0201_49:114

R04,R09,R01	The sense-tagged corpus SEMCOI~, prepared by (Miller et al. , 1994), contains a substantial subset of the Brown corpus tagged with the refined senses of WORDNET. ::: W97-0201_50:114

R09	However, as reported in (Miller et al. , 1994), there are not enough training examples per word in SP.MCOR to yield a broad coverage, high accuracy WSD program, due to the fact that sense tagging is done on every word in a running text in SEMCOR. ::: W97-0201_51:114

R01	To overcome this data sparseness problem of WSD, I initiated a mini-project in sense tagging and collected a corpus in which 192,800 occurrences of 191 words have been manually tagged with senses of WORDNET (Ng and Lee, 1996). ::: W97-0201_52:114

M36	To investigate the effect of the number of training examples on WSD accuracy, I ran the exemplar based WSD algorithm L~.XAS on varying number of training examples to obtain learning curves for the 191 words (details of LEXAS are described in (Ng and Lee, 1996)). ::: W97-0201_54:114

	The two test sets, BC50 and WSJ6, are the same as those reported in (Ng and Lee, 1996). ::: W97-0201_69:114

	The performance figures of LEXAS in Table 1 are higher than those reported in (Ng and Lee, 1996). ::: W97-0201_72:114

M41	(Yarowsky, 1995) pointed out that the sense of a target word is highly consistent within any given document (one sense per discourse). ::: W98-0701_136:168

	Because our algorithm does not consider the context given by the preceding sentences, we have conducted the following experiment to see to what extent the discourse context could improve the performance of the wordsense disambiguation: Using the semantic concordance files (Miller et al. , 1993), we have counted the occurrences of content words which previously appear in the same discourse file. ::: W98-0701_137:168

P04	Similarly to our work, (Resnik, 1995)(Agirre and Rigau, 1996) challenge the fine-grainedness of WordNet, but their work is limited to nouns only. ::: W98-0701_156:168

	(Agirre and Rigau, 1996) report coverage 86.2%, precision 71.2% and recall 61.4% for nouns in four randomly selected semantic concordance files. ::: W98-0701_157:168

M41	(Yarowsky, 1995), whose training corpus for the noun drug was 9 times bigger than that of Karov and Edelman, reports 91.4% correct performance improved to impressive 93.9% when using the "one sense per discourse" constraint. ::: W98-0701_161:168

R09	Both for the training and for the testing of our algorithm, we used the syntactically analysed sentences of the Brown Corpus (Marcus, 1993), which have been manually semantically tagged (Miller et al. , 1993) into semantic concordance files (SemCor). ::: W98-0701_19:168

