E02	One semiautomatic approach to evaluation is ROUGE (Lin and Hovy, 2003), which is primarily based on ngram co-occurrence between automatic and human summaries. ::: W05-0905_7:165

M20	Automatic text summarization approaches have offered reasonably well-performing approximations for identifiying important sentences (Lin and Hovy, 2002; Schiffman et al., 2002; Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Daume III and Marcu, 2006) but, not surprisingly, text (re)generation has been a major challange despite some work on sub-sentential modification (Jing and McKeown, 2000; Knight and Marcu, 2000; Barzilay and McKeown, 2005). ::: I08-1016_6:147

M24,V02	We selected TextRank as it has a performance competitive with the top systems participating in DUC 02 (Mihalcea and Tarau, 2004). ::: N10-1133_112:158

F07,F03	These methods usually represent the documents as term-sentence matrices (where each row represents a sentence and each column represents a term) or graphs (where each node is a sentence and each edge represents the pairwise relationship among corresponding sentences), and ranks the sentences according to their scores calculated by a set of predefined features, such as term frequency inverse sentence frequency (TF-ISF) (Radev et al., 2004; Lin and Hovy, 2002), sentence or term position (Yih et al., 2007), and number of keywords (Yih et al., 2007). ::: P09-2075_12:106

M21,M01,P23	The features used for the experiments reported here are inspired by previous work in text summarization on content selection (Kupiec et al., 1995), rhetorical classification (Teufel and Moens, 2002), and information ordering (Lapata, 2003). ::: W09-2807_49:117

F09	Graph-based methods (Erkan and Radev, 2004; Wan et al., 2007b) have also been proposed to rank sentences or passages based on the PageRank algorithm or its variants. ::: C10-1111_29:176

	Table 1: Sentence scoring metrics Name Description Source POS F Closeness to the beginning of the document: 1i (Edmundson, 1969) POS L Closeness to the end of the document: i (Baxendale, 1958) POS B Closeness to the borders of the document: max(1i, 1ni+1) (Lin and Hovy, 1997) LEN W Number of words in the sentence (Satoshi et al., 2001) LEN CH Number of characters in the sentence5 LUHN maxi{clusters(S)}{CSi}, CSi = W2iNi (Luhn, 1958) KEY Sum of the keywords frequencies: summation textt{Keywords(S)} tf(t) (Edmundson, 1969) COV Ratio of keywords number (Coverage): |Keywords(S)||Keywords(D)| (Liu et al., 2006a) TF Average term frequency for all sentence words: summationtext tS tf(t) N (Vanderwende et al., 2007) TFISF summationtexttS tf(t) isf(t), isf(t) = 1 log(n(t))log(n) , (Neto et al., 2000) n(t) is the number of sentences containing t SVD Length of a sentence vector in 2 V T after computing Singular Value (Steinberger and Jezek, 2004) Decomposition of a term by sentences matrix A = UV T TITLE O Overlap similarity6 to the title: sim(S,T) = |ST|min{|S|,|T|} (Edmundson, 1969) TITLE J Jaccard similarity to the title: sim(S,T) = |ST||ST| TITLE C Cosine similarity to the title: sim(vectorS, vectorT) = cos(vectorS, vectorT) = vectorSvectorT|vectorS||vectorT| D COV O Overlap similarity to the document complement new sim(S,D S) = |ST|min{|S|,|DS|} D COV J Jaccard similarity to the document complement sim(S,D S) = |ST||SDS| D COV C Cosine similarity to the document complement cos(vectorS, vectorD S) = vectorS vectorDS|vectorS|| vectorDS| LUHN DEG Graph-based extensions of LUHN, KEY and COV measures respectively. ::: P10-1095_79:187

F09	The graph-based baselines for single document summarization are described as follows: BasicRank: This baseline approach adopts the basic PageRank algorithm to rank sentences based on all sentence relationships in a single document, similar to previous work (Mihalcea and Tarau, 2004). ::: C10-1128_120:171

S01	These summarizers have been found to produce quantitatively similar results, and both significantly outperform a baseline summarizer, which is the MEAD summarization framework with all options set to the default (Radev et al., 2000). ::: W08-1106_46:197

	The evaluation corpus used is the one from Teufel and Moens (2002). ::: N07-1040_117:155

	(Lin, 2004) Skip-bigram is any pair of words in their sentence order, allowing for arbitrary gaps. ::: P06-2078_96:204

E02	We used the ROUGE evaluation approach (Lin and Hovy, 2003), which is based on n-gram cooccurrence between machine summaries and ideal human summaries. ::: W05-0905_62:165

E03	To evaluate our system, we use the pyramid evaluation method (Nenkova and Passonneau, 2004) at sentence level. ::: C08-1087_138:175

F10	In summarization, such words are called signature terms and are thought to be descriptive of the input; they can be identified using the log-likelihood ratio of each word (Lin and Hovy, 2000; Gupta et al., 2007). ::: N10-1131_60:182

E02	We used ROUGE (Lin, 2004) for evaluating the content of summaries. ::: P10-2060_97:149

M19,M20	Existing work in abstractive summarization has been quite limited and can be categorized into two categories: (1) approaches using prior knowledge (Radev and McKeown, 1998) (Finley and Harabagiu, 2002) (DeJong, 1982) and (2) approaches using Natural Language Generation (NLG) systems (Saggion and Lapalme, 2002) (Jing and McKeown, 2000). ::: C10-1039_20:303

F01	Repetition in the input is often exploited as an indicator of importance by different summarization approaches (Luhn, 1958; Barzilay et al., 1999; Radev et al., 2004; Nenkova et al., 2006). ::: P08-1094_108:191

F03,F06	Alternatively, a summarizer may reward passages that occupy important positions in the discourse structure of the text (Ono, Sumita, and Miike 1994; Marcu 1997b). ::: J02-4001_45:155

M20	For example, work on coherence in NLG (Lapata, 2003) could potentially inform summarization evaluation. ::: W05-0905_102:165

F10	Vectors contain only topic signatures from the input and all words of the summary Topic signatures are words highly descriptive of the input, as determined by the application of loglikelihood test (Lin and Hovy, 2000). ::: D09-1032_102:247

E02	In particular, ROUGE-2 is the recall in bigrams with a set of human-written abstractive summaries (Lin, 2004). ::: W09-1802_91:208

M24,M10	TextRank (Mihalcea and Tarau, 2005) and LexPageRank (Erkan and Radev, 2004) use algorithms similar to PageRank and HITS to compute sentence importance. ::: D11-1124_40:230

F10	Such words are called signature terms in Lin and Hovy (2000) who were the first to introduce the log-likelihood weighting scheme for summarization. ::: P07-2049_21:91

F06	Luckily, a variety of discourse theories have been developed over the years (e.g., Mann and Thompson, 1988; Grosz, Weinstein, and Joshi 1995; Halliday and Hasan 1976) and have found application in summarization (Barzilay and Elhadad 1997; Marcu 2000; Teufel and Moens 2002) and other text generation applications (Scott and de Souza 1990; Kibble and Power 2004). ::: J10-3005_41:601

V02,E02	Following the current practice in evaluating summarization, particularly DUC3, we use the ROUGE evaluation package (Lin and Hovy, 2003). ::: P08-1093_99:183

M24	More extensive experimental results with the TextRank system are reported in (Mihalcea and Tarau, 2004), (Mihalcea, 2004). ::: P05-3013_68:74

M04	Barzilay and Elhadad (1997) describe a technique for building lexical chains for extractive text summarization. ::: J10-3005_228:601

E02	ROUGE evaluation Table 4 presents ROUGE scores (Lin, 2004) of each of human-generated 250-word surveys against each other. ::: N09-1066_179:220

S01,M09,F09,M10,M25	Typical existing summarization methods include centroid-based methods (e.g., MEAD (Radev et al., 2004)), graph-ranking based methods (e.g., LexPageRank (Erkan and Radev, 2004)), non-negative matrix factorization (NMF) based methods (e.g., (Lee and Seung, 2001)), Conditional random field (CRF) based summarization (Shen et al., 2007), and LSA based methods (Gong and Liu, 2001). ::: P09-2075_13:106

E02,P01	The third part of the evaluation uses two ROUGE metrics (Lin 2004) to compare the machine-made and the baseline summaries with the model abstracts.The results suggest that these measures are not well suited for evaluating extractive indicative summaries of short stories. ::: J10-1003_38:325

E02	ROUGE (Lin and Hovy, 2003) metrics is used for evaluation1 and we mainly concern about ROUGE1. ::: N06-2046_59:118

P19,F06	As mentioned in Section 2 previous summarization work has mainly focused on cohesion (Sjorochodko 1972; Barzilay and Elhadad 1997) or global discourse structure (Marcu 2000; Daume III and Marcu 2002). ::: J10-3005_207:601

E03	We use the Pyramid model (Nenkova and Passonneau, 2004) to value different summary factoids. ::: P11-1110_113:200

M10,M24	Recently, graph-based ranking methods have been proposed for sentence ranking and scoring, such as LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004). ::: C10-2060_37:225

P19,M04,F10	For instance, Boguraev and Kennedy (1997) represent cohesion in terms of anaphoric relations, whereas Barzilay and Elhadad (1997) operationalize cohesion via lexical chains sequences of related words spanning a topical unit (Morris and Hirst 1991). ::: J10-3005_93:601

	This is similar to Relative Utility (Radev et al. , 2003). ::: C04-1077_89:200

	Early studies used text similarity measures such as cosine similarity (with or without weighting schema) to compare peer and model summaries (Donaway et al., 2000), various vocabulary overlap measures such as set of n-grams overlap or longest common subsequence between peer and model have also been proposed (Saggion et al., 2002; Radev et al., 2003). ::: C10-2122_18:160

M10,M24	Graph-based ranking methods, such as PageRank (Page et al., 1998) and HITS (Kleinberg, 1999) have recently applied and been successfully used for multi-document summarization (Erkan and Radev, 2004; Mihalcea and Tarau, 2005). ::: W10-2316_9:110

F10	This is similar to the idea of topic signature introduced in (Lin and Hovy 2000). ::: P02-1058_28:229

P03,F01,F03,F02,F10	In the task of single document summarization, various features have been investigated for ranking sentences in a document, including term frequency, sentence position, cue words, stigma words, and topic signature (Luhn 1969; Lin and Hovy, 2000). ::: P11-1155_36:192

P03,P01	Most research on single document summarization, particularly for domain independent tasks, uses sentence extraction to produce a summary (Lin and Hovy, 1997; Marcu, 1997; Salton et al. , 1991). ::: P99-1071_11:283

	These include both unsupervised sentence ranking (Luhn, 1958; Radev and Jing, 2004, Erkan and Radev, 2004), and supervised methods (Ouyang et al., 2007; Shen et al., 2007; Li et al., 2009). ::: C10-2060_17:225

F10	System overview 3.1 Signature Terms Extraction There are signature terms for different topic texts (Lin and Hovy, 2000). ::: W09-1312_75:190

	One could rely on existing trainable sentence selection (Kupiec et al., 1995) or even phrase selection (Banko et al., 2000) strategies to pick up appropriate is from the document to be abstracted and rely on recent information ordering techniques to sort the i fragments (Lapata, 2003). ::: W09-2807_42:117

	(Papineni et al, 2002) is a system for automatic evaluation of machine translation. ::: W03-0501_141:177

E02	Nowadays, a widespread summarization evaluation framework is ROUGE (Lin and Hovy, 2003) which, as we have mentioned before, offers a set of statistics that compare peer summaries with models. ::: C10-2122_51:160

	As the features, Lapata (2003) proposed the Cartesian product of content words in adjacent sentences. ::: P10-2060_64:149

E02,E03,M26	Empirical evaluations using two standard summarization metricsthe Pyramid method (Nenkova and Passonneau, 2004b) and ROUGE (Lin, 2004)show that the best performing system is a CRF incorporating both order-2 Markov dependencies and skip-chain dependencies, which achieves 91.3% of human performance in Pyramid score, and outperforms our best-performing non-sequential model by 3.9%. ::: W06-1643_21:186

	With our best performing features, we get ROUGE-2 (Lin, 2004) scores of 0.11 and 0.0925 on 2007 and 2006 5This threshold was derived experimentally with previous data. ::: P08-2052_46:86

M04	The first two of these scores are produced by Simfinder, and the salience score is computed using lexical chains (Morris and Hirst 1991; Barzilay and Elhadad 1997) as described below. ::: J05-3002_87:557

E02	According to (Lin and Hovy, 2003), among all sub-metrics in ROUGE, ROUGE-N (N=1, 2) is relatively simple and works well. ::: D11-1124_154:230

P03,E03	The first partition aims to develop manual evaluation criteria for determining the quality of a summary, and is typified by the extensive research done in single-document summarization by Halteren and Teufel (2003) and by the evaluation strategy proposed by Nenkova and Passonneau (2004). ::: W04-1016_12:184

E02	Automatic evaluation was performed with ROUGE (Lin, 2004), a widely used and recognized automated summarization evaluation method. ::: C10-2049_166:215

	We do not distinguish between the two types of links stated above, but only identify which citation(s) a linguistic expression is attributable 1We use a list of around 40 research methodology related nouns from Teufel and Moens (2002), such as e.g., study, account, investigation, result etc. These are nouns we are particularly interested in. ::: N07-1040_34:155

M11	To demonstrate the use of automatic scientific attribution classification, we studied its utility for one well known discourse annotation task: Argumentative Zoning (Teufel and Moens, 2002). ::: N07-1040_108:155

M04	Barzilay and Elhadad (1997) proposed lexical chains as an intermediate step in the text summarization process. ::: J02-4004_14:218

P01,F06	Comparing the Machine-Made Summaries and the Manually Created Extracts Measuring sentence co-selection between extractive summaries created by humans and those created by automatic summarizers has a long tradition in the text summarization community (Lin and Hovy 2000; Marcu 2000), but this family of measures has a number of well-known shortcomings.As many have remarked on previous occasions (Mani 2001; Radev et al.2003), co-selection measures do not provide a complete assessment of the quality of a summary.First of all, when a summary in question contains sentences that do not appear in any of the model extracts, one may not be sure that those sentences are uninformative or inappropriate for inclusion in a summary.In addition, documents have internal discourse structure and sentences are often inter-dependent.Therefore, even if a summary contains sentences found in one or more reference summaries, it does not always mean that it is advisable to include those sentences in the summary in question. ::: J10-1003_236:325

	In this respect, this is similar to work by Lapata (2003), who builds a conditional model of words across adjacent sentences, focusing on words in particular semantic roles. ::: D08-1057_103:247

	Evaluation We evaluated summarization quality using ROUGE (Lin and Hovy, 2003). ::: P10-1058_192:257

	Evaluation metrics such as BLEU (Papineni et al., 2002) have a built-in preference for shorter translations. ::: E09-1017_57:229

E02	The ROUGE method (Lin and Hovy, 2003) is based on n-gram overlap between the system-produced and ideal summaries. ::: C04-1057_125:156

M10,M24	Most recently, the graph-based models have been successfully applied for multi-document summarization by making use of the voting or recommendations between sentences in the documents (Erkan and Radev, 2004; Mihalcea and Tarau, 2005; Wan and Yang, 2006). ::: D08-1079_17:157

P01,P04,M22	A straightforward approach for approximating sentence fusion can be found in the use of sentence extraction for multi document summarization (Carbonell and Goldstein 1998; Radev, Jing, and Budzikowska 2000; Marcu and Gerber 2001; Lin and Hovy 2002). ::: J05-3002_11:557

E02	We use only the words that are content words (nouns, verbs, or adjectives) and not in the stopword list used in ROUGE (Lin, 2004). ::: E09-1089_85:254

F09	Erkan and Radev (2004) and Yoshioka (2004) evaluate the relevance (similarity) between any two sentences first. ::: C08-1124_69:230

E02,V02	The dif1The routinely used tool for automatic evaluation ROUGE was adopted exactly because it was demonstrated it is highly correlated with the manual DUC coverage scores (Lin and Hovy, 2003a; Lin, 2004). ::: P08-1094_31:191

	Like Mani and Barzil~,'s techniques, our approach focuses on the problem that how to identi~" differences and similarities across documents, rather than the problem that how to form the actual summar:,, (Sparck, 1993), (McKeown and Radev, 1995), (Radev and McKeown, 1998). ::: W00-0404_302:312

	ROUGE evaluation: NIST also evaluated the summaries automatically using ROUGE (Lin, 2004; Lin and Hovy, 2003). ::: D09-1032_69:247

E02	Among these different scores, unigram-based ROUGE score (ROUGE-1) has been shown to agree with human judgment most (Lin and Hovy, 2003). ::: P07-1070_111:166

E06,E03	Responsiveness differs from other measures of summary content such as SEE coverage (Lin and Hovy, 2002) and Pyramid scores (Nenkova and Passonneau, 2004) in that it does not compare a peer summary against a set of known human summaries. ::: C08-1019_34:164

M10	LexPageRank (Erkan and Radev, 2004) is an approach for computing sentence importance based on the concept of eigenvector centrality. ::: D08-1079_50:157

V02	Official DUC scoring utilizes the jackknife procedure and assesses significance using bootstrapping resampling (Lin, 2004). ::: N09-1041_26:207

E02	In machine translation, the rankings from the automatic BLEU method (Papineni et al. , 2002) have been shown to correlate well with human evaluation, and it has been widely used since and has even been adapted for summarization (Lin and Hovy, 2003). ::: N04-1019_7:245

	For our experiment, we have only considered unigrams (lemmatized words, excluding stop words), which gives good results with standard summaries (Lin and Hovy, 2003). ::: P04-1027_144:190

M10,M24	In recent years, graph-based ranking algorithms have been successfully used for document summarization (Mihalcea and Tarau, 2004, 2005; ErKan and Radev, 2004) and keyword extraction (Mihalcea and Tarau, 2004). ::: P07-1070_16:166

F03	Machine learning has also been applied to learning individual features; for example, Lin and Hovy (1997) applied machine learning to the problem of determining how sentence position affects the selection of sentences, and Witbrock and Mittal (1999) used statistical approaches to choose important words and phrases and their syntactic context. ::: J02-4001_41:155

F03	Predicates such as to present and to include have the tendency of appearing towards the very beginning or the very end of the abstract been therefore predicted by position-based features (Edmundson, 1969; Lin and Hovy, 1997). ::: W09-2807_85:117

E03	Pyramid approach was introduced by Nenkova and Passonneau (2004) as a method for evaluating machine-generated summaries based on a set of human model summaries. ::: C10-2045_158:188

E02	For the extractive or abstractive summaries, we use ROUGE scores (Lin, 2004), a metric used to evaluate automatic summarization performance, to measure the pairwise agreement of summaries from different annotators. ::: P11-1034_80:203

P01	As in (Radev et al. , 2000), in order to create an extract of a certain length, we simply extract the top scoring sentences that add up to that length. ::: P03-1048_28:179

E02,V02	Summary of datasets We used the ROUGE (Lin and Hovy, 2003) toolkit (i.e. ROUGEeval-1.4.2 in this study) for evaluation, which has been widely adopted by DUC for automatic summarization evaluation. ::: D08-1079_113:157

S01,P01,M09	MEAD (Radev et al. , 2000): MEAD is a centroid-based extractive summarizer that scores sentences based on sentence-level and inter-sentence features which indicate the quality of the sentence as a summary sentence. ::: P03-1048_35:179

	The fact that summary quality improves with increasing summary length has been observed in prior studies as well (Radev and Tam, 2003; Lin and Hovy, 2003b; Kolluru and Gotoh, 2005) but generally little attention has been paid to this fact in system development and no specific user studies are available to show what summary length might be most suitable for specific applications. ::: P08-1094_35:191

E02	ROUGE-2 (based on bigrams) and ROUGE-SU4 (based on both unigrams and skip-bigrams, separated by up to four words) are given by the official ROUGE toolkit with the standard options (Lin, 2004). ::: P11-1049_200:236

	For example, Hirst and St-Onge (1998) greedily disambiguate a word as soon as it is encountered by selecting the sense most strongly related to existing chain members, whereas Barzilay and Elhadad (1997) consider all possible alternatives of word senses and then choose the best one among them. ::: J10-3005_234:601

P01,M24	In this paper, we show that a method for extractive summarization relying on iterative graph-based algorithms, as previously proposed in (Mihalcea and Tarau, 2004) can be applied to the summarization of documents in different languages without any requirements for additional data. ::: I05-2004_8:124

M20	Later work merged information extraction approaches with regeneration of extracted text to improve summary generation (Radev and McKeown 1998). ::: J02-4001_74:155

E02	We employed a number of ROUGE variants, which have been proven to correlate with human judgments in multi-document summarization (Lin, 2004). ::: C10-2049_167:215

	The Bleu machine translation evaluation measure (Papineni et al., 2002) has also been tested in summarization (Pastra and Saggion, 2003). ::: C10-2122_19:160

	Lapata (2003) employed the probability of two sentences being adjacent as determined from a corpus. ::: P10-2060_57:149

	It bases on N-gram cooccurrence and compares the system generated summaries to human judges (Lin and Hovy, 2003). ::: P06-1047_137:181

P04,E03	In the field of multi-document summarization (MDS), the Pyramid method has become an important approach for evaluating machine generated summaries (Nenkova and Passonneau, 2004; Passonneau et al., 2005; Nenkova et al., 2007). ::: C10-2045_6:188

M11	Perhaps the most relevant is work by (Teufel, 1999; Teufel and Moens, 2002) who defined and studied argumentative zoning of texts, especially ones in computational linguistics. ::: C10-2049_35:215

M24	More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank keywords based on the co-occurrence links between words. ::: P07-1070_46:166

	Lapata (2003) presented a methodology for automatically evaluating generated orderings on the basis of their distance from observed sentence orderings in a corpus. ::: J09-1003_255:302

M01,M11	By analyzing rhetorical discourse structure of aim, background, solution, etc. or citation context, we can obtain appropriate abstracts and the most influential contents from scientific articles (Teufel and Moens, 2002; Mei and Zhai, 2008). ::: P09-1023_63:284

F03	Hovy and Lin (1997) present another system that learned how the position of a sentence affects its suitability for inclusion in a summary of the document. ::: P00-1038_157:173

E02	Among them, ROUGE-1 has been shown to agree most with human judgments (Lin and Hovy, 2003). ::: W09-1312_142:190

P07,F01	First, two estimates of importance on words have been used very successfully both in generic and query-focused summarization: frequency (Luhn, 1958; Nenkova et al. , 2006; Vanderwende et al. , 2006) and loglikelihood ratio (Lin and Hovy, 2000; Conroy et al. , 2006; Lacatusu et al. , 2006). ::: P07-2049_10:91

	Unlike many other methods that directly utilize noun phrase (NP) coreference (Nenkova 2008; Mani et al. 1999), we propose a method that employs insertion and substitution of phrases that modify the same chunk in the lead and other sentences. ::: W09-2808_10:308

M09,S01,F03,F07,F10,M02	The centroid-based method MEAD (Radev et al., 2004) is an implementation of the centroid based method that scores sentences based on features such as cluster centroids, position, and TF.IDF, etc. NeATS (Lin and Hovy, 2002) adds new features such as topic signature and term clustering to select important content, and use MMR (Goldstein et al., 1999) to remove redundancy. ::: D11-1040_52:262

E03,E09	We decided against using deeper approaches, such as the Pyramid method (Nenkova and Passonneau 2004), factoids (van Halteren and Teufel 2003), and relative utility (Radev and Tam 2003).The reason is practical: These approaches have an unfortunate disadvantage of being considerably more labor-intensive than the measures based on sentence co-selection. ::: J10-1003_248:325

F03	Conceptual units do not have to be directly observable as text snippets; they can represent abstract properties that particular text units may or may not satisfy, for example, status as a first sentence in a paragraph or generally position in the source text (Lin and Hovy, 1997). ::: C04-1057_30:156

	Simply using a variant of the Bilingual Evaluation Understudy (BLEU) scoring method (based on a linear combination of matching n-grams between the system output and the ideal summary) developed for machine translation (Papineni et al. 2001) is promising but not sufficient (Lin and Hovy 2002b). ::: J02-4001_118:155

E01,E09	Co-selection measures include precision and recall of co-selected sentences, relative utility (Radev et al. , 2000), and Kappa (Siegel and Castellan, 1988; Carletta, 1996). ::: P03-1048_49:179

	Related work (Mani et al. , 1999) addressed the problem of revising summaries to improve their quality. ::: A00-2024_187:205

	For our first approach we used a nugget-based evaluation methodology (Lin and Demner-Fushman, 2006; Nenkova and Passonneau, 2004; Hildebrandt et al., 2004; Voorhees, 2003). ::: N09-1066_138:220

M24,M10	Very briefly, the TextRank system (Mihalcea and Tarau, 2004) similar in spirit with the concurrently proposed LexRank method (Erkan and Radev, 2004) works by building a graph representation of the text, where sentences are represented as nodes, and weighted edges are drawn using inter-sentential word overlap. ::: D07-1040_142:286

	Our best results of Kappa=0.48 and Macro-F=0.53 are better than the best previously published results on task (Kappa=0.45 and Macro-F=0.50 in Teufel and Moens (2002)). ::: N07-1040_148:155

M19	The usual strategy employed by domain-specific summarizers is for humans to determine a priori what types of information from the originating documents should be included (e.g. , in stories about earthquakes, the number of victims) (Radev and McKeown, 1998; White et al. , 2001). ::: N04-1015_77:174

E02	ROUGE (Lin, 2004) is widely used for summarization evaluation and it has been shown that ROUGE-N scores are highly correlated with human evaluation (Lin, 2004). ::: P11-1052_89:238

F01,F06	A variety of approaches exist for determining the salient sentences in the text: statistical techniques based oll word distribution (Kupiec et al. , 1995), (Zechner, 1996), (Salton et al. , 1991), (Teufell and Moens, 1997), symbolic techniques based on discourse structure (Marcu, 1997) and semantic relations between words (Barzil~v and Elhadad, 1997). ::: W00-0404_278:312

P03,P04	Some of these approaches to single document summarization have been extended to deal with multi-document summarization (Mani and Bloedern, 1997; Goldstein and Carbonell, 1998; TIPSTER, 1998b; Radev and McKeown, 1998; Mani and Bloedorn, 1999; McKeown et al. ,.!999; Stein et al. , 1999). ::: W00-0405_35:197

M04	Lexical chains, which capture relationships between related terms in a document, have shown promise as an intermediate representation for producing summaries (Barzilay and Elhadad, 1997). ::: P11-3014_22:100

E03	Our aim is not only to determine the utility of citation texts for survey creation, but also to examine the quality distinctions between this form of input and others such as abstracts and full texts comparing the results to human-generated surveys using both automatic and nugget-based pyramid evaluation (Lin and Demner-Fushman, 2006; Nenkova and Passonneau, 2004; Lin, 2004). ::: N09-1066_85:220

M20	More recently, summarizers using sophisticated post extraction strategies, such as revision (McKeown et al. , 1999; Jing and McKeown, 1999; Mani et al. , 1999), and sophisticated grammar-based generation (Radev and McKeown, 1998) have also been presented. ::: P00-1041_20:138

P01,F02,F03,F06	Most previous work on summarization focused on extractive methods, investigating issues such as cue phrases (Luhn, 1958), positional indicators (Edmundson, 1964), lexical occurrence statistics (Mathis et al. , 1973), probabilistic measures for token salience (Salton et al. , 1997), and the use of implicit discourse structure (Marcu, 1997). ::: P00-1041_17:138

E03	In recent years the Pyramids evaluation method (Nenkova and Passonneau, 2004) was introduced. ::: C10-2122_24:160

V02	Lin and Hovy (2002) and Lin and Hovy (2003) were the first to systematically point out problems with the large scale DUC evaluation and to look to solutions by seeking more robust automatic alternatives. ::: N04-1019_231:245

P13,M19	These include comparing templates filled in by extracting information using specialized, domain specific knowledge sources from the doc"ument, and then generating natural language summaries from the templates (Radev and McKeown, 1998), com- paring named-entities extracted using specialized lists between documents and selecting the most relevant section (TIPSTER, 1998b), finding co-reference chains in the document set to identify common sections of interest (TIPSTER, 1998b), or building activation networks of related lexical items (identity mappings, synonyms, hypernyms, etc). ::: W00-0405_36:197

E02	Summaries can be evaluated manually, or with automatic metrics such as ROUGE (Lin, 2004). ::: W11-0507_11:105

	We provided more in-depth discussion of this issue in other papers (Lin and Hovy, 2002; Lin and Hovy 2003b). ::: W03-1101_149:160

M01	The understanding can be induced using dependencies between words (Barzilay and Elhadad, 1997), rhetorical relations (Paice and Johns, 1993), events (Filatova and Hatzivassiloglou, 2004). ::: I08-2101_35:143

M12	While earlier approaches for text compression were based on symbolic reduction rules (Grefenstette 1998; Mani, Gates, and Bloedorn 1999), more recent approaches use an aligned corpus of documents and their human written summaries to determine which constituents can be reduced (Knight and Marcu 2002; Jing and McKeown 2000; Reizler et al. 2003). ::: J05-3002_480:557

P03	In revising single-document summaries, [Mani et al, 1999] employed rules such as the referencing of pronouns with the most recently mentioned noun phrase. ::: W02-0404_159:199

M18,P01,P04	Corpus-based methods inspired by the notion of schemata have been explored in the past by Lapata (2003) and Barzilay and Lee (2004) for ordering sentences extracted in a multi-document summarisation application. ::: D08-1057_63:247

	DP 167 323 PBMT 186 516 Summ 839 1425 QA 238 202 TE 56 44 Table 1: Clusters and their citation network size 1.1 Related Work Although there has been work on analyzing citation and collaboration networks (Teufel et al., 2006; Newman, 2001) and scientific article summarization (Teufel and Moens, 2002), to the knowledge of the author there is no previous work that study the text of the citation summaries to produce a summary. ::: C08-1087_28:175

M09,F10,F01	Recently, content features were also well studied, including centroid (Radev et al., 2004), signature terms (Lin and Hovy, 2000) and high frequency words (Nenkova e t al., 2006). ::: C08-1124_50:230

F01,F02,F03	A lot of approaches were proposed in text summarization, such as word frequency based method (Luhn, 1958), cue phrase method (Edmundson, 1969), Position based methods (Edmundson, 1969; Hovy and Lin, 1997; Teufel and Moens, 1997). ::: W03-0506_40:204

S01	MEAD* The extractive approach is represented by MEAD*, which is adapted from the open source summarization framework MEAD (Radev et al., 2000). ::: W08-1106_67:197

	Other systems exploit the co-occurrence of particular concepts (Barzilay and Elhadad, 1997; Lin and Hovy, 2000) or syntactic constraints between concepts (McKeown et al. , 1999). ::: W04-1017_46:202

	Technical paper summarization has also been studied (Paice, 1981; Paice and Jones, 1993; Saggion and Lapalme, 2002; Teufel and Moens, 2002), but the previous work did not explore citation context to emphasize the impact of papers. ::: P08-1093_166:183

E02,E03	Moreover, summarization evaluation metrics such as Basic Element (Hovy et al., 2006), ROUGE (Lin and Hovy, 2003) and Pyramid (Passonneau et al., 2005) are all counting the concept overlap between generated summaries and human-written summaries. ::: C10-2060_46:225

	Volume 36, Number 1 report that people do not agree well on what sentences constitute a good summary of a document (Rath, Resnick, and Savage 1961; Salton et al.1997; Lin and Hovy 2003).In most cases the agreement corresponding to of 0.42 would not be sufcient for creating a resource, but we interpret this level of agreement as acceptable for evaluating a single facet of the summaries that are also evaluated in other ways. ::: J10-1003_278:325

E02	The most commonly used evaluation method for summarization during system development and for reporting results in publications is the automatic evaluation metric ROUGE (Lin, 2004; Lin and Hovy, 2003). ::: D09-1032_7:247

E02	By definition (Lin, 2004), ROUGE-N is the n-gram recall between a candidate summary and a set of reference summaries. ::: P11-1052_94:238

P04	This strategy is commonly used in multi-document summarization (Barzilay et al., 1999; Goldstein et al., 2000; Radev et al., 2000), where the combination step eliminates the redundancy across selected excerpts. ::: P09-1024_120:261

M24	In recent years, graph-based methods have been proposed for sentence ranking (Erkan and Radev, 2004; Mihalcea and Tarau, 2004). ::: P11-1155_40:192

P03	Therefore it is difficult to identi~" these information using a single-document summarizer technique (Mani and Bloedorn, 1997), (Barzilay et al. , 1999). ::: W00-0404_287:312

M10,M24	Graphs have been commonly used for extractive summarization (e.g., LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004)), but in these works the graph is often undirected with sentences as nodes and similarity as edges. ::: C10-1039_34:303

	Many approaches to summarization can be very broadly characterized as TERM-BASED: they attempt to identify the main topics, which generally are TERMS, and then to extract from the document the most important information about these terms (Hovy and Lin, 1997). ::: H05-1001_5:277

M18	Furthermore, to provide some assessment of the quality of the predicted orderings themselves, we follow Lapata (2003) in employing Kendalls a7, which is a measure of how much an ordering differs from the OSO the underlying assumption is that most reasonable sentence orderings should be fairly similar to it. ::: N04-1015_106:174

E03	Pyramid evaluation: The pyramid evaluation method (Nenkova and Passonneau, 2004) has been developed for reliable and diagnostic assessment of content selection quality in summarization and has been used in several large scale evaluations (Nenkova et al., 2007). ::: D09-1032_59:247

P02,M12	Abstraction, on the other hand, relies either on linguistic processing followed by structural compaction (Mani et al. , 1999) or on interpretation of the source text into a semantic representation, which is then condensed to retain only the most important information asserted in the source. ::: W04-2611_79:165

E02	Quality of summary We use the ROUGE (Lin and Hovy, 2003) metric for measuring the summarization system performance. ::: D11-1105_151:239

M10,M24	More recently, graph-based methods that rely on sentence connectivity have also been found successful, using algorithms such as node degree (Salton et al. , 1997) or eigenvector centrality (Mihalcea and Tarau, 2004; Erkan and Radev, 2004; Wolf and Gibson, 2004). ::: D07-1040_29:286

M01,M21	Cohesion information has been used in rhetorical-based parsing for summarization (Marcu, 1997) in order to decide between list or elaboration relations and also in content selection for summarization (Barzilay and Elhadad, 1997). ::: W09-2807_54:117

E02	ROUGE is used for performance measure (Lin and Hovy, 2003; Lin, 2004), which evaluates summaries based on the maxium number of overlapping units between generated summary text and a set of human summaries. ::: P10-1084_149:229

P04	A property that is unique to multi-document summarization is the effect of time perspective (Radev and McKeown, 1998). ::: P99-1071_125:283

	Previous studies have shown that it is feasible to evaluate the output of summarization systems automatically (Lin and Hovy, 2003). ::: P04-1027_104:190

P01,P04	(Lin and Hovy, 2002) is an extraction based multi-document summarization system. ::: W03-1101_33:160

P03	Previous research has addressed revision in single-document summaries [Jing & McKeown, 2000] [Mani et al, 1999] and has suggested that revising summaries can make them more informative and correct errors. ::: W02-0404_40:199

M15	Then one can apply the method inspired by (Barzilay et al., 1999) to identify common phrases across sentences and use language generation to form a more coherent summary. ::: C08-1087_171:175

P13	Like fact extraction methods (e.g. , Radev and McKeown 1998), our method also uses a template whose slots are being filled during analysis. ::: J02-4002_645:704

	The best possible rank is 0, and the worst is a4a6a5 a40 a39 . An additional difficulty we encountered in setting up our evaluation is that while we wanted to compare our algorithms against Lapatas (2003) state-of-the-art system, her method doesnt consider all permutations (see below), and so the rank metric cannot be computed for it. ::: N04-1015_104:174

	This result is presented as 0.053 with the official ROUGE scorer (Lin, 2004). ::: N09-1041_44:207

M14	A statistical model by Lapata (2003) considers both lexical and syntactic features in calculating local coherence. ::: P11-3002_13:131

P03,P04,P01	General text summarization, including single document summarization (Luhn, 1958; Goldstein et al., 1999) and multi-document summarization (Kraaij et al., 2001; Radev et al., 2003) has been well studied; our work is under the framework of extractive summarization (Luhn, 1958; McKeown and Radev, 1995; Goldstein et al., 1999; Kraaij et al., 2001), but our problem formulation differs from any existing formulation of the summarization problem. ::: P08-1093_162:183

	Lapata (2003) proposed an algorithm that computes the probability of two sentences being adjacent for ordering sentences. ::: P10-2060_25:149

F01	NP-rewrite enhanced frequency summarizer Frequency and frequency-related measures of importance have been traditionally used in text summarization as indicators of importance (Luhn, 1958; Lin and Hovy, 2000; Conroy et al., 2006). ::: I08-1016_31:147

F03,F06	This is true of automatic summarization systems too, which consider the position of a sentence in a document and how it relates to its surrounding sentences (Kupiec, Pedersen, and Chen 1995; Barzilay and Elhadad 1997; Marcu 2000; Teufel and Moens 2002). ::: J10-3005_32:601

E02,E03	They report a marginal increase in the automatic word overlap metric ROUGE (Lin, 2004), but a decline in manual Pyramid (Nenkova and Passonneau, 2004). ::: P11-1049_22:236

S01	Thus, we use MEAD (Radev et al., 2000) as our baseline. ::: C10-1039_207:303

E02	The experiments in Lin and Hovy (2003) show that among n-gram approaches to scoring, Rouge-1 (based on unigrams) has the highest correlation with human scores. ::: C04-1129_120:206

F10,P03,P04	The idea of topic signature terms was introduced by Lin and Hovy (Lin and Hovy, 2000) in the context of single document summarization, and was later used in several multi-document summarization systems (Conroy et al., 2006; Lacatusu et al., 2004; Gupta et al., 2007). ::: P08-1094_125:191

	In addition, this is the same even if we use the SummBank corpus (Radev et al. , 2003). ::: C04-1077_27:200

M22,M12	Because of this, it is generally accepted that some kind of postprocessing should be performed to improve the final result, by shortening, fusing, or otherwise revising the material (Grefenstette 1998; Mani, Gates, and Bloedorn 1999; Jing and McKeown 2000; Barzilay et al. 2000; Knight and Marcu 2000). ::: J02-4002_11:704

	The present paper deviates from Lapata (2003) insofar as we do not attempt to learn the ordering preferences between pairs of sentences. ::: N09-3014_33:162

M04	The idea has been formalized in the construct of lexical chains (Barzilay and Elhadad, 1997). ::: P08-1041_61:250

E02	Furthermore, automated summarization metrics like ROUGE (Lin and Hovy, 2003) are non-trivial to adapt to this domain as they require human curated outputs. ::: E09-1059_13:224

P03,P04	Questions regarding the agreement between people in the area of single document summarization and multi-document summarization have already been raised and are currently only partially answered (Halteren and Teufel, 2003; Nenkova and Passonneau, 2004; Marcu and Gerber, 2001). ::: W04-1016_174:184

	A simple ordering criterion is the chronological order of the events represented in the sentences, which is often augmented with other ordering criteria such as lexical overlap (Conroy et al., 2006), lexical cohesion (Barzilay et al., 2002) or syntactic features (Lapata 2003). ::: C10-2170_21:263

M10,M24,P14	Other than for email summarization, other document summarization methods have adopted graph ranking algorithms for summarization, e.g., (Wan et al., 2007), (Mihalcea and Tarau, 2004) and (Erkan and Radev, 2004). ::: P08-1041_54:250

F01,F03,F02,F10,P01	The scores are usually computed based on a combination of statistical and linguistic features, including term frequency, sentence position, cue words, stigma words, topic signature (Hovy and Lin, 1997; Lin and Hovy, 2000), etc. Machine learning methods have also been employed to extract sentences, including unsupervised methods (Nomoto and Matsumoto, 2001) and supervised methods (Kupiec et al. , 1995; Conroy and OLeary, 2001; Amini and Gallinari, 2002; Shen et al. , 2007). ::: P07-1070_32:166

F10	More advanced methods for query expansion use topic signatures words and grammatically related pairs of words that model the query and even the expected answer from sets of documents marked as relevant or not (Lin & Hovy, 2000; Harabagiu, 2004). ::: D08-1080_43:209

P01	Two of the articles focus on the extraction stage (Teufel and Moens 2002; Silber and McCoy 2002), whereas Jing (2002) examines tools for automatically constructing resources that can be used for the second stage. ::: J02-4001_128:155

F10	In addition, infobox could be considered as topic signature (Lin and Hovy, 2000) or keywords about the topic. ::: P09-1023_66:284

M10	Finally, the last system that we implement is TextRank, which uses a variation of the PageRank graph centrality algorithm in order to identify the most important sentences in a document (Page et al., 1999; Erkan and Radev, 2004; Mihalcea and Tarau, 2004). ::: N10-1133_111:158

E02	In terms of validation, a number of studies have claimed that ROUGE correlates with human ratings, for example Lin and Hovy (2003) and Dang (2006). ::: J09-4008_128:465

	We compared our results against those of a bigram language model (the baseline) and an improved version of the state-of-the-art probabilistic ordering method of Lapata (2003), both trained on the same data we used. ::: N04-1015_111:174

E02	Human correlations According to (Lin and Hovy, 2003), ROUGE1 correlates particularly well with human judgments of informativeness. ::: W05-0905_144:165

M10,M24	In recent years, graph-based ranking methods have been investigated for document summarization, such as TextRank (Mihalcea and Tarau, 2004; Mihalcea and Tarau, 2005) and LexPageRank (ErKan and Radev, 2004). ::: C10-1128_33:171

E02	In a recent study (Lin and Hovy, 2003a), we showed that the recall-based unigram cooccurrence automatic scoring metric correlates highly with human evaluation and has high recall and precision in predicting the statistical significance of results comparing with its human counterpart. ::: W03-1101_68:160

M11,M03	The large improvements in Aim and 322 Aim Ctr Txt Own Bkg Bas Oth P .44 .34 .57 .84 .40 .37 .52 R .65 .20 .66 .88 .50 .40 .39 F .52 .26 .61 .86 .44 .38 .44 Correctly Classified Instances 72.5% Kappa statistic 0.45 Macro-F 0.50 Table 7: Teufel and Moens (2002)s best AZ results (Naive Bayes Classifier). ::: N07-1040_146:155

F09	A common approach is to measure similarity between all pairs of sentences and then use clustering to identify themes of common information (McKeown et al. 1999; Radev, Jing, and Budzikowska 2000; Marcu and Gerber 2001). ::: J02-4001_78:155

E02	ROUGE (Lin, 2004) and its linguistically motivated descendent, Basic Elements (BE) (Hovy et al., 2005), evaluate a summary by computing its overlap with a set of model (human) summaries; ROUGE considers lexical n-grams as the unit for comparing the overlap between summaries, while Basic Elements uses larger units of comparison based on the output of syntactic parsers. ::: C08-1019_5:164

P01,P03	It is also notable the study reported in (Lin and Hovy, 2003b) discussing the usefulness and limitations of automatic sentence extraction for text summarization, 23 Single document Meta summarization algorithm summarization algo. ::: I05-2004_114:124

M20	Many NLG researchers are impressed by the BLEU evaluation metric (Papineni et al. 2002) in Machine Translation (MT), which has allowed MT researchers to quickly and cheaply evaluate the impact of new ideas, algorithms, and data sets. ::: J09-4008_6:465

M01,P02	A recent study (Kan et al. , 2001) uses topic composition from text headers, but other studies in the extraction paradigm (Goldstein et al. , 1999), extraction coupled with rhetorical structural identification (Teufel and Moens, 2002), and syntactic abstraction paradigms use different methodologies (Barzilay et al. , 1999; McKeown et al. , 1999). ::: W04-2611_121:165

	One is the longest common subsequence (LCS) based approach (Hori et al. , 2003; Lin, 2004a; Lin, 2004b; Lin and Och, 2004). ::: H05-1019_23:179

M04	Barzilay and Elhadad (1997) rank their chains heuristically by a score based on their length and homogeneity. ::: J10-3005_236:601

M03	The learners we used (with default Weka settings) are: NB: Naive Bayes learner HNB: Hidden Naive Bayes learner IBk: Memory based learner J48: Decision tree based learner STACKING: combining NB and J48 classifiers with the stacking method As mentioned under History feature above, we run each learner twice, the second time including the machine learning prediction for the previous sentence (as we found in Teufel and Moens (2002) for NB, we noticed a slight improvement in performance when using the history feature (between .005 and .01 on both and MacroF for all learners)). ::: N07-1040_141:155

M24,M10	TextRank (Mihalcea and Tarau, 2005) and LexPageRank (Erkan and Radev, 2004) use algorithms similar to PageRank and HITS to compute sentence importance. ::: D11-1040_54:262

E02	ROUGE version 1.5.5 (Lin, 2004) was used for evaluation.2 Among others, we focus on ROUGE-1 in the discussion of the result, because ROUGE-1 has proved to have strong correlation with human annotation (Lin, 2004; Lin and Hovy, 2003). ::: E09-1089_144:254

	NeATS (Lin and Hovy, 2002) makes use of new features such as topic signature to select important sentences. ::: P10-1094_48:240

E03	We now assess the significance of our results by comparing our best system against: (1) a lead summarizer that always selects the first N utterances to match the predefined length; (2) human performance, which is obtained by leave-one-out comparisons among references (Table 7); (3) optimal summaries generated using the procedure explained in (Nenkova and Passonneau, 2004b) by ranking document utterances by the number of model summaries in which they appear. ::: W06-1643_165:186

M10	LexPageRank (Erkan and Radev, 2004) is an approach for computing sentence importance based on the concept of eigenvector centrality. ::: N06-2046_15:118

M10,M24	Graph-based methods for text summarization work usually at the level of sentences (Erkan & Radev, 2004; Mihalcea & Tarau, 2004). ::: D08-1080_44:209

M04	Alternatives to the use 473 Computational Linguistics Volume 24, Number 3 of frequency of key phrases included the identification and representation of lexical chains (Halliday and Hasan 1976) to find the major themes of an article followed by the extraction of one or two sentences per chain (Barzilay and Elhadad 1997), training over the position of summary sentences in the full article (Hovy and Lin 1997), and the construction of a graph of important topics to identify paragraphs that should be extracted (Mitra, Singhal, and Buckley 1997). ::: J98-3005_96:614

E02	ROUGE-N (Lin, 2004) This measure compares n-grams of two summaries, and counts the number of matches. ::: P06-2078_91:204

M10	Next, the remaining sentences are ranked by the sum of two individual scores: a) an authority score from a lexical PageRank algorithm (Erkan and Radev, 2004) and b) a similarity score between the sentence and the Gene Ontology (GO) terms with which the gene is annotated (To date, over 190,000 genes have two or more associated GO terms). ::: W09-1312_26:190

M20,E05	In recent years there has been growing interest in evaluating NLG texts by comparing them to a corpus of human-written reference texts, using automatic metrics such as string-edit distance, tree similarity, or BLEU (Papineni et al. 2002); this is another type of intrinsic evaluation. ::: J09-4008_77:465

	For details see (Radev et al. , 2000). ::: H05-1001_124:277

M10,M24	Inspired by the idea of graph based algorithms to collectively rank and select the best candidate, research efforts in the natural language community have applied graph-based approaches on keyword selection (Mihalcea and Tarau, 2004), text summarization (Erkan and Radev, 2004; Mihalcea, 2004), word sense disambiguation (Mihalcea et al. , 2004; Mihalcea, 2005), sentiment analysis (Pang and Lee, 2004), and sentence retrieval for question answering (Otterbacher et al. , 2005). ::: N06-1027_41:204

S01,M10	Note that although the MEAD distribution also includes an optional feature calculated using the LexRank graph-based algorithm (Erkan and Radev, 2004), this feature could not be used since it takes days to compute for very long documents such as ours, and thus its application was not tractable. ::: D07-1040_88:286

	Lapata (2003) proposed most of these features. ::: C10-2105_122:271

P04,M20,M15	Naturally, an ideal multi-document summary would include a natural language generation component to create cohesive readable summaries (Radev and McKeown, 1998; McKeown et al. , 1999). ::: W00-0405_90:197

E04,E05	Evaluation methods are either extrinsic, in which the summaries are evaluated based on their quality in performing a specific task (Sparck-Jones, 1999) or intrinsic where the quality of the summary itself is evaluated, regardless of any applied task (van Halteren and Teufel, 2003; Nenkova and Passonneau, 2004). ::: P11-1110_43:200

P01,P04	(Lin and Hovy, 2002) is an extraction based multi-document summarization system. ::: P03-2021_20:87

M21,E08	An optimal summary, in terms of content selection, is obtained by maximizing the sum of SCU weights, given a maximum number of SCUs that can be included for a predefined summary length (Nenkova and Passonneau, 2004). ::: C10-2045_31:188

V02,E03,E02	It is also necessary to see whether our genre-specic approach shows any improvements over the existing generic state-of-the-art systems put to work on ction.To this end, we compared our summarizer with two systems that were top performers in the Document Understanding Conference (henceforth DUC) 2007, the annual competition for automatic summarizers.In DUC competitions the summarization systems are evaluated on a variety of metrics: manually assigned scores (ranking readability, grammaticality, non-redundancy, referential clarity, focus, and coherence), the pyramid method (Nenkova and Passonneau 2004), and ROUGE scores (Lin 2004).There is no unied ranking of the systems performance, and selecting the best summarizer is not straightforward.We chose two systems among the top performers in DUC 2007 GISTexter (Harabagiu, Hickl, and Lacatusu 2007) and CLASSY (Schlesinger, OLeary, and Conroy 2008; Conroy, Schlesinger, and OLeary 2007).GISTexter appears to be the best summarizer according to the scores assigned by the human judges.Apart from baselines, it is consistently ranked as the best or the second-best system on most characteristics evaluated by the judges (the only exception is non-redundancy where GISTexter is ranked eighth).CLASSY, on the other hand, is one of the four top systems according to ROUGE scores.The scores it received from the human judges are also quite good. ::: J10-1003_249:325

E01	Features proposed to create the appropriate order include publication date of document (Barzilay et al., 2002), content words (Lapata, 2003; Althaus et al., 2004), and syntactic role of 911 a0a2a1a3a1 a0a0 a0a2a1a4a0a2a1a5 a0 a0a2a1a6 a0 a0a0 a0a0 a5a7a5a0 a5a8a1 a0 a5a9a4 a0 a5a7a6 a0 a5a9a10 a0a11a4a11a1 a0a11a4a12a5 a0a11a4a7a4 a0 a13 a0 a6 Figure 1: Graph representation of summarization. ::: C10-2105_65:271

	This is specific to their approach as both Lapata (2003)s and Barzilay and Lee (2004)s approaches are not tailored to summarization and therefore do not experience the topic bias problem. ::: N09-3014_32:162

F10	In this regard, we use a method similar to Lin and Hovy (2000) to identify signature terms and subsequently use them 2 http://ir.ohsu.edu/genomics/ to discard sentences that contain none or few such terms. ::: W09-1312_63:190

F03	Concepts do not have to be directly observable as text snippets they can represent abstract properties that particular text units may or may not satisfy, for example, status as a first sentence in a paragraph or generally position in the source text (Baxendale, 1958; Lin and Hovy, 1997). ::: W04-1017_47:202

E02	We have compared several similarity metrics, including a few baseline measures (based on document, sentence and vocabulary overlap) and a stateof-the-art measure to evaluate summarization systems, ROUGE (Lin and Hovy, 2003). ::: P04-1027_26:190

	Second of all, even when evaluating linguistic quality, current automatic metrics should be used with caution, as a supplement rather than a replacement for human evaluation; similar comments have been made about the use of automatic metrics in MT (Papineni et al. 2002; Callison-Burch, Osborne, and Koehn 2006). ::: J09-4008_429:465

M18	In our experiments, content models outperform Lapatas (2003) state-of-the-art ordering method by a wide margin for one domain and performance metric, the gap was 78 percentage points. ::: N04-1015_23:174

	This is an extension of Lins method (Lin and Hovy, 2000). ::: C04-1077_163:200

	Lapata (2003), in contrast, does not attempt to model topics explicitly. ::: N09-3014_26:162

E02	Existing automatic evaluation measures such as BLEU (Papineni et al. , 2002) and ROUGE (Lin 2The collections are available from http://www.csail. ::: P05-1018_131:224

E02	Systems are automatically evalatued using ROUGE (Lin, 2004), which has good correlation with human judgments of summary content. ::: W11-0507_56:105

	Many of these documents are likely to repeat much the same information, while differing in certain i Most of these were based on statistical techniques applied to various document entities; examples include frait, 1983; Kupiec et al. , 1995; Paice, 1990, Klavans and Shaw, 1995; MeKeown et al. , 1995; Shaw, 1995; Aon et al. , 1997; Boguraev and Kennedy, 1997; Hovy and Lin, 1997; Mitra et al. , 1997; Teufel and Moens, 1997; Barzilay and Elhadad, 1997; Carbonell and Goldstein, 1998; Baldwin and Mortbn, 1998; Radev and McKeown, 1998; Strzalkowski et al. , 1998). ::: W00-0405_10:197

P04	Some previous studies on summarization (McKeown and Radev, 1995; Barzilay et al. , 1999; Mani and Bloedorn, 1999) deal with multiple docmnents about a single topic, but not about multiple topics 1. ::: C00-2129_15:198

F02,F01,F05,F03,F04	Statistical methods for calculating the relevance score of each fragment can be categorized into several classes: cue-based (Edmundson, 1969), keywordor frequency-based (Luhn, 1958; Edmundson, 1969; Neto et al., 2000; Steinberger and Jezek, 2004; Kallel et al., 2004; Vanderwende et al., 2007), title-based (Edmundson, 1969; Teufel and Moens, 1997), position-based (Baxendale, 1958; Edmundson, 1969; Lin and Hovy, 1997; Satoshi et al., 2001) and length-based (Satoshi et al., 2001). ::: P10-1095_31:187

M04	In the research presented here, we concentrate on the first step of the summarization process and follow Barzilay and Elhadad (1997) in employing lexical chains to extract important concepts from a document. ::: J02-4004_12:218

E08	These annotations give the list of nuggets covered by each sentence in each citation summary, which are equivalent to the summarization content unit (SCU) as described in (Nenkova and Passonneau, 2004). ::: C10-1101_118:170

P13,F06	Other systems based on information extraction (McKeown et al. , 2002; Radev and McKeown, 1998; White et al. , 2001) and discourse analysis (Marcu, 1999; Strzalkowski et al. , 1999) also exist but they are not yet usable for general-domain summarization. ::: W03-1101_8:160

M01,P03	RST can be used in sentence selection for single document summarization [Marcu, 1997]. ::: W02-0404_50:199

E02	We use ROUGE-1 Recall (Lin and Hovy, 2003) as a fitness function for measuring summarization quality, which is maximized during the optimization procedure. ::: P10-1095_95:187

M24	Sentence Rank is proposed in Mihalcea and Tarau (2004) to make use of only the sentence-tosentence relationships to rank sentences, which outperforms most popular summarization methods. ::: P07-1070_120:166

M22,M12	In addition to sentence fusion, compression algorithms (Chandrasekar, Doran, and Bangalore 1996; Grefenstette 1998; Mani, Gates, and Bloedorn 1999; Knight and Marcu 2002; Jing and McKeown 2000; Reizler et al. 2003) and methods for expansion of a multi parallel corpus (Pang, Knight, and Marcu 2003) are other instances of such methods. ::: J05-3002_471:557

	Many summarization systems (e.g. , (Teufel and Moens, 1997; McKeown et al. , 1999; Lin and Hovy, 2000)) include two levels of analysis: the sentence level, where every textual unit is scored according to c1 c2 c3 c4 c5 t1 1 1 0 1 1 t2 1 0 0 1 0 t3 0 1 0 0 1 t4 1 0 1 1 1 Table 1: Matrix for Summarization Model the concepts or features it covers, and the text level, where, before being added to the final output, textual units are compared to each other on the basis of those features. ::: W04-1017_26:202

	Because documents have pre-defined structures (e.g., sections, paragraphs, sentences) for different levels of concepts in a hierarchy, most recent summarization work has focused on structured probabilistic models to represent the corpus concepts (Barzilay et al., 1999; Daume-III and Marcu, 2006; Eisenstein and Barzilay, 2008; Tang et al., 2009; Chen et al., 2000; Wang et al., 2009). ::: P11-1050_11:197

M23,P04	Radev and McKeown (1998) have already addressed the issue of information fusion in the context of multi document summarization in one specific domain (i.e. , terrorism): The fusion of information is achieved through the implementation of summary operators that integrate the information of different templates from different documents referring to the same event. ::: J02-4005_497:526

E03,E02	This approach allows for variation in the way the content is expressed, which contrasts the Pyramid method with other evaluation methods such as ROUGE that measure word n-gram overlap (Lin and Hovy, 2003). ::: C10-2045_8:188

P03,P04	Some of these approaches to single document summarization have been extended to deal with multi-document summarization(Mani and E.Bloedorn, 1997), (Barzilay et al. , 1999), (McKeown et al. , 1999). ::: C02-1085_197:211

V04,E03,E08	The main summarization evaluation technique in the NIST TAC 2008 summarization track is the pyramid technique (Nenkova and Passonneau 2004), which is a structured human-based evaluation, based on asking human judges to identify summarization content units (SCU) in model and system-generated summaries, and measuring how many SCUs from the model summaries occur in the system summary. ::: J09-4008_126:465

E02	The method, ROUGE (Lin and Hovy, 2003), is based on n-gram overlap between the system-produced and ideal summaries. ::: W04-1017_147:202

F10	Topic signature is used as a novel feature for selecting important content in NeATS (Lin and Hovy, 2002). ::: C10-1128_39:171

E02,V02	We used the ROUGE (Lin and Hovy, 2003) toolkit (i.e.ROUGEeval-1.4.2 in this study) for evaluation, which has been widely adopted by DUC for automatic summarization evaluation. ::: P07-1070_108:166

E02,V02	This recall metric is similar to the ROUGE-1 metric, the unigram version of the ROUGE metric (Lin and Hovy, 2003) used in the Document Understanding Conferences2 (DUC). ::: D08-1057_168:247

M01	Some summarization systems assume that the importance of a sentence is derivable from a rhetorical representation of the source text (Marcu, 1997), while others leverage information from multiple texts to re-score the importance of conceptual units across all the sources (Hatzivassiloglou et al. , 2001). ::: C04-1057_31:156

E02	The third evaluation experiment compared the machine-made and the baseline summaries with the manually created abstracts using ROUGE (Lin 2004)a package for automatically evaluating summaries.This experiment is described in Section 6.4. ::: J10-1003_170:325

P04,M20	It serves as the first component of a domain-independent multi document summarization system \[McKeown et al. 1999\] which generates a summary through text reformulation \[Barzilay et al. 1999\] by combining information from these similar text passages. ::: W99-0625_14:194

	To avoid misleading the reader when juxtaposed passages from different dates all say yesterday, some systems add explicit time stamps (Lin and Hovy 2002a). ::: J02-4001_86:155

E08,E03	Once all SCUs have been identified, the Pyramid method is applied as in (Nenkova and Passonneau, 2004b): we compute as coreD by adding for each SCU present in the summary a score equal to the number of model summaries in which that SCU appears. ::: W06-1643_139:186

V02,E02	We evaluated our summarizer on the DUC test sets using the Rouge automatic scoring metric (Lin and Hovy, 2003). ::: C04-1129_119:206

F10	Lin and Hovy (2000) identified signature terms that were strongly associated with documents based on statistics measures. ::: C08-1124_53:230

E02,P03,P04	Among ROUGE metrics, ROUGE-N (models n-gram cooccurrence, N = 1, 2) and ROUGE-L (models longest common sequence) generally perform well in evaluating both single-document summarization and multi-document summarization (Lin and Hovy, 2003). ::: P08-1093_100:183

M10,M24,M03	The summary sentences can also be selected by using machine learning methods (Kupiec et al., 1995; Amini and Gallinari, 2002) or graph-based methods (ErKan and Radev, 2004; Mihalcea and Tarau, 2004). ::: P10-1094_45:240

E03	Another method for summary evaluation is the Pyramid method (Nenkova and Passonneau, 2004), which takes into account the fact that human summaries with different content can be equally informative. ::: W08-1106_32:197

	But the interpretation of the results is not simple; studies (Jing et al. 1998; Donaway, Drummey, and Mather 2000; Radev, Jing, and Budzikowska 2000) 404 Computational Linguistics Volume 28, Number 4 show how the same summaries receive different scores under different measures or when compared to different (but presumably equivalent) ideal summaries created by humans. ::: J02-4001_108:155

E02	As a result of this exploration, we generate a probability density function (pdf) of the ROUGE score (Lin, 2004) distributions for four different domains, which shows the distribution of the evaluation scores for the generated extracts, and allows us to assess the difficulty of each domain for extractive summarization. ::: N10-1133_17:158

	For example, in machine translation evaluation, approaches such as BLEU (Papineni et al., 2002) use n-gram overlap comparisons with a model to judge overall goodness, with higher n-grams meant to capture fluency considerations. ::: E09-1017_16:229

E02,E03	The authors address a number of shortcomings of manual and automatic summary evaluation methods such as ROUGE (Lin and Hovy, 2003), and argue that the Pyramid method is reliable, diagnostic and predictive. ::: C10-2045_159:188

	(Lin and Hovy, 2003) tried to apply BLEU as a measure to evaluate summaries, but the results were not as good as in Machine Translation. ::: P04-1027_139:190

S01,M09,F01,F07,F03,F10,M02	MEAD is an implementation of the centroid-based method (Radev et al. , 2004) that scores sentences based on sentence-level and inter-sentence features, including cluster centroids, position, TF*IDF, etc. NeATS (Lin and Hovy, 2002) selects important content using Ventence position, term frequency, topic signature and term clustering, and then uses MMR (Goldstein et al. , 1999) to remove redundancy. ::: N06-2046_11:118

M04	Barzilay and Elhadad (1997) constructed lexical chains and extracted strong chains in summaries. ::: C08-1124_58:230

E03,E08	Pyramid (Nenkova and Passonneau, 2004) is a manually evaluated measure of recall on facts or Semantic Content Units appearing in the reference summaries. ::: P11-1049_201:236

F10	These n-gram key concepts are called topic signatures (Lin and Hovy 2000). ::: W03-1101_38:160

E02	For evaluation, we are using the ROUGE evaluation toolkit1, which is a method based on Ngram statistics, found to be highly correlated with human evaluations (Lin and Hovy, 2003a). ::: I05-2004_70:124

E02	Lin (Lin and Hovy, 2003) has found that ROUGE-1 and ROUGE-2 correlate well with human judgments. ::: W05-0905_67:165

M10	WordRank is proposed in Mihalcea and Tarau (2004) to make use of only the co-occurrence relationships between words to rank words, which outperforms traditional keyword extraction methods. ::: P07-1070_155:166

E02	The generated summaries are evaluated using the ROUGE toolkit (Lin and Hovy, 2003). ::: C10-2060_145:225

E03	We use the pyramid evaluation method (Nenkova and Passonneau, 2004) at the sentence level to evaluate the summary created for each set. ::: C10-1101_116:170

E09	The first of these, relative utility (RU) (Radev et al. , 2000) allows model summaries to consist of sentences with variable ranking. ::: H05-1001_112:277

P15,P14,P16,P12,P01	Introduction In the last decade, automatic text summarization has become a popular research topic with a curiously restricted scope of applications.A few innovative research directions have emerged, including headline generation (Soricut and Marcu 2007), summarization of books (Mihalcea and Ceylan 2007), personalized summarization (Daz and Gervas 2007), generation of tables-of-contents (Branavan, Deshpande, and Barzilay 2007), summarization of speech (Fuentes et al.2005), dialogues (Zechner 2002), evaluative text (Carenini, Ng, and Pauls 2006), and biomedical documents (Reeve, Han, and Brooks 2007).In addition, more researchers have been venturing past purely extractive summarization (Krahmer, Marsi, and van Pelt 2008; Nomoto 2007; McDonald 2006).By and large, however, most research in text summarization still revolves around texts characterized by rigid structure.The better explored among such texts are news articles (Barzilay and McKeown 2005), medical documents (Elhadad et al.2005), legal documents (Moens 2007), and papers in the area of computer science (Teufel and Moens 2002; Mei and Zhai 2008).Although summarizing these genres is a formidable challenge in itself, it excludes a continually increasing number of informal documents available electronically.Such documents, ranging from novels to personal Web pages, offer a wealth of information that merits the attention of the text summarization community. ::: J10-1003_5:325

E02,E03	To date, various methods and metrics have been developed for English summary evaluation by comparing system summary with reference summary, such as the pyramid method (Nenkova et al., 2007) and the ROUGE metrics (Lin and Hovy, 2003). ::: P10-1094_178:240

M10,M24,P03,P04,P01	Earlier experiments with graph-based ranking algorithms for text summarization, as previously reported in (Mihalcea and Tarau, 2004) and (Erkan and Radev, 2004), were either limited to single document English summarization, or they were applied to English multi-document summarization, but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone. ::: I05-2004_10:124

M24,P01	TextRank (Mihalcea and Tarau, 2004), (Mihalcea, 2004) is specifically designed to address this problem, by using an extractive summarization technique that does not require any training data or any language-specific knowledge sources. ::: P05-3013_9:74

E02	ROUGE (Lin, 2004), a recall-oriented evaluation package for automatic summarization. ::: E09-1048_187:218

F09	Lexical approaches to term-based summarization use lexical relations to identify central terms (Barzilay and Elhadad, 1997; Gong and Liu, 2002); coreference(or anaphora-) based approaches (Baldwin and Morton, 1998; Boguraev and Kennedy, 1999; Azzam et al. , 1999; Bergler et al. , 2003; Stuckardt, 2003) identify these terms by running a coreferenceor anaphoric resolver over the text.1 We are not aware, however, of any attempt to use both lexical and anaphoric information to identify the main terms. ::: H05-1001_7:277

F01	On the other hand, redundancy can be exploited to identify important and accurate information for applications such as summarization and question answering (Mani and Bloedorn 1997; Radev and McKeown 1998; Radev, Prager, and Samn 2000; Clarke, Cormack, and Lynam 2001; Dumais et al. 2002; Chu-Carroll et al. 2003). ::: J05-3002_8:557

F09,M10,M24	Previous approaches include supervised learning (Hirao et al. , 2002), (Teufel and Moens, 1997), vectorial similarity computed between an initial abstract and sentences in the given document, intra document similarities (Salton et al. , 1997), or graph algorithms (Mihalcea and Tarau, 2004), (Erkan and Radev, 2004), (Wolf and Gibson, 2004). ::: I05-2004_113:124

P28,V02,E02	For the generic, topic-focused and update summarization tasks, the experiment sare perform the DUC data sets using ROUGE-2 and ROUGE-SU (Lin and Hovy, 2003) as evaluation measures. ::: C10-1111_109:176

E02	Although there are automatic corpus-based metrics for summarization such as ROUGE (Lin and Hovy 2003), they do not seem to dominate summarization evaluation in the same way that BLEU-type metrics dominate MT evaluation. ::: J09-4008_125:465

F10	Lin and Hovy (2000) first introduced topic signatures which are topic relevant terms for summarization. ::: W09-1312_37:190

V02,E02	Automated evaluation will utilize the standard DUC evaluation metric ROUGE (Lin, 2004) which representsrecallovervariousn-grams statistics from asystem-generated summary against a set of human generated peer summaries.5 We compute ROUGE scores with and without stop words removed from peer and proposed summaries. ::: N09-1041_23:207

F03	Other approaches to the summarization of news articles make use of the typical journalistic writing style, for example, the fact that the most newsworthy information comes first; as a result, the first few sentences of a news article are good candidates for a summary (Brandow, Mitze, and Rau 1995; Lin and Hovy 1997). ::: J02-4002_24:704

	In general, rules are applied to revise summaries produced by a summarisation system (Mani et al. , 1999; Otterbacher et al. , 2002). ::: W03-1205_191:217

F01,F06	A variety of approaches exist for determining the salient sentences in the text: statistical techniques based on word distribution (Salton et al. , 1991), symbolic techniques based on discourse structure (Marcu, 1997), and semantic relations between words (Barzilay and Elhadad, 1997). ::: P99-1071_44:283

E03	This has been leveraged by Nenkova and Passonneau (2004) to produce a manual scoring method for summaries, though the fact that humans show so little agreement in this task is somewhat disheartening. ::: W04-1016_20:184

	They reported that their method is superior to BLEU (Papineni et al. , 2002) in terms of the correlation between human assessment and automatic evaluation. ::: H05-1019_29:179

M01	Marcu (1997) uses a rhetorical parser to build rhetorical structure trees for arbitrary texts and produces a summary by extracting sentences that span the major rhetorical nodes of the tree. ::: J98-3005_112:614

	Work in this area includes that of Lin and Hovy (2003) and Pastra and Saggion (2003), both of whom inspect the use of Bleu-like metrics (Papineni et al. , 2002) in summarization. ::: W04-1016_14:184

E02	According to (Lin and Hovy, 2003), among all sub-metrics, unigram-based ROUGE (ROUGE-1) has been shown to agree with human judgment most and bigram-based ROUGE (ROUGE-2) fits summarization well. ::: D11-1040_151:262

M18,P23	Text structuring algorithms (Lapata, 2003; Barzilay and Lee, 2004; Karamanis et al. , 2004) are commonly evaluated by their performance at information-ordering. ::: P05-1018_109:224

F01,F02,F03	The score is usually computed based on a combination of statistical and linguistic features, such as term frequency, sentence position, cue words and stigma words (Luhn, 1969; Edmundson, 1969; Hovy and Lin, 1997). ::: C10-1128_28:171

E02	There are also automatic methods for summary evaluation, such as ROUGE (Lin, 2004), which gives a score based on the similarity in the sequences of words between a human-written model summary and the machine summary. ::: W08-1106_41:197

E08	Each fact in the citation summary of a paper is a summarization content unit (SCU) (Nenkova and Passonneau, 2004), and the fact distribution matrix, created by annotation, provides the information about the importance of each fact in the citation summary. ::: C08-1087_139:175

E02	We also tested other automatic methods: content-based evaluation, BLEU (Papineni et al. , 2001) and ROUGE-1 (Lin, 2004), and compared their results with that of evaluation by revision as reference. ::: P06-2078_153:204

E02	ROUGE1 and ROUGE-2 have been shown to have most correlation with human summaries (Lin and Hovy, 2003) and higher order ROUGE-N scores (N > 1) estimate the fluency of summaries. ::: C10-1039_196:303

E02	This restriction is necessary because the problem of optimizing many-to-many alignments 5 Our preliminary experiments with n-gram-based overlap measures, such as BLEU (Papineni et al. 2002) and ROUGE (Lin and Hovy 2003), show that these metrics do not correlate with human judgments on the fusion task, when tested against two reference outputs. ::: J05-3002_200:557

M01	Marcu (1997a) provides some evidence that other genres will deliver less consistency. ::: J02-4001_110:155

S01,P04	We implemented this system, called MEAD*, by 305 adapting MEAD (Radev et al. , 2003), an opensource framework for multi-document summarization. ::: E06-1039_24:233

	BLEU was rst proposed as a supplement (the U in BLEU stands for understudy) for human evaluation (Papineni et al. 2002), but it is now routinely used as the main technique for evaluating research contributions. ::: J09-4008_106:465

E02	For evaluation, we are using the ROUGE evaluation toolkit, which is a method based on Ngram statistics, found to be highly correlated with human evaluations (Lin and Hovy, 2003a). ::: P04-3020_45:103

	At the other extreme, previous approaches (Radev and McKeown 1998) have demonstrated that this task is feasible when a detailed semantic representation of the input sentences is available. ::: J05-3002_109:557

E02	We tested ve automatic corpus-based metrics: two variants of the BLEU metric used in machine translation (Papineni et al. 2002); two variants of the ROUGE metric used in document summarization (Lin and Hovy 2003); and a simple sting-edit distance metric (as a baseline). ::: J09-4008_285:465

P04,P01	It has been observed that in the context of multi document summarization of news articles, extraction may be inappropriate because it may produce summaries which are overly verbose or biased towards some sources (Barzilay et al., 1999). ::: W08-1106_6:197

	Related work Previous work has focused on the analysis of citation and collaboration networks (Teufel et al., 2006; Newman, 2001) and scientific article summarization (Teufel and Moens, 2002). ::: N09-1066_63:220

	First, our method focuses on subject shift of the documents from the target event rather than the sets of documents from dierent events(Radev et al. , 2000). ::: C02-1085_199:211

E02,V02	ROUGE measure is widely used for evaluation (Lin and Hovy, 2003): the DUC contests usually officially employ ROUGE for automatic summarization evaluation. ::: D11-1040_145:262

E02	We tested several measures, such as ROUGE (Lin, 2004) and the cosine distance. ::: P06-2078_75:204

M10,M24	Recently, graph-based ranking methods, such as LexPageRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004), 1 http://haydn.isi.edu/ROUGE/ have been proposed for multi-document summarization. ::: W09-1312_35:190

F10	Other research rewards passages that include topic words, that is, words that have been determined to correlate well with the topic of interest to the user (for topic-oriented summaries) or with the general theme of the source text (Buckley and Cardie 1997; Strzalkowski et al. 1999; Radev, Jing, and Budzikowska 2000). ::: J02-4001_44:155

P01,M18	To achieve readable summaries, the extracted sentences must be appropriately ordered (Barzilay et al., 2002; Lapata, 2003; Barzilay and Lee, 2004; Barzilay and Lapata, 2005). ::: P10-2060_22:149

S01,P04	To do this, we adapted MEAD (Radev et al. , 2003), an open-source framework for multi document summarization, to suit our purposes. ::: E06-1039_54:233

F06,M14	Much more attention has been devoted to discourse-level constraints on adjacent sentences indicative of coherence and good text flow (Lapata, 2003; Barzilay and Lapata, 2008; Karamanis et al., to appear). ::: E09-1017_14:229

	This method of evaluation has already been used in other summarization evaluations such as Edmundson (1969) and Marcu (1997). ::: J02-4005_417:526

M01	Related to this is the work by Teufel and Moens (2002) on rhetorical classification for content selection. ::: W09-2807_101:117

M04,P03	Since each of these scores has a different range of values, we perform ranking based on each score separately, then induce total ranking by summing ranks from individual categories: Rank (theme) = Rank (Number of sentences in theme) + Rank (Similarity of sentences in theme) + Rank (Sum of lexical chain scores in theme) Lexical chains sequences of semantically related wordsare tightly connected to the lexical cohesive structure of the text and have been shown to be useful for determining which sentences are important for single-document summarization (Barzilay and Elhadad 1997; Silber and McCoy 2002). ::: J05-3002_89:557

E02	We verify that our method generates summaries that are significantly better than the baseline results in terms of ROUGE score (Lin, 2004) and subjective readability measures. ::: P10-2060_29:149

E02	An automatic evaluation package, i.e., ROUGE (Lin and Hovy, 2003) is employed to evaluate the summarization performance. ::: C08-1124_156:230

E02	For ROUGE-S and ROUGE-SU, we use three variations following (Lin, 2004b): the maximum skip distances are 4, 9 and infinity 7. ::: H05-1019_124:179

M01	This technique is the focus of one of the articles in this special issue (Teufel and Moens 2002), which shows how particular types of rhetorical relations in the genre of scientific journal articles can be reliably identified through the use of classification. ::: J02-4001_47:155

E02	ROUGE is a method based on Ngram statistics, found to be highly correlated with human evaluations (Lin and Hovy, 2003).1 Throughout the paper, the evaluations are reported using the ROUGE1 setting, which seeks unigram matches between the generated and the reference summaries, and which was found to have high correlation with human judgments at a 95% confidence level. ::: D07-1040_57:286

E02	The results of the comparison with ROUGE-N (Lin and Hovy, 2003; Lin, 2004a; Lin, 2004b), ROUGE-S(U) (Lin, 2004b; Lin and Och, 2004) and ROUGE-L (Lin, 2004a; Lin, 2004b) show that our method correlates more closely with human evaluations and is more robust. ::: H05-1019_21:179

P02,P01	Non-extractive systems are less common previous related work included reformulation of extracted models (McKeown et al. , 1999), gist extraction (Berger and Mittal, 2000), machine translation-like approaches (Witbrock and Mittal, 1999) and generative models (De Jong, 1982; Radev and McKeown, 1998; Fum et al. , 1986; Reihmer and Hahn, 1988; Rau et al. , 1989). ::: C04-1143_60:140

E02	In all our experiments we used the ROUGE (Lin, 2004) evaluation package and its ROUGE1, ROUGE-2, and ROUGE-SU4 recall scores. ::: N10-1133_150:158

M20	Association for Computational Linguistics Computational Linguistics Volume 35, Number 4 and automatic evaluation metrics in machine translation and document summarization (Doddington 2002; Papineni et al. 2002; Lin and Hovy 2003), much less is known about how well automatic metrics correlate with human judgments in NLG. ::: J09-4008_16:465

M10	This is due to the fact that LexPageRank ranks the sentence using eigenvector centrality which implicitly accounts for information subsumption among all sentences (Erkan and Radev, 2004). ::: P09-2075_99:106

E02	Summaries using Lexical Overlap ROUGE (Lin 2004) is a package for automatically evaluating summaries.Given one or more gold-standard summaries (usually written by people), ROUGE offers several metrics for evaluating the summary in question.The metrics reward lexical overlap between the model summaries and the candidate one.Depending on the metric, the lexical units taken into consideration are n-grams, word sequences, and word pairs. ::: J10-1003_289:325

M04	To select these, we use the idea of strong chains introduced by Barzilay and Elhadad (1997). ::: J02-4004_96:218

E02,V02	For evaluation we use ROUGE (Lin, 2004) SU4 recall metric1, which was among the official automatic evaluation metrics for DUC. ::: P07-2049_41:91

E02	Weakly-constrained algorithms In evaluation with ROUGE (Lin, 2004), summaries are truncated to a target length K. Yih et al.(2007)used as tack decoding with a slight modi cation, which allows the last sentence in a summary to be truncated to a target length. ::: E09-1089_130:254

M01	Some summarization systems assume that the importance of a sentence is derivable from a rhetorical representation of the source text (Marcu, 1997). ::: W04-1017_48:202

P01	It is also notable the study reported in (Lin and Hovy, 2003b) discussing the usefulness and limitations of automatic sentence extraction for summarization, which emphasizes the need of accurate tools for sentence extraction, as an integral part of automatic summarization systems. ::: P04-3020_97:103

P04,M22	The use of sentence fusion in multi document summarization has been extensively explored by Barzilay in her thesis (Barzilay, 2003; Barzilay et al. , 1999), though in the multi-document setting, one has redundancy to fall back on. ::: W04-1016_35:184

E08	In our own annotation of three meetings with SCUs defined as in (Nenkova and Passonneau, 2004a), we found that repetitions and reformulation of the same information are particularly infrequent, and that textual units that express the same content among model summaries are generally originating from the same document sentence (e.g. , in the figure, the first sentence in model 1 and 2 emanate from the same document sentence). ::: W06-1643_132:186

E02	For evaluation, we are using the ROUGE evaluation toolkit1, which is a method based on Ngram statistics, found to be highly correlated with human evaluations (Lin and Hovy, 2003). ::: P05-3013_57:74

E02,V02	Summary of datasets We used the ROUGE toolkit 2 (Lin and Hovy, 2003) for evaluation, which has been widely adopted by DUC for automatic summarization evaluation. ::: C10-1128_108:171

P01	Present summarization systems (Watanabe, 1996: Hovy and Lin, 1997) use such clues to calculate an importance score for each sentence, choose sentences 918 according to the score, and simply put the selected sentences together in order of their occurrences in the original document. ::: P98-2151_78:219

	The resulting outputs are compared to the original input sentences in an MTstyle evaluation under two commonly-used metrics: BLEU (Papineni et al., 2002) and NIST (Doddington, 2002). ::: W11-1606_197:216

	Conceptual units can also be defined out of more basic conceptual units, based on the co-occurrence of important concepts (Barzilay and Elhadad, 1997) or syntactic constraints between representations of concepts (Hatzivassiloglou et al. , 2001). ::: C04-1057_29:156

	From this point of view, some of the measures used in the evaluation of Machine Translation systems, such as BLEU (Papineni et al. , 2002), have been imported into the summarization task. ::: P04-1027_137:190

E03	This is represented by the idea behind the Pyramid evaluation framework (Nenkova and Passonneau, 2004; Passonneau et al., 2005), where different levels of the pyramid represent the proportion of concepts (Summary Content Units, or SCUs) mentioned by 1 to n summarizers in summaries of the same text. ::: W11-0504_12:129

E02	In the following, ROUGE-SN denotes ROUGE-S with maximum skip distance N. ROUGE-SU (Lin, 2004) This measure is an extension of ROUGE-S; it adds a unigram as a counting unit. ::: P06-2078_99:204

E03,E02	Nenkova and Passonneau (2004) showed that Pyramids scores produced reliable system rankings when multiple (4 or more) models were used and that Pyramids rankings correlate with rankings produced by ROUGE-2 and ROUGE-SU2 (i.e. ROUGE with skip bi-grams). ::: C10-2122_30:160

P01,P13,M12,P04	Other researchers have investigated the topic of automatic generation of abstracts, but the focus has been different, e.g., sentence extraction (Edmundson, 1969; Johnson et al, 1993; Kupiec et al. , 1995; Mann et al. , 1992; Teufel and Moens, 1997; Zechner, 1995), processing of structured templates (Paice and Jones, 1993), sentence compression (Hori et al. , 2002; Knight and Marcu, 2001; Grefenstette, 1998, Luhn, 1958), and generation of abstracts from multiple sources (Radev and McKeown, 1998). ::: W03-0501_19:177

	Our results improve on the results of Teufel and Moens (2002) (reproduced in Table 7) both overall and for each individual category. ::: N07-1040_149:155

E02	We use ROUGE (Lin, 2004b) to quantitatively assess the agreement of Opinosis summaries with human composed summaries. ::: C10-1039_193:303

F06	Teufel and Moens (2002) identify discourse relations on a sentence-by-sentence basis without presupposing an explicit discourse structure. ::: J10-3005_102:601

E02	We use the publicly available ROUGE toolkit (Lin, 2004)to compute recall, precision, andF-scorefor ROUGE-1. ::: P09-1024_218:261

E02	Among them, ROUGE5 (Lin and Hovy, 2003) is supposed to produce the most reliable scores in correspondence with human evaluations. ::: C08-1062_164:213

M04	First, previous methods for computing lexical chains have either been manual (Morris and Hirst 1991) or automated, but with exponential efficiency (Hirst and St.-Onge 1997; Barzilay and Elhadad 1997). ::: J02-4004_16:218

S01,P01	MEAD summarizer The MEAD summarizer [Radev et al, 2000] [Radev et al 2002] is based on sentence extraction and uses a linear combination of three features to rank the sentences in the source documents. ::: W02-0404_94:199

P13	Extracts are often useful in an information retrieval environment since they give users an idea as to what the source document is about (Tombros and Sanderson 1998; Mani et al. 1999), but they are texts of relatively low quality. ::: J02-4002_10:704

E09	Utility (RU) (Radev et al. , 2000) is tested on a large corpus for the first time in this project. ::: P03-1048_86:179

M11	We evaluate intrinsically by comparing to human-annotated attribution, and extrinsically by showing that automatically acquired knowledge about scientific attribution improves performance on a discourse classification task Argumentative Zoning (Teufel and Moens, 2002), where sentences are labelled as one of fOwn, Other, Background, Textual, Aim, Basis, Contrastg according to their role in the authors argument. ::: N07-1040_41:155

E03,E02	Two metrics have become quite popular in multi-document summarization, namely the Pyramid method (Nenkova and Passonneau, 2004b) and ROUGE (Lin, 2004). ::: W06-1643_128:186

M04,F01,F06,F02,P01	Previous related work on extractive systems included the use of semantic tagging and coreference/lexical chains (Saggion et al. , 2003; Barzilay and Elhadad, 1997; Azzam et al. , 1998), lexical occurrence/structural statistics (Mathis et al. , 1973), discourse structure (Marcu, 1998), cue phrases (Luhn, 1958; Paice, 1990; Rau et al. , 1994), positional indicators (Edmunson, 1964) and other extraction methods (Kuipec et al. , 1995). ::: C04-1143_59:140

	Evaluation We evaluated the quality of the headlines using ROUGE (Lin and Hovy, 2003). ::: D10-1050_180:270

E02	For the intrinsic evaluation of a large number of summaries, we made use of the ROUGE metrics that has been widely used in automatic evaluation of summarization systems (Lin and Hovy, 2003; Hickl et al., 2007). ::: W09-1312_139:190

M10,M24	Ranking algorithms, such as Kleinbergs HITS algorithm (Kleinberg, 1999) or Googles PageRank (Brin and Page, 1998), have been traditionally and successfully used in Web-link analysis (Brin and Page, 1998), social networks, and more recently in text processing applications (Mihalcea and Tarau, 2004), (Mihalcea et al. , 2004), (Erkan and Radev, 2004). ::: I05-2004_17:124

F10	Log-likelihood ratio for words in the input Number of topic signature words (Lin and Hovy, 2000; Conroy et al., 2006) and percentage of signature words in the vocabulary. ::: E09-1062_69:211

F07,F03,F02,F05,M15,M01	Since then several methods and theories have been applied, including the use of term frequency inverse document frequency (TF IDF) measures, sentence position, and cue and title words (Luhn 1958; Edmundson 1969; Kupiec, Pedersen, and Chen 1995; Brandow, Mitze, and Rau 1995); partial understanding using conceptual structures (DeJong 1982; Tait 1982); bottom-up understanding, top-down parsing, and automatic linguistic acquisition (Rau, Jacobs, and Zernik 1989); recognition of thematic text structures (Hahn 1990); cohesive properties of texts (Benbrahim and Ahmad 1995; Barzilay and Elhadad 1997); and rhetorical structure theory (Ono, Sumita, and Miike 1994; Marcu 1997). ::: J02-4005_466:526

	In this task we assume a document collection D consisting of documents D1,,Dn describing the same (or closely related) narrative (Lapata, 2003). ::: N09-1041_18:207

S01,P01	We thus adapt some state-of-the-art conventional summarization methods implemented in the MEAD toolkit (Radev et al., 2003)4 to obtain three baseline methods: (1) LEAD: It simply extracts sentences from the beginning of a paper, i.e., sentences in the abstract or beginning of the introduction section; we include LEAD to see if such leading sentences re ect the impact of a paper as authors presumably would expect to summarize a papers contributions in the abstract. ::: P08-1093_104:183

	However, recent progress on this problem (Marcu 1997) is encouraging. ::: P99-1072_39:157

E02	We include both subjective evaluation from 3 evaluators based on their personalized interests and preference, and the objective evaluation based on the widely used ROUGE metrics (Lin and Hovy, 2003). ::: D11-1124_146:230

S01,F01,M02	MEAD (Radev et al., 2004) and NeATS (Lin and Hovy, 2002) are such implementations, using position and term frequency, etc. MMR (Goldstein et al., 1999) algorithm is used to remove redundancy. ::: D11-1124_37:230

P03,F01,F03,F02,F10	For single document summarization, the sentence score is usually computed by empirical combination of a number of statistical and linguistic feature values, such as term frequency, sentence position, cue words, stigma words, topic signature (Luhn 1969; Lin and Hovy, 2000). ::: P10-1094_44:240

P03	Revision of single-document summaries [Mani et al, 1999] focused on the revision of single-document summaries in order to improve their informativeness. ::: W02-0404_66:199

E02	Based on this information, it is possible to select one or more of the outputs as the gold standard and compare the rest in the pyramid scoring scheme described by Nenkova and Passonneau (2004). ::: W04-1016_90:184

E01,D02	For the Amazon corpus, we also report a coarser metric which measures extraction precision and recall while ignoring labels (Binary labels) as well as ROUGE (Lin, 2004). ::: D10-1037_189:231

E02,V02	As ROUGE (Lin, 2004) has been officially adopted for DUC evaluations since 2004, we also take it as our main evaluation criterion. ::: P11-1052_157:238

P01,P02,M23,M12	Extractive summarization is a simple but robust method for text summarization and it involves assigning saliency scores to some units (e.g. sentences, paragraphs) of the documents and extracting those with highest scores, while abstraction summarization usually needs information fusion (Barzilay et al., 1999), sentence compression (Knight and Marcu, 2002) and reformulation (McKeown et al., 1999). ::: D08-1079_35:157

P04	Topic-oriented multi-document summarization has already been studied in other evaluation initiatives which provide testbeds to compare alternative approaches (Over, 2003; Goldstein et al. , 2000; Radev et al. , 2000). ::: P04-1027_18:190

E02	ROUGE-L (Lin, 2004) This measure evaluates summaries by longest common subsequence (LCS) defined by Equation 4. ::: P06-2078_94:204

E02	Following the successful applic ation of automatic evaluation methods, such as BLEU (Papineni et al. , 2001), in machine translation evaluation, Lin and Hovy (2003) showed that methods similar to BLEU, i.e. n-gram co-occurrence statistics, could be applied to evaluate summaries. ::: W04-1013_14:203

E02,E03	Generally, methods for reducing redundancy are evaluated using ROUGE (Lin, 2004), BE (Hovy et al., 2005), or Pyramid (Nenkova and Passonneau, 2004). ::: W10-3903_162:197

E02,E03,V02	We used the ROUGE-1.5.5 (Lin and Hovy, 2003) toolkit for evaluation, which has been widely adopted by DUC and TAC for automatic summarization evaluation. ::: P11-1155_139:192

