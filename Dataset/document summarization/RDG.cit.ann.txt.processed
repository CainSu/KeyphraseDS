One semiautomatic approach to evaluation is ROUGE (Lin and Hovy 2003) which is primarily based on ngram co-occurrence between automatic and human summaries 
Automatic text summarization approaches have offered reasonably well-performing approximations for identifiying important sentences (Lin and Hovy 2002 Schiffman et al 2002 Erkan and Radev 2004 Mihalcea and Tarau 2004 Daume III and Marcu 2006) but not surprisingly text (re)generation has been a major challange despite some work on sub-sentential modification (Jing and McKeown 2000 Knight and Marcu 2000 Barzilay and McKeown 2005) 
We selected TextRank as it has a performance competitive with the top systems participating in DUC 02 (Mihalcea and Tarau 2004) 
These methods usually represent the documents as term-sentence matrices (where each row represents a sentence and each column represents a term) or graphs (where each node is a sentence and each edge represents the pairwise relationship among corresponding sentences) and ranks the sentences according to their scores calculated by a set of predefined features such as term frequency inverse sentence frequency (TF-ISF) (Radev et al 2004 Lin and Hovy 2002) sentence or term position (Yih et al 2007) and number of keywords (Yih et al 2007) 
The features used for the experiments reported here are inspired by previous work in text summarization on content selection (Kupiec et al 1995) rhetorical classification (Teufel and Moens 2002) and information ordering (Lapata 2003) 
Graph-based methods (Erkan and Radev 2004 Wan et al 2007b) have also been proposed to rank sentences or passages based on the PageRank algorithm or its variants 
Table 1 Sentence scoring metrics Name Description Source POS F Closeness to the beginning of the document 1i (Edmundson 1969) POS L Closeness to the end of the document i (Baxendale 1958) POS B Closeness to the borders of the document max(1i 1ni+1) (Lin and Hovy 1997) LEN W Number of words in the sentence (Satoshi et al 2001) LEN CH Number of characters in the sentence5 LUHN maxi{clusters(S)}{CSi} CSi = W2iNi (Luhn 1958) KEY Sum of the keywords frequencies summation textt{Keywords(S)} tf(t) (Edmundson 1969) COV Ratio of keywords number (Coverage) |Keywords(S)||Keywords(D)| (Liu et al 2006a) TF Average term frequency for all sentence words summationtext tS tf(t) N (Vanderwende et al 2007) TFISF summationtexttS tf(t) isf(t) isf(t) = 1 log(n(t))log(n)  (Neto et al 2000) n(t) is the number of sentences containing t SVD Length of a sentence vector in 2 V T after computing Singular Value (Steinberger and Jezek 2004) Decomposition of a term by sentences matrix A = UV T TITLE O Overlap similarity6 to the title sim(ST) = |ST|min{|S||T|} (Edmundson 1969) TITLE J Jaccard similarity to the title sim(ST) = |ST||ST| TITLE C Cosine similarity to the title sim(vectorS vectorT) = cos(vectorS vectorT) = vectorSvectorT|vectorS||vectorT| D COV O Overlap similarity to the document complement new sim(SD S) = |ST|min{|S||DS|} D COV J Jaccard similarity to the document complement sim(SD S) = |ST||SDS| D COV C Cosine similarity to the document complement cos(vectorS vectorD S) = vectorS vectorDS|vectorS|| vectorDS| LUHN DEG Graph-based extensions of LUHN KEY and COV measures respectively 
The graph-based baselines for single document summarization are described as follows BasicRank This baseline approach adopts the basic PageRank algorithm to rank sentences based on all sentence relationships in a single document similar to previous work (Mihalcea and Tarau 2004) 
These summarizers have been found to produce quantitatively similar results and both significantly outperform a baseline summarizer which is the MEAD summarization framework with all options set to the default (Radev et al 2000) 
The evaluation corpus used is the one from Teufel and Moens (2002) 
(Lin 2004) Skip-bigram is any pair of words in their sentence order allowing for arbitrary gaps 
We used the ROUGE evaluation approach (Lin and Hovy 2003) which is based on n-gram cooccurrence between machine summaries and ideal human summaries 
To evaluate our system we use the pyramid evaluation method (Nenkova and Passonneau 2004) at sentence level 
In summarization such words are called signature terms and are thought to be descriptive of the input they can be identified using the log-likelihood ratio of each word (Lin and Hovy 2000 Gupta et al 2007) 
We used ROUGE (Lin 2004) for evaluating the content of summaries 
Existing work in abstractive summarization has been quite limited and can be categorized into two categories (1) approaches using prior knowledge (Radev and McKeown 1998) (Finley and Harabagiu 2002) (DeJong 1982) and (2) approaches using Natural Language Generation (NLG) systems (Saggion and Lapalme 2002) (Jing and McKeown 2000) 
Repetition in the input is often exploited as an indicator of importance by different summarization approaches (Luhn 1958 Barzilay et al 1999 Radev et al 2004 Nenkova et al 2006) 
Alternatively a summarizer may reward passages that occupy important positions in the discourse structure of the text (Ono Sumita and Miike 1994 Marcu 1997b) 
For example work on coherence in NLG (Lapata 2003) could potentially inform summarization evaluation 
Vectors contain only topic signatures from the input and all words of the summary Topic signatures are words highly descriptive of the input as determined by the application of loglikelihood test (Lin and Hovy 2000) 
In particular ROUGE-2 is the recall in bigrams with a set of human-written abstractive summaries (Lin 2004) 
TextRank (Mihalcea and Tarau 2005) and LexPageRank (Erkan and Radev 2004) use algorithms similar to PageRank and HITS to compute sentence importance 
Such words are called signature terms in Lin and Hovy (2000) who were the first to introduce the log-likelihood weighting scheme for summarization 
Luckily a variety of discourse theories have been developed over the years (eg Mann and Thompson 1988 Grosz Weinstein and Joshi 1995 Halliday and Hasan 1976) and have found application in summarization (Barzilay and Elhadad 1997 Marcu 2000 Teufel and Moens 2002) and other text generation applications (Scott and de Souza 1990 Kibble and Power 2004) 
Following the current practice in evaluating summarization particularly DUC3 we use the ROUGE evaluation package (Lin and Hovy 2003) 
More extensive experimental results with the TextRank system are reported in (Mihalcea and Tarau 2004) (Mihalcea 2004) 
Barzilay and Elhadad (1997) describe a technique for building lexical chains for extractive text summarization 
ROUGE evaluation Table 4 presents ROUGE scores (Lin 2004) of each of human-generated 250-word surveys against each other 
Typical existing summarization methods include centroid-based methods (eg MEAD (Radev et al 2004)) graph-ranking based methods (eg LexPageRank (Erkan and Radev 2004)) non-negative matrix factorization (NMF) based methods (eg (Lee and Seung 2001)) Conditional random field (CRF) based summarization (Shen et al 2007) and LSA based methods (Gong and Liu 2001) 
The third part of the evaluation uses two ROUGE metrics (Lin 2004) to compare the machine-made and the baseline summaries with the model abstractsThe results suggest that these measures are not well suited for evaluating extractive indicative summaries of short stories 
ROUGE (Lin and Hovy 2003) metrics is used for evaluation1 and we mainly concern about ROUGE1 
As mentioned in Section 2 previous summarization work has mainly focused on cohesion (Sjorochodko 1972 Barzilay and Elhadad 1997) or global discourse structure (Marcu 2000 Daume III and Marcu 2002) 
We use the Pyramid model (Nenkova and Passonneau 2004) to value different summary factoids 
Recently graph-based ranking methods have been proposed for sentence ranking and scoring such as LexRank (Erkan and Radev 2004) and TextRank (Mihalcea and Tarau 2004) 
For instance Boguraev and Kennedy (1997) represent cohesion in terms of anaphoric relations whereas Barzilay and Elhadad (1997) operationalize cohesion via lexical chains sequences of related words spanning a topical unit (Morris and Hirst 1991) 
This is similar to Relative Utility (Radev et al  2003) 
Early studies used text similarity measures such as cosine similarity (with or without weighting schema) to compare peer and model summaries (Donaway et al 2000) various vocabulary overlap measures such as set of n-grams overlap or longest common subsequence between peer and model have also been proposed (Saggion et al 2002 Radev et al 2003) 
Graph-based ranking methods such as PageRank (Page et al 1998) and HITS (Kleinberg 1999) have recently applied and been successfully used for multi-document summarization (Erkan and Radev 2004 Mihalcea and Tarau 2005) 
This is similar to the idea of topic signature introduced in (Lin and Hovy 2000) 
In the task of single document summarization various features have been investigated for ranking sentences in a document including term frequency sentence position cue words stigma words and topic signature (Luhn 1969 Lin and Hovy 2000) 
Most research on single document summarization particularly for domain independent tasks uses sentence extraction to produce a summary (Lin and Hovy 1997 Marcu 1997 Salton et al  1991) 
These include both unsupervised sentence ranking (Luhn 1958 Radev and Jing 2004 Erkan and Radev 2004) and supervised methods (Ouyang et al 2007 Shen et al 2007 Li et al 2009) 
System overview 31 Signature Terms Extraction There are signature terms for different topic texts (Lin and Hovy 2000) 
One could rely on existing trainable sentence selection (Kupiec et al 1995) or even phrase selection (Banko et al 2000) strategies to pick up appropriate is from the document to be abstracted and rely on recent information ordering techniques to sort the i fragments (Lapata 2003) 
(Papineni et al 2002) is a system for automatic evaluation of machine translation 
Nowadays a widespread summarization evaluation framework is ROUGE (Lin and Hovy 2003) which as we have mentioned before offers a set of statistics that compare peer summaries with models 
As the features Lapata (2003) proposed the Cartesian product of content words in adjacent sentences 
Empirical evaluations using two standard summarization metricsthe Pyramid method (Nenkova and Passonneau 2004b) and ROUGE (Lin 2004)show that the best performing system is a CRF incorporating both order-2 Markov dependencies and skip-chain dependencies which achieves 913% of human performance in Pyramid score and outperforms our best-performing non-sequential model by 39% 
With our best performing features we get ROUGE-2 (Lin 2004) scores of 011 and 00925 on 2007 and 2006 5This threshold was derived experimentally with previous data 
The first two of these scores are produced by Simfinder and the salience score is computed using lexical chains (Morris and Hirst 1991 Barzilay and Elhadad 1997) as described below 
According to (Lin and Hovy 2003) among all sub-metrics in ROUGE ROUGE-N (N=1 2) is relatively simple and works well 
The first partition aims to develop manual evaluation criteria for determining the quality of a summary and is typified by the extensive research done in single-document summarization by Halteren and Teufel (2003) and by the evaluation strategy proposed by Nenkova and Passonneau (2004) 
Automatic evaluation was performed with ROUGE (Lin 2004) a widely used and recognized automated summarization evaluation method 
We do not distinguish between the two types of links stated above but only identify which citation(s) a linguistic expression is attributable 1We use a list of around 40 research methodology related nouns from Teufel and Moens (2002) such as eg study account investigation result etc These are nouns we are particularly interested in 
To demonstrate the use of automatic scientific attribution classification we studied its utility for one well known discourse annotation task Argumentative Zoning (Teufel and Moens 2002) 
Barzilay and Elhadad (1997) proposed lexical chains as an intermediate step in the text summarization process 
Comparing the Machine-Made Summaries and the Manually Created Extracts Measuring sentence co-selection between extractive summaries created by humans and those created by automatic summarizers has a long tradition in the text summarization community (Lin and Hovy 2000 Marcu 2000) but this family of measures has a number of well-known shortcomingsAs many have remarked on previous occasions (Mani 2001 Radev et al2003) co-selection measures do not provide a complete assessment of the quality of a summaryFirst of all when a summary in question contains sentences that do not appear in any of the model extracts one may not be sure that those sentences are uninformative or inappropriate for inclusion in a summaryIn addition documents have internal discourse structure and sentences are often inter-dependentTherefore even if a summary contains sentences found in one or more reference summaries it does not always mean that it is advisable to include those sentences in the summary in question 
In this respect this is similar to work by Lapata (2003) who builds a conditional model of words across adjacent sentences focusing on words in particular semantic roles 
Evaluation We evaluated summarization quality using ROUGE (Lin and Hovy 2003) 
Evaluation metrics such as BLEU (Papineni et al 2002) have a built-in preference for shorter translations 
The ROUGE method (Lin and Hovy 2003) is based on n-gram overlap between the system-produced and ideal summaries 
Most recently the graph-based models have been successfully applied for multi-document summarization by making use of the voting or recommendations between sentences in the documents (Erkan and Radev 2004 Mihalcea and Tarau 2005 Wan and Yang 2006) 
A straightforward approach for approximating sentence fusion can be found in the use of sentence extraction for multi document summarization (Carbonell and Goldstein 1998 Radev Jing and Budzikowska 2000 Marcu and Gerber 2001 Lin and Hovy 2002) 
We use only the words that are content words (nouns verbs or adjectives) and not in the stopword list used in ROUGE (Lin 2004) 
Erkan and Radev (2004) and Yoshioka (2004) evaluate the relevance (similarity) between any two sentences first 
The dif1The routinely used tool for automatic evaluation ROUGE was adopted exactly because it was demonstrated it is highly correlated with the manual DUC coverage scores (Lin and Hovy 2003a Lin 2004) 
Like Mani and Barzil~'s techniques our approach focuses on the problem that how to identi~" differences and similarities across documents rather than the problem that how to form the actual summar (Sparck 1993) (McKeown and Radev 1995) (Radev and McKeown 1998) 
ROUGE evaluation NIST also evaluated the summaries automatically using ROUGE (Lin 2004 Lin and Hovy 2003) 
Among these different scores unigram-based ROUGE score (ROUGE-1) has been shown to agree with human judgment most (Lin and Hovy 2003) 
Responsiveness differs from other measures of summary content such as SEE coverage (Lin and Hovy 2002) and Pyramid scores (Nenkova and Passonneau 2004) in that it does not compare a peer summary against a set of known human summaries 
LexPageRank (Erkan and Radev 2004) is an approach for computing sentence importance based on the concept of eigenvector centrality 
Official DUC scoring utilizes the jackknife procedure and assesses significance using bootstrapping resampling (Lin 2004) 
In machine translation the rankings from the automatic BLEU method (Papineni et al  2002) have been shown to correlate well with human evaluation and it has been widely used since and has even been adapted for summarization (Lin and Hovy 2003) 
For our experiment we have only considered unigrams (lemmatized words excluding stop words) which gives good results with standard summaries (Lin and Hovy 2003) 
In recent years graph-based ranking algorithms have been successfully used for document summarization (Mihalcea and Tarau 2004 2005 ErKan and Radev 2004) and keyword extraction (Mihalcea and Tarau 2004) 
Machine learning has also been applied to learning individual features for example Lin and Hovy (1997) applied machine learning to the problem of determining how sentence position affects the selection of sentences and Witbrock and Mittal (1999) used statistical approaches to choose important words and phrases and their syntactic context 
Predicates such as to present and to include have the tendency of appearing towards the very beginning or the very end of the abstract been therefore predicted by position-based features (Edmundson 1969 Lin and Hovy 1997) 
Pyramid approach was introduced by Nenkova and Passonneau (2004) as a method for evaluating machine-generated summaries based on a set of human model summaries 
For the extractive or abstractive summaries we use ROUGE scores (Lin 2004) a metric used to evaluate automatic summarization performance to measure the pairwise agreement of summaries from different annotators 
As in (Radev et al  2000) in order to create an extract of a certain length we simply extract the top scoring sentences that add up to that length 
Summary of datasets We used the ROUGE (Lin and Hovy 2003) toolkit (ie ROUGEeval-142 in this study) for evaluation which has been widely adopted by DUC for automatic summarization evaluation 
MEAD (Radev et al  2000) MEAD is a centroid-based extractive summarizer that scores sentences based on sentence-level and inter-sentence features which indicate the quality of the sentence as a summary sentence 
The fact that summary quality improves with increasing summary length has been observed in prior studies as well (Radev and Tam 2003 Lin and Hovy 2003b Kolluru and Gotoh 2005) but generally little attention has been paid to this fact in system development and no specific user studies are available to show what summary length might be most suitable for specific applications 
ROUGE-2 (based on bigrams) and ROUGE-SU4 (based on both unigrams and skip-bigrams separated by up to four words) are given by the official ROUGE toolkit with the standard options (Lin 2004) 
For example Hirst and St-Onge (1998) greedily disambiguate a word as soon as it is encountered by selecting the sense most strongly related to existing chain members whereas Barzilay and Elhadad (1997) consider all possible alternatives of word senses and then choose the best one among them 
In this paper we show that a method for extractive summarization relying on iterative graph-based algorithms as previously proposed in (Mihalcea and Tarau 2004) can be applied to the summarization of documents in different languages without any requirements for additional data 
Later work merged information extraction approaches with regeneration of extracted text to improve summary generation (Radev and McKeown 1998) 
We employed a number of ROUGE variants which have been proven to correlate with human judgments in multi-document summarization (Lin 2004) 
The Bleu machine translation evaluation measure (Papineni et al 2002) has also been tested in summarization (Pastra and Saggion 2003) 
Lapata (2003) employed the probability of two sentences being adjacent as determined from a corpus 
It bases on N-gram cooccurrence and compares the system generated summaries to human judges (Lin and Hovy 2003) 
In the field of multi-document summarization (MDS) the Pyramid method has become an important approach for evaluating machine generated summaries (Nenkova and Passonneau 2004 Passonneau et al 2005 Nenkova et al 2007) 
Perhaps the most relevant is work by (Teufel 1999 Teufel and Moens 2002) who defined and studied argumentative zoning of texts especially ones in computational linguistics 
More recently Mihalcea and Tarau (2004) propose the TextRank model to rank keywords based on the co-occurrence links between words 
Lapata (2003) presented a methodology for automatically evaluating generated orderings on the basis of their distance from observed sentence orderings in a corpus 
By analyzing rhetorical discourse structure of aim background solution etc or citation context we can obtain appropriate abstracts and the most influential contents from scientific articles (Teufel and Moens 2002 Mei and Zhai 2008) 
Hovy and Lin (1997) present another system that learned how the position of a sentence affects its suitability for inclusion in a summary of the document 
Among them ROUGE-1 has been shown to agree most with human judgments (Lin and Hovy 2003) 
First two estimates of importance on words have been used very successfully both in generic and query-focused summarization frequency (Luhn 1958 Nenkova et al  2006 Vanderwende et al  2006) and loglikelihood ratio (Lin and Hovy 2000 Conroy et al  2006 Lacatusu et al  2006) 
Unlike many other methods that directly utilize noun phrase (NP) coreference (Nenkova 2008 Mani et al 1999) we propose a method that employs insertion and substitution of phrases that modify the same chunk in the lead and other sentences 
The centroid-based method MEAD (Radev et al 2004) is an implementation of the centroid based method that scores sentences based on features such as cluster centroids position and TFIDF etc NeATS (Lin and Hovy 2002) adds new features such as topic signature and term clustering to select important content and use MMR (Goldstein et al 1999) to remove redundancy 
We decided against using deeper approaches such as the Pyramid method (Nenkova and Passonneau 2004) factoids (van Halteren and Teufel 2003) and relative utility (Radev and Tam 2003)The reason is practical These approaches have an unfortunate disadvantage of being considerably more labor-intensive than the measures based on sentence co-selection 
Conceptual units do not have to be directly observable as text snippets they can represent abstract properties that particular text units may or may not satisfy for example status as a first sentence in a paragraph or generally position in the source text (Lin and Hovy 1997) 
Simply using a variant of the Bilingual Evaluation Understudy (BLEU) scoring method (based on a linear combination of matching n-grams between the system output and the ideal summary) developed for machine translation (Papineni et al 2001) is promising but not sufficient (Lin and Hovy 2002b) 
Co-selection measures include precision and recall of co-selected sentences relative utility (Radev et al  2000) and Kappa (Siegel and Castellan 1988 Carletta 1996) 
Related work (Mani et al  1999) addressed the problem of revising summaries to improve their quality 
For our first approach we used a nugget-based evaluation methodology (Lin and Demner-Fushman 2006 Nenkova and Passonneau 2004 Hildebrandt et al 2004 Voorhees 2003) 
Very briefly the TextRank system (Mihalcea and Tarau 2004) similar in spirit with the concurrently proposed LexRank method (Erkan and Radev 2004) works by building a graph representation of the text where sentences are represented as nodes and weighted edges are drawn using inter-sentential word overlap 
Our best results of Kappa=048 and Macro-F=053 are better than the best previously published results on task (Kappa=045 and Macro-F=050 in Teufel and Moens (2002)) 
The usual strategy employed by domain-specific summarizers is for humans to determine a priori what types of information from the originating documents should be included (eg  in stories about earthquakes the number of victims) (Radev and McKeown 1998 White et al  2001) 
ROUGE (Lin 2004) is widely used for summarization evaluation and it has been shown that ROUGE-N scores are highly correlated with human evaluation (Lin 2004) 
A variety of approaches exist for determining the salient sentences in the text statistical techniques based oll word distribution (Kupiec et al  1995) (Zechner 1996) (Salton et al  1991) (Teufell and Moens 1997) symbolic techniques based on discourse structure (Marcu 1997) and semantic relations between words (Barzil~v and Elhadad 1997) 
Some of these approaches to single document summarization have been extended to deal with multi-document summarization (Mani and Bloedern 1997 Goldstein and Carbonell 1998 TIPSTER 1998b Radev and McKeown 1998 Mani and Bloedorn 1999 McKeown et al !999 Stein et al  1999) 
Lexical chains which capture relationships between related terms in a document have shown promise as an intermediate representation for producing summaries (Barzilay and Elhadad 1997) 
Our aim is not only to determine the utility of citation texts for survey creation but also to examine the quality distinctions between this form of input and others such as abstracts and full texts comparing the results to human-generated surveys using both automatic and nugget-based pyramid evaluation (Lin and Demner-Fushman 2006 Nenkova and Passonneau 2004 Lin 2004) 
More recently summarizers using sophisticated post extraction strategies such as revision (McKeown et al  1999 Jing and McKeown 1999 Mani et al  1999) and sophisticated grammar-based generation (Radev and McKeown 1998) have also been presented 
Most previous work on summarization focused on extractive methods investigating issues such as cue phrases (Luhn 1958) positional indicators (Edmundson 1964) lexical occurrence statistics (Mathis et al  1973) probabilistic measures for token salience (Salton et al  1997) and the use of implicit discourse structure (Marcu 1997) 
In recent years the Pyramids evaluation method (Nenkova and Passonneau 2004) was introduced 
Lin and Hovy (2002) and Lin and Hovy (2003) were the first to systematically point out problems with the large scale DUC evaluation and to look to solutions by seeking more robust automatic alternatives 
These include comparing templates filled in by extracting information using specialized domain specific knowledge sources from the doc"ument and then generating natural language summaries from the templates (Radev and McKeown 1998) com- paring named-entities extracted using specialized lists between documents and selecting the most relevant section (TIPSTER 1998b) finding co-reference chains in the document set to identify common sections of interest (TIPSTER 1998b) or building activation networks of related lexical items (identity mappings synonyms hypernyms etc) 
Summaries can be evaluated manually or with automatic metrics such as ROUGE (Lin 2004) 
We provided more in-depth discussion of this issue in other papers (Lin and Hovy 2002 Lin and Hovy 2003b) 
The understanding can be induced using dependencies between words (Barzilay and Elhadad 1997) rhetorical relations (Paice and Johns 1993) events (Filatova and Hatzivassiloglou 2004) 
While earlier approaches for text compression were based on symbolic reduction rules (Grefenstette 1998 Mani Gates and Bloedorn 1999) more recent approaches use an aligned corpus of documents and their human written summaries to determine which constituents can be reduced (Knight and Marcu 2002 Jing and McKeown 2000 Reizler et al 2003) 
In revising single-document summaries [Mani et al 1999] employed rules such as the referencing of pronouns with the most recently mentioned noun phrase 
Corpus-based methods inspired by the notion of schemata have been explored in the past by Lapata (2003) and Barzilay and Lee (2004) for ordering sentences extracted in a multi-document summarisation application 
DP 167 323 PBMT 186 516 Summ 839 1425 QA 238 202 TE 56 44 Table 1 Clusters and their citation network size 11 Related Work Although there has been work on analyzing citation and collaboration networks (Teufel et al 2006 Newman 2001) and scientific article summarization (Teufel and Moens 2002) to the knowledge of the author there is no previous work that study the text of the citation summaries to produce a summary 
Recently content features were also well studied including centroid (Radev et al 2004) signature terms (Lin and Hovy 2000) and high frequency words (Nenkova e t al 2006) 
A lot of approaches were proposed in text summarization such as word frequency based method (Luhn 1958) cue phrase method (Edmundson 1969) Position based methods (Edmundson 1969 Hovy and Lin 1997 Teufel and Moens 1997) 
MEAD* The extractive approach is represented by MEAD* which is adapted from the open source summarization framework MEAD (Radev et al 2000) 
Other systems exploit the co-occurrence of particular concepts (Barzilay and Elhadad 1997 Lin and Hovy 2000) or syntactic constraints between concepts (McKeown et al  1999) 
Technical paper summarization has also been studied (Paice 1981 Paice and Jones 1993 Saggion and Lapalme 2002 Teufel and Moens 2002) but the previous work did not explore citation context to emphasize the impact of papers 
Moreover summarization evaluation metrics such as Basic Element (Hovy et al 2006) ROUGE (Lin and Hovy 2003) and Pyramid (Passonneau et al 2005) are all counting the concept overlap between generated summaries and human-written summaries 
Volume 36 Number 1 report that people do not agree well on what sentences constitute a good summary of a document (Rath Resnick and Savage 1961 Salton et al1997 Lin and Hovy 2003)In most cases the agreement corresponding to of 042 would not be sufcient for creating a resource but we interpret this level of agreement as acceptable for evaluating a single facet of the summaries that are also evaluated in other ways 
The most commonly used evaluation method for summarization during system development and for reporting results in publications is the automatic evaluation metric ROUGE (Lin 2004 Lin and Hovy 2003) 
By definition (Lin 2004) ROUGE-N is the n-gram recall between a candidate summary and a set of reference summaries 
This strategy is commonly used in multi-document summarization (Barzilay et al 1999 Goldstein et al 2000 Radev et al 2000) where the combination step eliminates the redundancy across selected excerpts 
In recent years graph-based methods have been proposed for sentence ranking (Erkan and Radev 2004 Mihalcea and Tarau 2004) 
Therefore it is difficult to identi~" these information using a single-document summarizer technique (Mani and Bloedorn 1997) (Barzilay et al  1999) 
Graphs have been commonly used for extractive summarization (eg LexRank (Erkan and Radev 2004) and TextRank (Mihalcea and Tarau 2004)) but in these works the graph is often undirected with sentences as nodes and similarity as edges 
Many approaches to summarization can be very broadly characterized as TERM-BASED they attempt to identify the main topics which generally are TERMS and then to extract from the document the most important information about these terms (Hovy and Lin 1997) 
Furthermore to provide some assessment of the quality of the predicted orderings themselves we follow Lapata (2003) in employing Kendalls a7 which is a measure of how much an ordering differs from the OSO the underlying assumption is that most reasonable sentence orderings should be fairly similar to it 
Pyramid evaluation The pyramid evaluation method (Nenkova and Passonneau 2004) has been developed for reliable and diagnostic assessment of content selection quality in summarization and has been used in several large scale evaluations (Nenkova et al 2007) 
Abstraction on the other hand relies either on linguistic processing followed by structural compaction (Mani et al  1999) or on interpretation of the source text into a semantic representation which is then condensed to retain only the most important information asserted in the source 
Quality of summary We use the ROUGE (Lin and Hovy 2003) metric for measuring the summarization system performance 
More recently graph-based methods that rely on sentence connectivity have also been found successful using algorithms such as node degree (Salton et al  1997) or eigenvector centrality (Mihalcea and Tarau 2004 Erkan and Radev 2004 Wolf and Gibson 2004) 
Cohesion information has been used in rhetorical-based parsing for summarization (Marcu 1997) in order to decide between list or elaboration relations and also in content selection for summarization (Barzilay and Elhadad 1997) 
ROUGE is used for performance measure (Lin and Hovy 2003 Lin 2004) which evaluates summaries based on the maxium number of overlapping units between generated summary text and a set of human summaries 
A property that is unique to multi-document summarization is the effect of time perspective (Radev and McKeown 1998) 
Previous studies have shown that it is feasible to evaluate the output of summarization systems automatically (Lin and Hovy 2003) 
(Lin and Hovy 2002) is an extraction based multi-document summarization system 
Previous research has addressed revision in single-document summaries [Jing & McKeown 2000] [Mani et al 1999] and has suggested that revising summaries can make them more informative and correct errors 
Then one can apply the method inspired by (Barzilay et al 1999) to identify common phrases across sentences and use language generation to form a more coherent summary 
Like fact extraction methods (eg  Radev and McKeown 1998) our method also uses a template whose slots are being filled during analysis 
The best possible rank is 0 and the worst is a4a6a5 a40 a39  An additional difficulty we encountered in setting up our evaluation is that while we wanted to compare our algorithms against Lapatas (2003) state-of-the-art system her method doesnt consider all permutations (see below) and so the rank metric cannot be computed for it 
This result is presented as 0053 with the official ROUGE scorer (Lin 2004) 
A statistical model by Lapata (2003) considers both lexical and syntactic features in calculating local coherence 
General text summarization including single document summarization (Luhn 1958 Goldstein et al 1999) and multi-document summarization (Kraaij et al 2001 Radev et al 2003) has been well studied our work is under the framework of extractive summarization (Luhn 1958 McKeown and Radev 1995 Goldstein et al 1999 Kraaij et al 2001) but our problem formulation differs from any existing formulation of the summarization problem 
Lapata (2003) proposed an algorithm that computes the probability of two sentences being adjacent for ordering sentences 
NP-rewrite enhanced frequency summarizer Frequency and frequency-related measures of importance have been traditionally used in text summarization as indicators of importance (Luhn 1958 Lin and Hovy 2000 Conroy et al 2006) 
This is true of automatic summarization systems too which consider the position of a sentence in a document and how it relates to its surrounding sentences (Kupiec Pedersen and Chen 1995 Barzilay and Elhadad 1997 Marcu 2000 Teufel and Moens 2002) 
They report a marginal increase in the automatic word overlap metric ROUGE (Lin 2004) but a decline in manual Pyramid (Nenkova and Passonneau 2004) 
Thus we use MEAD (Radev et al 2000) as our baseline 
The experiments in Lin and Hovy (2003) show that among n-gram approaches to scoring Rouge-1 (based on unigrams) has the highest correlation with human scores 
The idea of topic signature terms was introduced by Lin and Hovy (Lin and Hovy 2000) in the context of single document summarization and was later used in several multi-document summarization systems (Conroy et al 2006 Lacatusu et al 2004 Gupta et al 2007) 
In addition this is the same even if we use the SummBank corpus (Radev et al  2003) 
Because of this it is generally accepted that some kind of postprocessing should be performed to improve the final result by shortening fusing or otherwise revising the material (Grefenstette 1998 Mani Gates and Bloedorn 1999 Jing and McKeown 2000 Barzilay et al 2000 Knight and Marcu 2000) 
The present paper deviates from Lapata (2003) insofar as we do not attempt to learn the ordering preferences between pairs of sentences 
The idea has been formalized in the construct of lexical chains (Barzilay and Elhadad 1997) 
Furthermore automated summarization metrics like ROUGE (Lin and Hovy 2003) are non-trivial to adapt to this domain as they require human curated outputs 
Questions regarding the agreement between people in the area of single document summarization and multi-document summarization have already been raised and are currently only partially answered (Halteren and Teufel 2003 Nenkova and Passonneau 2004 Marcu and Gerber 2001) 
A simple ordering criterion is the chronological order of the events represented in the sentences which is often augmented with other ordering criteria such as lexical overlap (Conroy et al 2006) lexical cohesion (Barzilay et al 2002) or syntactic features (Lapata 2003) 
Other than for email summarization other document summarization methods have adopted graph ranking algorithms for summarization eg (Wan et al 2007) (Mihalcea and Tarau 2004) and (Erkan and Radev 2004) 
The scores are usually computed based on a combination of statistical and linguistic features including term frequency sentence position cue words stigma words topic signature (Hovy and Lin 1997 Lin and Hovy 2000) etc Machine learning methods have also been employed to extract sentences including unsupervised methods (Nomoto and Matsumoto 2001) and supervised methods (Kupiec et al  1995 Conroy and OLeary 2001 Amini and Gallinari 2002 Shen et al  2007) 
More advanced methods for query expansion use topic signatures words and grammatically related pairs of words that model the query and even the expected answer from sets of documents marked as relevant or not (Lin & Hovy 2000 Harabagiu 2004) 
Two of the articles focus on the extraction stage (Teufel and Moens 2002 Silber and McCoy 2002) whereas Jing (2002) examines tools for automatically constructing resources that can be used for the second stage 
In addition infobox could be considered as topic signature (Lin and Hovy 2000) or keywords about the topic 
Finally the last system that we implement is TextRank which uses a variation of the PageRank graph centrality algorithm in order to identify the most important sentences in a document (Page et al 1999 Erkan and Radev 2004 Mihalcea and Tarau 2004) 
In terms of validation a number of studies have claimed that ROUGE correlates with human ratings for example Lin and Hovy (2003) and Dang (2006) 
We compared our results against those of a bigram language model (the baseline) and an improved version of the state-of-the-art probabilistic ordering method of Lapata (2003) both trained on the same data we used 
Human correlations According to (Lin and Hovy 2003) ROUGE1 correlates particularly well with human judgments of informativeness 
In recent years graph-based ranking methods have been investigated for document summarization such as TextRank (Mihalcea and Tarau 2004 Mihalcea and Tarau 2005) and LexPageRank (ErKan and Radev 2004) 
In a recent study (Lin and Hovy 2003a) we showed that the recall-based unigram cooccurrence automatic scoring metric correlates highly with human evaluation and has high recall and precision in predicting the statistical significance of results comparing with its human counterpart 
The large improvements in Aim and 322 Aim Ctr Txt Own Bkg Bas Oth P 44 34 57 84 40 37 52 R 65 20 66 88 50 40 39 F 52 26 61 86 44 38 44 Correctly Classified Instances 725% Kappa statistic 045 Macro-F 050 Table 7 Teufel and Moens (2002)s best AZ results (Naive Bayes Classifier) 
A common approach is to measure similarity between all pairs of sentences and then use clustering to identify themes of common information (McKeown et al 1999 Radev Jing and Budzikowska 2000 Marcu and Gerber 2001) 
ROUGE (Lin 2004) and its linguistically motivated descendent Basic Elements (BE) (Hovy et al 2005) evaluate a summary by computing its overlap with a set of model (human) summaries ROUGE considers lexical n-grams as the unit for comparing the overlap between summaries while Basic Elements uses larger units of comparison based on the output of syntactic parsers 
It is also notable the study reported in (Lin and Hovy 2003b) discussing the usefulness and limitations of automatic sentence extraction for text summarization 23 Single document Meta summarization algorithm summarization algo 
Many NLG researchers are impressed by the BLEU evaluation metric (Papineni et al 2002) in Machine Translation (MT) which has allowed MT researchers to quickly and cheaply evaluate the impact of new ideas algorithms and data sets 
A recent study (Kan et al  2001) uses topic composition from text headers but other studies in the extraction paradigm (Goldstein et al  1999) extraction coupled with rhetorical structural identification (Teufel and Moens 2002) and syntactic abstraction paradigms use different methodologies (Barzilay et al  1999 McKeown et al  1999) 
One is the longest common subsequence (LCS) based approach (Hori et al  2003 Lin 2004a Lin 2004b Lin and Och 2004) 
Barzilay and Elhadad (1997) rank their chains heuristically by a score based on their length and homogeneity 
The learners we used (with default Weka settings) are NB Naive Bayes learner HNB Hidden Naive Bayes learner IBk Memory based learner J48 Decision tree based learner STACKING combining NB and J48 classifiers with the stacking method As mentioned under History feature above we run each learner twice the second time including the machine learning prediction for the previous sentence (as we found in Teufel and Moens (2002) for NB we noticed a slight improvement in performance when using the history feature (between 005 and 01 on both and MacroF for all learners)) 
TextRank (Mihalcea and Tarau 2005) and LexPageRank (Erkan and Radev 2004) use algorithms similar to PageRank and HITS to compute sentence importance 
ROUGE version 155 (Lin 2004) was used for evaluation2 Among others we focus on ROUGE-1 in the discussion of the result because ROUGE-1 has proved to have strong correlation with human annotation (Lin 2004 Lin and Hovy 2003) 
NeATS (Lin and Hovy 2002) makes use of new features such as topic signature to select important sentences 
We now assess the significance of our results by comparing our best system against (1) a lead summarizer that always selects the first N utterances to match the predefined length (2) human performance which is obtained by leave-one-out comparisons among references (Table 7) (3) optimal summaries generated using the procedure explained in (Nenkova and Passonneau 2004b) by ranking document utterances by the number of model summaries in which they appear 
LexPageRank (Erkan and Radev 2004) is an approach for computing sentence importance based on the concept of eigenvector centrality 
Graph-based methods for text summarization work usually at the level of sentences (Erkan & Radev 2004 Mihalcea & Tarau 2004) 
Alternatives to the use 473 Computational Linguistics Volume 24 Number 3 of frequency of key phrases included the identification and representation of lexical chains (Halliday and Hasan 1976) to find the major themes of an article followed by the extraction of one or two sentences per chain (Barzilay and Elhadad 1997) training over the position of summary sentences in the full article (Hovy and Lin 1997) and the construction of a graph of important topics to identify paragraphs that should be extracted (Mitra Singhal and Buckley 1997) 
ROUGE-N (Lin 2004) This measure compares n-grams of two summaries and counts the number of matches 
Next the remaining sentences are ranked by the sum of two individual scores a) an authority score from a lexical PageRank algorithm (Erkan and Radev 2004) and b) a similarity score between the sentence and the Gene Ontology (GO) terms with which the gene is annotated (To date over 190000 genes have two or more associated GO terms) 
In recent years there has been growing interest in evaluating NLG texts by comparing them to a corpus of human-written reference texts using automatic metrics such as string-edit distance tree similarity or BLEU (Papineni et al 2002) this is another type of intrinsic evaluation 
For details see (Radev et al  2000) 
Inspired by the idea of graph based algorithms to collectively rank and select the best candidate research efforts in the natural language community have applied graph-based approaches on keyword selection (Mihalcea and Tarau 2004) text summarization (Erkan and Radev 2004 Mihalcea 2004) word sense disambiguation (Mihalcea et al  2004 Mihalcea 2005) sentiment analysis (Pang and Lee 2004) and sentence retrieval for question answering (Otterbacher et al  2005) 
Note that although the MEAD distribution also includes an optional feature calculated using the LexRank graph-based algorithm (Erkan and Radev 2004) this feature could not be used since it takes days to compute for very long documents such as ours and thus its application was not tractable 
Lapata (2003) proposed most of these features 
Naturally an ideal multi-document summary would include a natural language generation component to create cohesive readable summaries (Radev and McKeown 1998 McKeown et al  1999) 
Evaluation methods are either extrinsic in which the summaries are evaluated based on their quality in performing a specific task (Sparck-Jones 1999) or intrinsic where the quality of the summary itself is evaluated regardless of any applied task (van Halteren and Teufel 2003 Nenkova and Passonneau 2004) 
(Lin and Hovy 2002) is an extraction based multi-document summarization system 
An optimal summary in terms of content selection is obtained by maximizing the sum of SCU weights given a maximum number of SCUs that can be included for a predefined summary length (Nenkova and Passonneau 2004) 
It is also necessary to see whether our genre-specic approach shows any improvements over the existing generic state-of-the-art systems put to work on ctionTo this end we compared our summarizer with two systems that were top performers in the Document Understanding Conference (henceforth DUC) 2007 the annual competition for automatic summarizersIn DUC competitions the summarization systems are evaluated on a variety of metrics manually assigned scores (ranking readability grammaticality non-redundancy referential clarity focus and coherence) the pyramid method (Nenkova and Passonneau 2004) and ROUGE scores (Lin 2004)There is no unied ranking of the systems performance and selecting the best summarizer is not straightforwardWe chose two systems among the top performers in DUC 2007 GISTexter (Harabagiu Hickl and Lacatusu 2007) and CLASSY (Schlesinger OLeary and Conroy 2008 Conroy Schlesinger and OLeary 2007)GISTexter appears to be the best summarizer according to the scores assigned by the human judgesApart from baselines it is consistently ranked as the best or the second-best system on most characteristics evaluated by the judges (the only exception is non-redundancy where GISTexter is ranked eighth)CLASSY on the other hand is one of the four top systems according to ROUGE scoresThe scores it received from the human judges are also quite good 
Features proposed to create the appropriate order include publication date of document (Barzilay et al 2002) content words (Lapata 2003 Althaus et al 2004) and syntactic role of 911 a0a2a1a3a1 a0a0 a0a2a1a4a0a2a1a5 a0 a0a2a1a6 a0 a0a0 a0a0 a5a7a5a0 a5a8a1 a0 a5a9a4 a0 a5a7a6 a0 a5a9a10 a0a11a4a11a1 a0a11a4a12a5 a0a11a4a7a4 a0 a13 a0 a6 Figure 1 Graph representation of summarization 
This is specific to their approach as both Lapata (2003)s and Barzilay and Lee (2004)s approaches are not tailored to summarization and therefore do not experience the topic bias problem 
In this regard we use a method similar to Lin and Hovy (2000) to identify signature terms and subsequently use them 2 http//irohsuedu/genomics/ to discard sentences that contain none or few such terms 
Concepts do not have to be directly observable as text snippets they can represent abstract properties that particular text units may or may not satisfy for example status as a first sentence in a paragraph or generally position in the source text (Baxendale 1958 Lin and Hovy 1997) 
We have compared several similarity metrics including a few baseline measures (based on document sentence and vocabulary overlap) and a stateof-the-art measure to evaluate summarization systems ROUGE (Lin and Hovy 2003) 
Second of all even when evaluating linguistic quality current automatic metrics should be used with caution as a supplement rather than a replacement for human evaluation similar comments have been made about the use of automatic metrics in MT (Papineni et al 2002 Callison-Burch Osborne and Koehn 2006) 
In our experiments content models outperform Lapatas (2003) state-of-the-art ordering method by a wide margin for one domain and performance metric the gap was 78 percentage points 
This is an extension of Lins method (Lin and Hovy 2000) 
Lapata (2003) in contrast does not attempt to model topics explicitly 
Existing automatic evaluation measures such as BLEU (Papineni et al  2002) and ROUGE (Lin 2The collections are available from http//wwwcsail 
Systems are automatically evalatued using ROUGE (Lin 2004) which has good correlation with human judgments of summary content 
Many of these documents are likely to repeat much the same information while differing in certain i Most of these were based on statistical techniques applied to various document entities examples include frait 1983 Kupiec et al  1995 Paice 1990 Klavans and Shaw 1995 MeKeown et al  1995 Shaw 1995 Aon et al  1997 Boguraev and Kennedy 1997 Hovy and Lin 1997 Mitra et al  1997 Teufel and Moens 1997 Barzilay and Elhadad 1997 Carbonell and Goldstein 1998 Baldwin and Mortbn 1998 Radev and McKeown 1998 Strzalkowski et al  1998) 
Some previous studies on summarization (McKeown and Radev 1995 Barzilay et al  1999 Mani and Bloedorn 1999) deal with multiple docmnents about a single topic but not about multiple topics 1 
Statistical methods for calculating the relevance score of each fragment can be categorized into several classes cue-based (Edmundson 1969) keywordor frequency-based (Luhn 1958 Edmundson 1969 Neto et al 2000 Steinberger and Jezek 2004 Kallel et al 2004 Vanderwende et al 2007) title-based (Edmundson 1969 Teufel and Moens 1997) position-based (Baxendale 1958 Edmundson 1969 Lin and Hovy 1997 Satoshi et al 2001) and length-based (Satoshi et al 2001) 
In the research presented here we concentrate on the first step of the summarization process and follow Barzilay and Elhadad (1997) in employing lexical chains to extract important concepts from a document 
These annotations give the list of nuggets covered by each sentence in each citation summary which are equivalent to the summarization content unit (SCU) as described in (Nenkova and Passonneau 2004) 
Other systems based on information extraction (McKeown et al  2002 Radev and McKeown 1998 White et al  2001) and discourse analysis (Marcu 1999 Strzalkowski et al  1999) also exist but they are not yet usable for general-domain summarization 
RST can be used in sentence selection for single document summarization [Marcu 1997] 
We use ROUGE-1 Recall (Lin and Hovy 2003) as a fitness function for measuring summarization quality which is maximized during the optimization procedure 
Sentence Rank is proposed in Mihalcea and Tarau (2004) to make use of only the sentence-tosentence relationships to rank sentences which outperforms most popular summarization methods 
In addition to sentence fusion compression algorithms (Chandrasekar Doran and Bangalore 1996 Grefenstette 1998 Mani Gates and Bloedorn 1999 Knight and Marcu 2002 Jing and McKeown 2000 Reizler et al 2003) and methods for expansion of a multi parallel corpus (Pang Knight and Marcu 2003) are other instances of such methods 
Many summarization systems (eg  (Teufel and Moens 1997 McKeown et al  1999 Lin and Hovy 2000)) include two levels of analysis the sentence level where every textual unit is scored according to c1 c2 c3 c4 c5 t1 1 1 0 1 1 t2 1 0 0 1 0 t3 0 1 0 0 1 t4 1 0 1 1 1 Table 1 Matrix for Summarization Model the concepts or features it covers and the text level where before being added to the final output textual units are compared to each other on the basis of those features 
Because documents have pre-defined structures (eg sections paragraphs sentences) for different levels of concepts in a hierarchy most recent summarization work has focused on structured probabilistic models to represent the corpus concepts (Barzilay et al 1999 Daume-III and Marcu 2006 Eisenstein and Barzilay 2008 Tang et al 2009 Chen et al 2000 Wang et al 2009) 
Radev and McKeown (1998) have already addressed the issue of information fusion in the context of multi document summarization in one specific domain (ie  terrorism) The fusion of information is achieved through the implementation of summary operators that integrate the information of different templates from different documents referring to the same event 
This approach allows for variation in the way the content is expressed which contrasts the Pyramid method with other evaluation methods such as ROUGE that measure word n-gram overlap (Lin and Hovy 2003) 
Some of these approaches to single document summarization have been extended to deal with multi-document summarization(Mani and EBloedorn 1997) (Barzilay et al  1999) (McKeown et al  1999) 
The main summarization evaluation technique in the NIST TAC 2008 summarization track is the pyramid technique (Nenkova and Passonneau 2004) which is a structured human-based evaluation based on asking human judges to identify summarization content units (SCU) in model and system-generated summaries and measuring how many SCUs from the model summaries occur in the system summary 
The method ROUGE (Lin and Hovy 2003) is based on n-gram overlap between the system-produced and ideal summaries 
Topic signature is used as a novel feature for selecting important content in NeATS (Lin and Hovy 2002) 
We used the ROUGE (Lin and Hovy 2003) toolkit (ieROUGEeval-142 in this study) for evaluation which has been widely adopted by DUC for automatic summarization evaluation 
This recall metric is similar to the ROUGE-1 metric the unigram version of the ROUGE metric (Lin and Hovy 2003) used in the Document Understanding Conferences2 (DUC) 
Some summarization systems assume that the importance of a sentence is derivable from a rhetorical representation of the source text (Marcu 1997) while others leverage information from multiple texts to re-score the importance of conceptual units across all the sources (Hatzivassiloglou et al  2001) 
The third evaluation experiment compared the machine-made and the baseline summaries with the manually created abstracts using ROUGE (Lin 2004)a package for automatically evaluating summariesThis experiment is described in Section 64 
It serves as the first component of a domain-independent multi document summarization system \[McKeown et al 1999\] which generates a summary through text reformulation \[Barzilay et al 1999\] by combining information from these similar text passages 
To avoid misleading the reader when juxtaposed passages from different dates all say yesterday some systems add explicit time stamps (Lin and Hovy 2002a) 
Once all SCUs have been identified the Pyramid method is applied as in (Nenkova and Passonneau 2004b) we compute as coreD by adding for each SCU present in the summary a score equal to the number of model summaries in which that SCU appears 
We evaluated our summarizer on the DUC test sets using the Rouge automatic scoring metric (Lin and Hovy 2003) 
Lin and Hovy (2000) identified signature terms that were strongly associated with documents based on statistics measures 
Among ROUGE metrics ROUGE-N (models n-gram cooccurrence N = 1 2) and ROUGE-L (models longest common sequence) generally perform well in evaluating both single-document summarization and multi-document summarization (Lin and Hovy 2003) 
The summary sentences can also be selected by using machine learning methods (Kupiec et al 1995 Amini and Gallinari 2002) or graph-based methods (ErKan and Radev 2004 Mihalcea and Tarau 2004) 
Another method for summary evaluation is the Pyramid method (Nenkova and Passonneau 2004) which takes into account the fact that human summaries with different content can be equally informative 
But the interpretation of the results is not simple studies (Jing et al 1998 Donaway Drummey and Mather 2000 Radev Jing and Budzikowska 2000) 404 Computational Linguistics Volume 28 Number 4 show how the same summaries receive different scores under different measures or when compared to different (but presumably equivalent) ideal summaries created by humans 
As a result of this exploration we generate a probability density function (pdf) of the ROUGE score (Lin 2004) distributions for four different domains which shows the distribution of the evaluation scores for the generated extracts and allows us to assess the difficulty of each domain for extractive summarization 
For example in machine translation evaluation approaches such as BLEU (Papineni et al 2002) use n-gram overlap comparisons with a model to judge overall goodness with higher n-grams meant to capture fluency considerations 
The authors address a number of shortcomings of manual and automatic summary evaluation methods such as ROUGE (Lin and Hovy 2003) and argue that the Pyramid method is reliable diagnostic and predictive 
(Lin and Hovy 2003) tried to apply BLEU as a measure to evaluate summaries but the results were not as good as in Machine Translation 
MEAD is an implementation of the centroid-based method (Radev et al  2004) that scores sentences based on sentence-level and inter-sentence features including cluster centroids position TF*IDF etc NeATS (Lin and Hovy 2002) selects important content using Ventence position term frequency topic signature and term clustering and then uses MMR (Goldstein et al  1999) to remove redundancy 
Barzilay and Elhadad (1997) constructed lexical chains and extracted strong chains in summaries 
Pyramid (Nenkova and Passonneau 2004) is a manually evaluated measure of recall on facts or Semantic Content Units appearing in the reference summaries 
These n-gram key concepts are called topic signatures (Lin and Hovy 2000) 
For evaluation we are using the ROUGE evaluation toolkit1 which is a method based on Ngram statistics found to be highly correlated with human evaluations (Lin and Hovy 2003a) 
Lin (Lin and Hovy 2003) has found that ROUGE-1 and ROUGE-2 correlate well with human judgments 
WordRank is proposed in Mihalcea and Tarau (2004) to make use of only the co-occurrence relationships between words to rank words which outperforms traditional keyword extraction methods 
The generated summaries are evaluated using the ROUGE toolkit (Lin and Hovy 2003) 
We use the pyramid evaluation method (Nenkova and Passonneau 2004) at the sentence level to evaluate the summary created for each set 
The first of these relative utility (RU) (Radev et al  2000) allows model summaries to consist of sentences with variable ranking 
Introduction In the last decade automatic text summarization has become a popular research topic with a curiously restricted scope of applicationsA few innovative research directions have emerged including headline generation (Soricut and Marcu 2007) summarization of books (Mihalcea and Ceylan 2007) personalized summarization (Daz and Gervas 2007) generation of tables-of-contents (Branavan Deshpande and Barzilay 2007) summarization of speech (Fuentes et al2005) dialogues (Zechner 2002) evaluative text (Carenini Ng and Pauls 2006) and biomedical documents (Reeve Han and Brooks 2007)In addition more researchers have been venturing past purely extractive summarization (Krahmer Marsi and van Pelt 2008 Nomoto 2007 McDonald 2006)By and large however most research in text summarization still revolves around texts characterized by rigid structureThe better explored among such texts are news articles (Barzilay and McKeown 2005) medical documents (Elhadad et al2005) legal documents (Moens 2007) and papers in the area of computer science (Teufel and Moens 2002 Mei and Zhai 2008)Although summarizing these genres is a formidable challenge in itself it excludes a continually increasing number of informal documents available electronicallySuch documents ranging from novels to personal Web pages offer a wealth of information that merits the attention of the text summarization community 
To date various methods and metrics have been developed for English summary evaluation by comparing system summary with reference summary such as the pyramid method (Nenkova et al 2007) and the ROUGE metrics (Lin and Hovy 2003) 
Earlier experiments with graph-based ranking algorithms for text summarization as previously reported in (Mihalcea and Tarau 2004) and (Erkan and Radev 2004) were either limited to single document English summarization or they were applied to English multi-document summarization but in conjunction with other extractive summarization techniques that did not allow for a clear evaluation of the impact of the graph algorithms alone 
TextRank (Mihalcea and Tarau 2004) (Mihalcea 2004) is specifically designed to address this problem by using an extractive summarization technique that does not require any training data or any language-specific knowledge sources 
ROUGE (Lin 2004) a recall-oriented evaluation package for automatic summarization 
Lexical approaches to term-based summarization use lexical relations to identify central terms (Barzilay and Elhadad 1997 Gong and Liu 2002) coreference(or anaphora-) based approaches (Baldwin and Morton 1998 Boguraev and Kennedy 1999 Azzam et al  1999 Bergler et al  2003 Stuckardt 2003) identify these terms by running a coreferenceor anaphoric resolver over the text1 We are not aware however of any attempt to use both lexical and anaphoric information to identify the main terms 
On the other hand redundancy can be exploited to identify important and accurate information for applications such as summarization and question answering (Mani and Bloedorn 1997 Radev and McKeown 1998 Radev Prager and Samn 2000 Clarke Cormack and Lynam 2001 Dumais et al 2002 Chu-Carroll et al 2003) 
Previous approaches include supervised learning (Hirao et al  2002) (Teufel and Moens 1997) vectorial similarity computed between an initial abstract and sentences in the given document intra document similarities (Salton et al  1997) or graph algorithms (Mihalcea and Tarau 2004) (Erkan and Radev 2004) (Wolf and Gibson 2004) 
For the generic topic-focused and update summarization tasks the experiment sare perform the DUC data sets using ROUGE-2 and ROUGE-SU (Lin and Hovy 2003) as evaluation measures 
Although there are automatic corpus-based metrics for summarization such as ROUGE (Lin and Hovy 2003) they do not seem to dominate summarization evaluation in the same way that BLEU-type metrics dominate MT evaluation 
Lin and Hovy (2000) first introduced topic signatures which are topic relevant terms for summarization 
Automated evaluation will utilize the standard DUC evaluation metric ROUGE (Lin 2004) which representsrecallovervariousn-grams statistics from asystem-generated summary against a set of human generated peer summaries5 We compute ROUGE scores with and without stop words removed from peer and proposed summaries 
Other approaches to the summarization of news articles make use of the typical journalistic writing style for example the fact that the most newsworthy information comes first as a result the first few sentences of a news article are good candidates for a summary (Brandow Mitze and Rau 1995 Lin and Hovy 1997) 
In general rules are applied to revise summaries produced by a summarisation system (Mani et al  1999 Otterbacher et al  2002) 
A variety of approaches exist for determining the salient sentences in the text statistical techniques based on word distribution (Salton et al  1991) symbolic techniques based on discourse structure (Marcu 1997) and semantic relations between words (Barzilay and Elhadad 1997) 
This has been leveraged by Nenkova and Passonneau (2004) to produce a manual scoring method for summaries though the fact that humans show so little agreement in this task is somewhat disheartening 
They reported that their method is superior to BLEU (Papineni et al  2002) in terms of the correlation between human assessment and automatic evaluation 
Marcu (1997) uses a rhetorical parser to build rhetorical structure trees for arbitrary texts and produces a summary by extracting sentences that span the major rhetorical nodes of the tree 
Work in this area includes that of Lin and Hovy (2003) and Pastra and Saggion (2003) both of whom inspect the use of Bleu-like metrics (Papineni et al  2002) in summarization 
According to (Lin and Hovy 2003) among all sub-metrics unigram-based ROUGE (ROUGE-1) has been shown to agree with human judgment most and bigram-based ROUGE (ROUGE-2) fits summarization well 
Text structuring algorithms (Lapata 2003 Barzilay and Lee 2004 Karamanis et al  2004) are commonly evaluated by their performance at information-ordering 
The score is usually computed based on a combination of statistical and linguistic features such as term frequency sentence position cue words and stigma words (Luhn 1969 Edmundson 1969 Hovy and Lin 1997) 
There are also automatic methods for summary evaluation such as ROUGE (Lin 2004) which gives a score based on the similarity in the sequences of words between a human-written model summary and the machine summary 
Each fact in the citation summary of a paper is a summarization content unit (SCU) (Nenkova and Passonneau 2004) and the fact distribution matrix created by annotation provides the information about the importance of each fact in the citation summary 
We also tested other automatic methods content-based evaluation BLEU (Papineni et al  2001) and ROUGE-1 (Lin 2004) and compared their results with that of evaluation by revision as reference 
ROUGE1 and ROUGE-2 have been shown to have most correlation with human summaries (Lin and Hovy 2003) and higher order ROUGE-N scores (N > 1) estimate the fluency of summaries 
This restriction is necessary because the problem of optimizing many-to-many alignments 5 Our preliminary experiments with n-gram-based overlap measures such as BLEU (Papineni et al 2002) and ROUGE (Lin and Hovy 2003) show that these metrics do not correlate with human judgments on the fusion task when tested against two reference outputs 
Marcu (1997a) provides some evidence that other genres will deliver less consistency 
We implemented this system called MEAD* by 305 adapting MEAD (Radev et al  2003) an opensource framework for multi-document summarization 
BLEU was rst proposed as a supplement (the U in BLEU stands for understudy) for human evaluation (Papineni et al 2002) but it is now routinely used as the main technique for evaluating research contributions 
For evaluation we are using the ROUGE evaluation toolkit which is a method based on Ngram statistics found to be highly correlated with human evaluations (Lin and Hovy 2003a) 
At the other extreme previous approaches (Radev and McKeown 1998) have demonstrated that this task is feasible when a detailed semantic representation of the input sentences is available 
We tested ve automatic corpus-based metrics two variants of the BLEU metric used in machine translation (Papineni et al 2002) two variants of the ROUGE metric used in document summarization (Lin and Hovy 2003) and a simple sting-edit distance metric (as a baseline) 
It has been observed that in the context of multi document summarization of news articles extraction may be inappropriate because it may produce summaries which are overly verbose or biased towards some sources (Barzilay et al 1999) 
Related work Previous work has focused on the analysis of citation and collaboration networks (Teufel et al 2006 Newman 2001) and scientific article summarization (Teufel and Moens 2002) 
First our method focuses on subject shift of the documents from the target event rather than the sets of documents from dierent events(Radev et al  2000) 
ROUGE measure is widely used for evaluation (Lin and Hovy 2003) the DUC contests usually officially employ ROUGE for automatic summarization evaluation 
We tested several measures such as ROUGE (Lin 2004) and the cosine distance 
Recently graph-based ranking methods such as LexPageRank (Erkan and Radev 2004) and TextRank (Mihalcea and Tarau 2004) 1 http//haydnisiedu/ROUGE/ have been proposed for multi-document summarization 
Other research rewards passages that include topic words that is words that have been determined to correlate well with the topic of interest to the user (for topic-oriented summaries) or with the general theme of the source text (Buckley and Cardie 1997 Strzalkowski et al 1999 Radev Jing and Budzikowska 2000) 
To achieve readable summaries the extracted sentences must be appropriately ordered (Barzilay et al 2002 Lapata 2003 Barzilay and Lee 2004 Barzilay and Lapata 2005) 
To do this we adapted MEAD (Radev et al  2003) an open-source framework for multi document summarization to suit our purposes 
Much more attention has been devoted to discourse-level constraints on adjacent sentences indicative of coherence and good text flow (Lapata 2003 Barzilay and Lapata 2008 Karamanis et al to appear) 
This method of evaluation has already been used in other summarization evaluations such as Edmundson (1969) and Marcu (1997) 
Related to this is the work by Teufel and Moens (2002) on rhetorical classification for content selection 
Since each of these scores has a different range of values we perform ranking based on each score separately then induce total ranking by summing ranks from individual categories Rank (theme) = Rank (Number of sentences in theme) + Rank (Similarity of sentences in theme) + Rank (Sum of lexical chain scores in theme) Lexical chains sequences of semantically related wordsare tightly connected to the lexical cohesive structure of the text and have been shown to be useful for determining which sentences are important for single-document summarization (Barzilay and Elhadad 1997 Silber and McCoy 2002) 
We verify that our method generates summaries that are significantly better than the baseline results in terms of ROUGE score (Lin 2004) and subjective readability measures 
An automatic evaluation package ie ROUGE (Lin and Hovy 2003) is employed to evaluate the summarization performance 
For ROUGE-S and ROUGE-SU we use three variations following (Lin 2004b) the maximum skip distances are 4 9 and infinity 7 
This technique is the focus of one of the articles in this special issue (Teufel and Moens 2002) which shows how particular types of rhetorical relations in the genre of scientific journal articles can be reliably identified through the use of classification 
ROUGE is a method based on Ngram statistics found to be highly correlated with human evaluations (Lin and Hovy 2003)1 Throughout the paper the evaluations are reported using the ROUGE1 setting which seeks unigram matches between the generated and the reference summaries and which was found to have high correlation with human judgments at a 95% confidence level 
The results of the comparison with ROUGE-N (Lin and Hovy 2003 Lin 2004a Lin 2004b) ROUGE-S(U) (Lin 2004b Lin and Och 2004) and ROUGE-L (Lin 2004a Lin 2004b) show that our method correlates more closely with human evaluations and is more robust 
Non-extractive systems are less common previous related work included reformulation of extracted models (McKeown et al  1999) gist extraction (Berger and Mittal 2000) machine translation-like approaches (Witbrock and Mittal 1999) and generative models (De Jong 1982 Radev and McKeown 1998 Fum et al  1986 Reihmer and Hahn 1988 Rau et al  1989) 
In all our experiments we used the ROUGE (Lin 2004) evaluation package and its ROUGE1 ROUGE-2 and ROUGE-SU4 recall scores 
Association for Computational Linguistics Computational Linguistics Volume 35 Number 4 and automatic evaluation metrics in machine translation and document summarization (Doddington 2002 Papineni et al 2002 Lin and Hovy 2003) much less is known about how well automatic metrics correlate with human judgments in NLG 
This is due to the fact that LexPageRank ranks the sentence using eigenvector centrality which implicitly accounts for information subsumption among all sentences (Erkan and Radev 2004) 
Summaries using Lexical Overlap ROUGE (Lin 2004) is a package for automatically evaluating summariesGiven one or more gold-standard summaries (usually written by people) ROUGE offers several metrics for evaluating the summary in questionThe metrics reward lexical overlap between the model summaries and the candidate oneDepending on the metric the lexical units taken into consideration are n-grams word sequences and word pairs 
To select these we use the idea of strong chains introduced by Barzilay and Elhadad (1997) 
For evaluation we use ROUGE (Lin 2004) SU4 recall metric1 which was among the official automatic evaluation metrics for DUC 
Weakly-constrained algorithms In evaluation with ROUGE (Lin 2004) summaries are truncated to a target length K Yih et al(2007)used as tack decoding with a slight modi cation which allows the last sentence in a summary to be truncated to a target length 
Some summarization systems assume that the importance of a sentence is derivable from a rhetorical representation of the source text (Marcu 1997) 
It is also notable the study reported in (Lin and Hovy 2003b) discussing the usefulness and limitations of automatic sentence extraction for summarization which emphasizes the need of accurate tools for sentence extraction as an integral part of automatic summarization systems 
The use of sentence fusion in multi document summarization has been extensively explored by Barzilay in her thesis (Barzilay 2003 Barzilay et al  1999) though in the multi-document setting one has redundancy to fall back on 
In our own annotation of three meetings with SCUs defined as in (Nenkova and Passonneau 2004a) we found that repetitions and reformulation of the same information are particularly infrequent and that textual units that express the same content among model summaries are generally originating from the same document sentence (eg  in the figure the first sentence in model 1 and 2 emanate from the same document sentence) 
For evaluation we are using the ROUGE evaluation toolkit1 which is a method based on Ngram statistics found to be highly correlated with human evaluations (Lin and Hovy 2003) 
Summary of datasets We used the ROUGE toolkit 2 (Lin and Hovy 2003) for evaluation which has been widely adopted by DUC for automatic summarization evaluation 
Present summarization systems (Watanabe 1996 Hovy and Lin 1997) use such clues to calculate an importance score for each sentence choose sentences 918 according to the score and simply put the selected sentences together in order of their occurrences in the original document 
The resulting outputs are compared to the original input sentences in an MTstyle evaluation under two commonly-used metrics BLEU (Papineni et al 2002) and NIST (Doddington 2002) 
Conceptual units can also be defined out of more basic conceptual units based on the co-occurrence of important concepts (Barzilay and Elhadad 1997) or syntactic constraints between representations of concepts (Hatzivassiloglou et al  2001) 
From this point of view some of the measures used in the evaluation of Machine Translation systems such as BLEU (Papineni et al  2002) have been imported into the summarization task 
This is represented by the idea behind the Pyramid evaluation framework (Nenkova and Passonneau 2004 Passonneau et al 2005) where different levels of the pyramid represent the proportion of concepts (Summary Content Units or SCUs) mentioned by 1 to n summarizers in summaries of the same text 
In the following ROUGE-SN denotes ROUGE-S with maximum skip distance N ROUGE-SU (Lin 2004) This measure is an extension of ROUGE-S it adds a unigram as a counting unit 
Nenkova and Passonneau (2004) showed that Pyramids scores produced reliable system rankings when multiple (4 or more) models were used and that Pyramids rankings correlate with rankings produced by ROUGE-2 and ROUGE-SU2 (ie ROUGE with skip bi-grams) 
Other researchers have investigated the topic of automatic generation of abstracts but the focus has been different eg sentence extraction (Edmundson 1969 Johnson et al 1993 Kupiec et al  1995 Mann et al  1992 Teufel and Moens 1997 Zechner 1995) processing of structured templates (Paice and Jones 1993) sentence compression (Hori et al  2002 Knight and Marcu 2001 Grefenstette 1998 Luhn 1958) and generation of abstracts from multiple sources (Radev and McKeown 1998) 
Our results improve on the results of Teufel and Moens (2002) (reproduced in Table 7) both overall and for each individual category 
We use ROUGE (Lin 2004b) to quantitatively assess the agreement of Opinosis summaries with human composed summaries 
Teufel and Moens (2002) identify discourse relations on a sentence-by-sentence basis without presupposing an explicit discourse structure 
We use the publicly available ROUGE toolkit (Lin 2004)to compute recall precision andF-scorefor ROUGE-1 
Among them ROUGE5 (Lin and Hovy 2003) is supposed to produce the most reliable scores in correspondence with human evaluations 
First previous methods for computing lexical chains have either been manual (Morris and Hirst 1991) or automated but with exponential efficiency (Hirst and St-Onge 1997 Barzilay and Elhadad 1997) 
MEAD summarizer The MEAD summarizer [Radev et al 2000] [Radev et al 2002] is based on sentence extraction and uses a linear combination of three features to rank the sentences in the source documents 
Extracts are often useful in an information retrieval environment since they give users an idea as to what the source document is about (Tombros and Sanderson 1998 Mani et al 1999) but they are texts of relatively low quality 
Utility (RU) (Radev et al  2000) is tested on a large corpus for the first time in this project 
We evaluate intrinsically by comparing to human-annotated attribution and extrinsically by showing that automatically acquired knowledge about scientific attribution improves performance on a discourse classification task Argumentative Zoning (Teufel and Moens 2002) where sentences are labelled as one of fOwn Other Background Textual Aim Basis Contrastg according to their role in the authors argument 
Two metrics have become quite popular in multi-document summarization namely the Pyramid method (Nenkova and Passonneau 2004b) and ROUGE (Lin 2004) 
Previous related work on extractive systems included the use of semantic tagging and coreference/lexical chains (Saggion et al  2003 Barzilay and Elhadad 1997 Azzam et al  1998) lexical occurrence/structural statistics (Mathis et al  1973) discourse structure (Marcu 1998) cue phrases (Luhn 1958 Paice 1990 Rau et al  1994) positional indicators (Edmunson 1964) and other extraction methods (Kuipec et al  1995) 
Evaluation We evaluated the quality of the headlines using ROUGE (Lin and Hovy 2003) 
For the intrinsic evaluation of a large number of summaries we made use of the ROUGE metrics that has been widely used in automatic evaluation of summarization systems (Lin and Hovy 2003 Hickl et al 2007) 
Ranking algorithms such as Kleinbergs HITS algorithm (Kleinberg 1999) or Googles PageRank (Brin and Page 1998) have been traditionally and successfully used in Web-link analysis (Brin and Page 1998) social networks and more recently in text processing applications (Mihalcea and Tarau 2004) (Mihalcea et al  2004) (Erkan and Radev 2004) 
Log-likelihood ratio for words in the input Number of topic signature words (Lin and Hovy 2000 Conroy et al 2006) and percentage of signature words in the vocabulary 
Since then several methods and theories have been applied including the use of term frequency inverse document frequency (TF IDF) measures sentence position and cue and title words (Luhn 1958 Edmundson 1969 Kupiec Pedersen and Chen 1995 Brandow Mitze and Rau 1995) partial understanding using conceptual structures (DeJong 1982 Tait 1982) bottom-up understanding top-down parsing and automatic linguistic acquisition (Rau Jacobs and Zernik 1989) recognition of thematic text structures (Hahn 1990) cohesive properties of texts (Benbrahim and Ahmad 1995 Barzilay and Elhadad 1997) and rhetorical structure theory (Ono Sumita and Miike 1994 Marcu 1997) 
In this task we assume a document collection D consisting of documents D1Dn describing the same (or closely related) narrative (Lapata 2003) 
We thus adapt some state-of-the-art conventional summarization methods implemented in the MEAD toolkit (Radev et al 2003)4 to obtain three baseline methods (1) LEAD It simply extracts sentences from the beginning of a paper ie sentences in the abstract or beginning of the introduction section we include LEAD to see if such leading sentences re ect the impact of a paper as authors presumably would expect to summarize a papers contributions in the abstract 
However recent progress on this problem (Marcu 1997) is encouraging 
We include both subjective evaluation from 3 evaluators based on their personalized interests and preference and the objective evaluation based on the widely used ROUGE metrics (Lin and Hovy 2003) 
MEAD (Radev et al 2004) and NeATS (Lin and Hovy 2002) are such implementations using position and term frequency etc MMR (Goldstein et al 1999) algorithm is used to remove redundancy 
For single document summarization the sentence score is usually computed by empirical combination of a number of statistical and linguistic feature values such as term frequency sentence position cue words stigma words topic signature (Luhn 1969 Lin and Hovy 2000) 
Revision of single-document summaries [Mani et al 1999] focused on the revision of single-document summaries in order to improve their informativeness 
Based on this information it is possible to select one or more of the outputs as the gold standard and compare the rest in the pyramid scoring scheme described by Nenkova and Passonneau (2004) 
For the Amazon corpus we also report a coarser metric which measures extraction precision and recall while ignoring labels (Binary labels) as well as ROUGE (Lin 2004) 
As ROUGE (Lin 2004) has been officially adopted for DUC evaluations since 2004 we also take it as our main evaluation criterion 
Extractive summarization is a simple but robust method for text summarization and it involves assigning saliency scores to some units (eg sentences paragraphs) of the documents and extracting those with highest scores while abstraction summarization usually needs information fusion (Barzilay et al 1999) sentence compression (Knight and Marcu 2002) and reformulation (McKeown et al 1999) 
Topic-oriented multi-document summarization has already been studied in other evaluation initiatives which provide testbeds to compare alternative approaches (Over 2003 Goldstein et al  2000 Radev et al  2000) 
ROUGE-L (Lin 2004) This measure evaluates summaries by longest common subsequence (LCS) defined by Equation 4 
Following the successful applic ation of automatic evaluation methods such as BLEU (Papineni et al  2001) in machine translation evaluation Lin and Hovy (2003) showed that methods similar to BLEU ie n-gram co-occurrence statistics could be applied to evaluate summaries 
Generally methods for reducing redundancy are evaluated using ROUGE (Lin 2004) BE (Hovy et al 2005) or Pyramid (Nenkova and Passonneau 2004) 
We used the ROUGE-155 (Lin and Hovy 2003) toolkit for evaluation which has been widely adopted by DUC and TAC for automatic summarization evaluation 
