	Most existing work to capture label consistency, has attempted to create all parenleftbign2parenrightbig pairwise dependencies between the different occurrences of an entity, (Finkel et al. , 2005; Sutton and McCallum, 2004), where n is the number of occurrences of the given entity. ::: P06-1141_108:181
	
	We are also focusing on other potential applications, including chunking (Sha and Pereira, 2003), named entity recognition (Florian et al. , 2004; Ando and Zhang, 2005b; Daume III and Marcu, 2006), and speaker adaptation (Kuhn et al. , 1998). ::: W06-1615_253:260
	
V01,M02	Since MUC-6, BBN's Hidden Markov Model (HMM) based IdentiFinder (Bikel et al. , 1997) has achieved remarkably good performance. ::: C02-1025_20:201
	
M02,M03	For the Penn Treebank, (Ratnaparkhi, 1996) reports an accuracy of 96.6% using the Maximum Entropy approach, our much simpler and therefore faster HMM approach delivers 96.7%. ::: A00-1031_179:207
	
M03,M02	We use the maximum entropy tagging method described in (Kazama et al. , 2001) for the experiments, which is a variant of (Ratnaparkhi, 1996) modified to use HMM state features. ::: W02-0301_103:341
	
	We use a tagger based on Adwait Ratnaparkhi's method (Ratnaparkhi, 1996). ::: W00-0904_34:155
	
V02	For example, the average F1 of the Stanford NER (Finkel et al., 2005) , which is trained on the CoNLL03 shared task data set and achieves state-of-the-artperformanceonthattask, dropsfrom 90.8%(RatinovandRoth,2009)to45.8%ontweets. ::: P11-1037_14:230
	
M09	Supervised learning methods can effectively solve NER problem by learning a model from manually labeled data (Borthwick, 1999; Sang and Meulder, 2003; Gao et al., 2005; Florian et al., 2003). ::: N09-1032_14:250
	
V02,M08	NER proves to be a knowledge intensive task, and it was reassuring to observe that System Resources Used F1 + LBJ-NER Wikipedia, Nonlocal Features, Word-class Model 90.80 (Suzuki and Isozaki, 2008) Semi-supervised on 1Gword unlabeled data 89.92 (Ando and Zhang, 2005) Semi-supervised on 27Mword unlabeled data 89.31 (Kazama and Torisawa, 2007a) Wikipedia 88.02 (Krishnan and Manning, 2006) Non-local Features 87.24 (Kazama and Torisawa, 2007b) Non-local Features 87.17 + (Finkel et al., 2005) Non-local Features 86.86 Table 7: Results for CoNLL03 data reported in the literature. ::: W09-1119_202:223
	
V02	The results we obtained on the CoNLL03 test set were consistent with what was reported in (Finkel et al., 2005). ::: W09-1119_189:223
	
M05,M06	Nadeau and Sekine (2007) reported that a strong trend exists recently in applying machine learning (ML) techniques such as Support Vector Machine (SVM) (Kazama et al., 2002; Isozaki and Kazawa, 2002) and Conditional Random Field (CRF) (Settles, 2004) to NER, which can address these issues. ::: W11-0208_15:21
	
M03,M05,M06	On the other hand, in text-based NER, better results are obtained using discriminative schemes such as maximum entropy (ME) models (Borthwick, 1999; Chieu and Ng, 2003), support vector machines (SVMs) (Isozaki and Kazawa, 2002), and conditional random fields (CRFs) (McCallum and Li, 2003). ::: P06-1078_17:177
	
	This forced the best sequential classi er systems to resort to heuristic combinations of forward-moving and backward-moving sequential classi ers (Kudo and Matsumoto, 2001). ::: N03-1028_27:169
	
M02,M03	The Penn Treebank results reported here for the Markov model approach are at least equivalent to those reported for the Maximum Entropy approach in (Ratnaparkhi, 1996). ::: A00-1031_24:207
	
	One participating team has used externally trained named entity recognition systems for English as a part in a combined system (Florian et al. , 2003). ::: W03-0419_109:153
	
	Isozaki (Isozaki and Kazawa, 2002) controls the parameters of a statistical morphological analyzer so as to produce more fine-grained output. ::: N03-1002_22:203
	
M03	When building probabilistic models for tag sequences, we often decompose the global probability of sequences using a directed graphical model (e.g. , an HMM (Brants, 2000) or a conditional Markov model (CMM) (Ratnaparkhi, 1996)). ::: N03-1033_25:202
	
M06	Additionally, our approach makes it possible to do inference in just about twice the inference time with a single sequential CRF; in contrast, approaches like Gibbs Sampling that model the dependencies directly can increase inference time by a factor of 30 (Finkel et al. , 2005). ::: P06-1141_142:181
	
	This enables us to build a high performance NER without using separate classifiers to take care of global consistency or complex formulation on smoothing and backoff models (Bikel et al. , 1997). ::: C02-1025_194:201
	
	See (Florian et al. , 2003) for such a study. ::: W03-0434_87:105
	
V01	In fact, the highest score obtained by a knowledge based system in MUC-7 reached F 93.39 % (Mikheev et al. , 1998). ::: W06-2809_24:138
	
	The per-state models in this paper are log-linear models, building upon the models in (Ratnaparkhi, 1996) and (Toutanova and Manning, 2000), though some models are in fact strictly simpler. ::: N03-1033_101:202
	
	(Klein et al. , 2003), (Mayfield et al. , 2003), (Wu et al. , 2003), (Kozareva et al. , 2005c) among others, combined several classifiers to obtain better named entity coverage rate. ::: E06-3004_9:290
	
	In natural language processing, recent years have seen ME techniques used for sentence boundary detection, part of speech tagging, parse selection and ambiguity resolution, and stochastic attribute-value grammars, to name just a few applications (Abney, 1997; Berger et al. , 1996; Ratnaparkhi, 1998; Johnson et al. , 1999). ::: W02-2018_7:125
	
	There is a similar brief class (BWC) (Settles 2004) which collapses consecutive characters into one. ::: I08-5013_59:193
	
	Prefix and suffix tries were also used previously (Cucerzan and Yarowsky, 1999). ::: I08-5010_74:141
	
M05	For the formulation of SVMs in the context of NLP applications, see (Kudo and Matsumoto, 2001). ::: D09-1119_63:258
	
M05,M03	Discriminative classifiers, which directly model the posterior distribution of class label given features, i.e. SVM (Isozaki and Kazawa 2002) and Maximum Entropy model for NER (Chieu and Ng 2003), have been shown to outperform generative model based classifiers. ::: D11-1144_101:236
	
	We also applied the classifier combination technique discussed in this paper to English and German (Florian et al. , 2003b). ::: W03-1026_185:193
	
V01	Furthermore, we evaluate these features and compare with those used in MUC (Zhou and Su, 2002). ::: W03-1307_60:199
	
	A NER sys901 tem often builds some generative/discriminative model, then, either uses only one classifier (Carreras et al., 2002) or combines many classifiers using some heuristics (Florian et al., 2003). ::: C10-2104_21:177
	
	High-performance taggers typically also include joint three-tag counts in some way, either as tag trigrams (Brants, 2000) or tag-triple features (Ratnaparkhi, 1996, Toutanova and Manning, 2000). ::: N03-1033_124:202
	
	There has been significant work with such models for greedy sequence modeling in NLP (Ratnaparkhi, 1996; Borthwick et al. , 1998). ::: W03-0430_4:87
	
M02	Hidden Markov Models were employed by four of the systems that took part in the shared task (Florian et al. , 2003; Klein et al. , 2003; Mayfield et al. , 2003; Whitelaw and Patrick, 2003). ::: W03-0419_73:153
	
	There have been several efforts to apply machine learning techniques to the same task (Cowie, 1995) (Bikel et al, 1997) (Gallippi, 1996) (Bennett et al, 1997) (Borthwick et al, 1997). ::: W98-1120_177:221
	
	Isozaki (Isozaki and Kazawa, 2002) introduces the thesaurus NTT Goi Taikei (Ikehara et al. , 1999) to augment the Table 5: The depth of redundant analysis and the extraction accuracy Pair Wise Method Depth of morph. ::: N03-1002_153:203
	
M06	In fact, many off-the-shelf CRF implementations now replicate Sha and Pereira (2003), including their choice of feature set: CRF++ by Taku Kudo (http://crfpp. ::: P10-1040_134:310
	
F01	Various techniques have been used for solving the NER problem (Mikheev et al., 1999; Borthwick, 1999; Cucerzan and Yarowsky, 1999; Chieu and Ng, 2003; Klein et al., 2003; Kim and Woodland, 2000) ranging from naively using gazetteers to rules based techniques to purely statistical techniques, even hybrid approaches. ::: I08-5003_51:248
	
M06,M09,P11	Indeed CRF has been established by many as the state-of-theart supervised named entity recognition system for traditional NER tasks (Feng and McCallum 2004; McCallum 2003), for NER in biomedical texts (Settles 2004), and in various languages besides English, such as Bengali (Ekbal et al. 2008) and Chinese (Mao et al 2008). ::: D11-1144_146:236
	
	Although some earlier reports suggested accuracy (F1-number) of machine learning based systems to be in the lower 90s with relatively small amount of labeled data (for example, (Bikel et al. , 1999; Mikheev et al. , 1998; Sundheim, 1995)), these studies were often performed on relatively restricted domains. ::: W03-0434_11:105
	
M06,M08	The Linear CRF model is used as the fine model, with the following considerations: 1) It is well studied and has been successfully used in state-ofthe-art NER systems (Finkel et al., 2005; Wang, 2009); 2) it can output the probability of a label sequence, which can be used as the labeling confidence that is necessary for the semi-supervised learning framework. ::: P11-1037_137:230
	
M03,M05	Isozaki and Kazawa (2002) compared three commonly used methods for named entity recognition the SVM with quadratic kernel, maximal entropy method, and a rule based learning system, and showed that the SVM-based system performed better than the other two. ::: W05-0610_39:275
	
M03	According to current tagger comparisons (van Halteren et al. , 1998; Zavrel and Daelemans, 1999), and according to a comparsion of the results presented here with those in (Ratnaparkhi, 1996), the Maximum Entropy framework seems to be the only other approach yielding comparable results to the one presented here. ::: A00-1031_198:207
	
F02	There can easily be over 100,000 atomic tests (mostly based on tests for the identity of words in the vocabulary), and ten or more shifted-conjunction patterns resulting in several million features (Sha and Pereira, 2003). ::: W03-0430_46:87
	
	The first, suggested in (Collins and Singer, 1999; Abney, 2002), divides into one view capturing internal features of the NE, and the other capturing features of its left-right contexts (hereafter referred to as Greedy Agreement pure, or GAa16 ). ::: W03-1504_83:186
	
M02,M03,V02,P01,P18,P20	Our system is a Maximum Entropy Markov Model, which further develops a system earlier used for the CoNLL 2003 shared task (Klein et al. , 2003) and the 2004 BioCreative critical assessment of information extraction systems, a task that involved identifying gene and protein name mentions but not distinguishing between them (Dingare et al. , 2004). ::: W04-1217_10:73
	
F02	Previous work has built lexicons from fixed corpora by determining linguistic patterns for the context in which relevant words appear (Collins and Singer, 1999; Jones et al. , 1999). ::: W03-0430_72:87
	
	Since in previous work, many NER systems have been applied successfully in newswire domain (Zhou and Su 2002; Bikel et al. 1999; Borthwich et al. 1999), more and more explorations have been done to port existing NER system into biomedical domain (Kazama et al. 2002; Takeuchi et al. 2002; Nobata et al. 1999 and 2000; Collier et al. 2000; Gaizauskas et al. 2000; Fukuda et al. 1998; Proux et al. 1998). ::: W03-1307_7:199
	
	Although Named Entity Recognition is reported in the literature to have an accuracy rate of 85-95% (Finkel et al., 2005; Ratinov and Roth, 2009), it was clear by inspection that both the Stanford and the LBJ tagger made a number of mistakes. ::: W11-0810_54:178
	
M05	SVMs have given high performance in various classification tasks (Joachims, 1998; Kudo and Matsumoto, 2001). ::: C02-1054_12:206
	
F03	Name Class of Previous Occurrences The name class of previous occurrences of w is used as a feature, similar to (Zhou and Su, 2002). ::: W03-0423_88:115
	
	An alternative back-off modeling approach by means of constraint relaxation is applied in our model (Zhou and Su 2002). ::: W03-1307_43:199
	
M06	Settles (2004)s CRF system deserves special note in the sense that it achieved comparable performance to top ranked systems with a rather simple feature set. ::: W04-1213_103:117
	
P15	Unsupervised learning approaches do not require labelled training data training requires only very few seed lists and large unannotated corpora (Collins and Singer, 1999). ::: I08-5007_34:229
	
	Active learning is a framework which can be used for reducing the amount of human effort required to create a training corpus (Dagan and Engelson, 1995; Engelson and Dagan, 1996; Thompson et al., 1999; Shen et al., 2004). ::: W08-0605_12:174
	
	For unknown words, SCL gives a relative reduction in error of 19.5% over Ratnaparkhi (1996), even with 40,000 sentences of source domain training data. ::: W06-1615_182:260
	
	Recent comparisons of approaches that can be trained on corpora (van Halteren et al. , 1998; Volk and Schneider, 1998) have shown that in most cases statistical aproaches (Cutting et al. , 1992; Schmid, 1995; Ratnaparkhi, 1996) yield better results than finite-state, rule-based, or memory-based taggers (Brill, 1993; Daelemans et al. , 1996). ::: A00-1031_11:207
	
	We then describe a transformation-based learning (TBL, Brill, 1995) method that is used to adapt our system to different segmentation standards. ::: P04-1059_29:233
	
M06	CRFs have been successfully applied to a number of real-world tasks, including NP chunking (Sha and Pereira, 2003), Chinese word segmentation (Peng et al., 2004), information extraction (Pinto et al., 2003; Peng and McCallum, 2004), named entity identification (McCallum and Li, 2003; Settles, 2004), and many others. ::: I08-1044_37:215
	
	Following (Ratnaparkhi 1996), we only include features which occur 5 times or more in training data. ::: P02-1062_53:174
	
M05	Support vector machines (SVM) method produces high performance in many classification tasks (Joachims, 1998; Kudo and Matsumoto, 2001). ::: I05-3013_169:246
	
	A number of bootstrapping methods have been proposed for NLP tasks (e.g. Yarowsky (1995), Collins and Singer (1999), Riloff and Jones (1999)). ::: P05-1001_10:242
	
	(Collins and Singer, 1999) classified NEs through co-training, (Kozareva et al. , 2005a) used self-training and cotraining to detect and classify named entities in news domain, (Shen et al. , 2004) conducted experiments with multi-criteria-based active learning for biomedical NER. ::: E06-3004_58:290
	
	They can often achieve high accuracy provided that a large annotated training set similar to the test data is available (Borthwick, 1999; Zhou and Su, 2002; Florian et al. , 2003; Klein et al. , 2003; Finkel et al. , 2005). ::: N06-1010_14:171
	
V01V02	The Entity Detection and Tracking task (EDT henceforth) has close ties to the named entity recognition (NER) and coreference resolution tasks, which have been the focus of attention of much investigation in the recent past (Bikel et al. , 1997; Borthwick et al. , 1998; Mikheev et al. , 1999; Miller et al. , 1998; Aberdeen et al. , 1995; Ng and Cardie, 2002; Soon et al. , 2001), and have been at the center of several evaluations: MUC-6, MUC-7, CoNLL02 and CoNLL03 shared tasks. ::: N04-1001_9:189
	
M09,P15	These frequencies can be used to calculate probability estimates P(c | a1a2 an) for each category c. Tries have previously been used in both supervised (Patrick et al. , 2002) and unsupervised (Cucerzan and Yarowsky, 1999) named entity recognition. ::: W03-0432_15:106
	
M06	For the baseline method, we performed a conditional random field (CRF), which is exactly the same training procedure described in (Sha and Pereira, 2003) with L-BFGS. ::: D07-1083_116:316
	
	Given the usefulness of Named Entities (NEs) in many natural language processing tasks, there has been a lot of work aimed at developing accurate named entity extractors (Borthwick, 1999; Velardi et al. , 2001; Arevalo et al. , 2002; Zhou and Su, 2002; Florian, 2002; Zhang and Johnson, 2003). ::: P05-2005_8:125
	
M03,M02,M05,P11	Statistical methods often suffer from the problem of data sparsity, and machine learning approaches (e.g., Hidden Markov Models (HMMs) (Bikel et al., 1999; Zhou and Su, 2002), Support Vector Machines (SVMs) (Isozaki and Kazawa, 2002), Maximum Entropy (MaxEnt) (Borthwick, 1999; Chieu and Ng, 2003), Transformation-based Learning (TBL) (Brill, 1995) or variants of them) might be unsatisfactory to learn linguistic information in Chinese NEs. ::: I08-1044_15:215
	
M06	For our experiments we use the limited memory variable metric (LMVM) (Sha and Pereira, 2003) routine, which has become the standard algorithm for CRF training with a likelihood-based objective function. ::: W06-2918_27:203
	
	There has been a considerable amount of work on English NER yielding good performance (Tjong Kim Sang et al. 2002, 2003; Cucerzan & Yarowsky 1999; Wu et al. 2003). ::: N04-4010_11:168
	
M03,V05	For mention detection we use approaches based on Maximum Entropy (MaxEnt henceforth) (Berger et al. , 1996) and Robust Risk Minimization (RRM henceforth) 1For a description of the ACE program see http://www.nist.gov/speech/tests/ace/. ::: N04-1001_21:189
	
M02	In this regard, we are similar to (Bikel et al, 1997), which also uses a probabilistic method in their HMM based system. ::: W98-1120_184:221
	
	Further details on this can be found in (Mikheev et al. , 1998). ::: E99-1001_172:225
	
F05	Most commonly, feature-based classifiers use a set of capitalisation features and a sentence-initial feature (Bikel et al. , 1997). ::: W03-0432_34:106
	
	Discriminative taggers and chunkers have been the state-of-the-art for more than a decade (Ratnaparkhi, 1996; Sha and Pereira, 2003). ::: W06-1615_9:260
	
	(Collins and Singer, 1999) used contextual information of a di erent sort than we do. ::: W02-1028_101:187
	
M09,M02,M06	Most existing work has focused on supervised learning approaches, employing models such as HMMs (Zhou and Su, 2002), MEMMs (Bender et al. , 2003; Finkel et al. , 2005), and CRFs (McCallum and Li, 2003). ::: N06-1010_156:171
	
	It is only recently employed in NER (Shen et al. , 2004). ::: W06-1660_142:202
	
V01	The reported result of the simple deterministic features used in MUC can achieve Fmeasure of 74.1 (Zhou and Su 2002), but when they are used in biomedical domain, they only get F-measure of 24.3. ::: W03-1307_61:199
	
	The model weights are trained using the improved iterative scaling algorithm (Berger et al. , 1996). ::: W03-0425_20:71
	
F01	Four groups examined the usability of unannotated data, either for extracting training instances (Bender et al. , 2003; Hendrickx and Van den Bosch, 2003) or obtaining extra named entities for gazetteers (De Meulder and Daelemans, 2003; McCallum and Li, 2003). ::: W03-0419_107:153
	
	Attempts have been made to use global information (e.g. , the same named entity occurring in different sentences of the same document), but they usually consist of incorporating an additional classifier, which tries to correct the errors in the output of a first NER (Mikheev et al. , 1998; Borthwick, 1999). ::: C02-1025_9:201
	
M03,M05	The sequential classi cation approach can handle many correlated features, as demonstrated in work on maximum-entropy (McCallum et al. , 2000; Ratnaparkhi, 1996) and a variety of other linear classi ers, including winnow (Punyakanok and Roth, 2001), AdaBoost (Abney et al. , 1999), and support-vector machines (Kudo and Matsumoto, 2001). ::: N03-1028_20:169
	
P16	Collins and Singer (1999) proposed an unsupervised method for named entity classification based on the idea of cotraining. ::: N06-1010_157:171
	
	In this section, we try to compare our results with those obtained by IdentiFinder '97 (Bikel et al. , 1997), IdentiFinder '99 (Bikel et al. , 1999), and MENE (Borthwick, 1999). ::: C02-1025_166:201
	
F05	A common approach is to extract word-internal features from unknown words, for example suffix, capitalization, or punctuation features (Mikheev, 1997, Wacholder et al. , 1997, Bikel et al. , 1997). ::: W03-0428_8:91
	
	Moreover, if we can map the abbreviation to its full form in the current document, the recognized abbreviation is still helpful for classifying the same forthcoming abbreviations in the same document, as in (Zhou and Su 2002). ::: W03-1307_109:199
	
M02	Previously (Klein et al., 2003) experimented with character-level models for English using character based HMM which is a generative model. ::: I08-5010_42:141
	
V02	Zhou and Su (2002) used a wide variety of features, which suggests that the relatively poor performance of the taggers used in CoNLL-2002 was largely due to the feature sets used rather than the machine learning method. ::: W03-0424_10:92
	
	Previous uses of this model include language modeling(Lau et al. , 1993), machine translation(Berger et al. , 1996), prepositional phrase attachment(Ratnaparkhi et al. , 1994), and word morphology(Della Pietra et al. , 1995). ::: W96-0213_12:123
	
M02,M03,M01,M06,M05	A wide variety of machine learning methods have been applied to this problem, including Hidden Markov Models (Bikel et al. 1997), Maximum Entropy methods (Borthwick et al. 1998, Chieu and Ng 2002), Decision Trees (Sekine et al. 1998), Conditional Random Fields (McCallum and Li 2003), Class-based Language Model (Sun et al. 2002), Agent-based Approach (Ye et al. 2002) and Support Vector Machines. ::: W04-0705_8:209
	
	Earlier papers have taken a character-level approach to named entity recognition (NER), notably Cucerzan and Yarowsky (1999), which used prefix and suffix tries, though to our knowledge incorporating all character a4 grams is new. ::: W03-0428_13:91
	
	The leader of the pack is the MXPOST tagger (Ratnaparkhi, 1996). ::: W02-2010_9:67
	
V02,M03,M02	It is worthwhile noting that the best published results for CoNLL03 (Florian et al., 2003) were obtained by using four different classifiers (Robust Risk Minimization, Maximum Entropy, Transformation-based learning, and Hidden Markov Model) and trying six different classifier combination methods. ::: D10-1098_222:273
	
	While NER over formal text such as news articles and webpages is a well-studied problem (Bikel et al., 1999; McCallum and Li, 2003; Etzioni et al., 2005), there has been recent work on NER over informal text such as emails and blogs (Huang et al., 2001; Poibeau and Kosseim, 2001; Jansche and Abney, 2002; Minkov et al., 2005; Gruhl et al., 2009). ::: D10-1098_11:273
	
	A majority vote of five systems (Chieu and Ng, 2003; Florian et al. , 2003; Klein et al. , 2003; McCallum and Li, 2003; Whitelaw and Patrick, 2003) performed best on the English development data. ::: W03-0419_137:153
	
	This kind of features have been widely used in both newswire NER system, such as (Zhou and Su 2002), and biomedical NER system, such as (Nobata et al. 1999; Gaizauskas et al. 2000; Collier et al. 2000; Takeuchi and Collier 2002; Kazama et al. 2002). ::: W03-1307_48:199
	
	Examples of AL used in language engineering include named entity recognition (Shen et al., 2004; Tomanek et al., 2007), text categorization (Lewis and Gale, 1994; Hoi et al., 2006), part-of-speech tagging (Ringger et al., 2007), and parsing (Thompson et al., 1999; Becker and Osborne, 2005). ::: W09-1118_9:222
	
	Finally, in section 4 we add additional features to the maxent model, and chain these models into a conditional markov model (CMM), as used for tagging (Ratnaparkhi, 1996) or earlier NER work (Borthwick, 1999). ::: W03-0428_15:91
	
	The use of char N-gram (N-gram substring) features was inspired by the work of (Klein et al. 2003), where the introduction of such features has been shown to improve the overall F1 score by over 20%. ::: D11-1144_127:236
	
M02,F01	For instance, Zhou and Su trained HMM with a set of attributes combining internal features such as gazetteer information, and external features such as the context of other NEs already recognized (Zhou and Su, 2002). ::: P05-2005_24:125
	
M06	CRFs have shown empirical successes recently in POS tagging (Lafferty et al. , 2001), noun phrase segmentation (Sha and Pereira, 2003) and Chinese word segmentation (McCallum and Feng, 2003). ::: W03-0430_7:87
	
	We employed a tagger similar to the one presented by Settles (2004). ::: P08-1098_130:197
	
M02	(Bikel et al. , 1997) and (Bikel et al. , 1999) are other examples of the use of HMMs. ::: P05-2005_25:125
	
M03,M02	The best performance for both languages has been obtained by a combined learning system that used Maximum Entropy Models, transformation-based learning, Hidden Markov Models as well as robust risk minimization (Florian et al. , 2003). ::: W03-0419_142:153
	
	Resource limitation, directed NER research (Collins and Singer, 1999), (Carreras et al. , 2003), (Kozareva et al. , 2005a) toward the usage of semi-supervised techniques. ::: E06-3004_12:290
	
P01	In recent years discriminative probabilistic models have been successfully applied to a number of information extraction tasks in natural language processing (NLP), such as named entity recognition (NER) (McCallum and Li, 2003), noun phrase chunking (Sha and Pereira, 2003) and information extraction from research papers (Peng and McCallum, 2004). ::: W06-2918_8:203
	
F01	It turns out that while problems of coverage and ambiguity prevent straightforward lookup, injection of gazetteer matches as features in machine-learning based approaches is critical for good performance (Cohen, 2004; Kazama and Torisawa, 2007a; Toral and Munoz, 2006; Florian et al., 2003). ::: W09-1119_158:223
	
V01,V02	Previous work on mention detection The MD task has close ties to named-entity recognition, which has been the focus of much recent research (Bikel et al., 1997; Borthwick et al., 1998; Tjong Kim Sang, 2002; Florian et al., 2003; Benajiba et al., 2009), and has been at the center of several evaluations: MUC-6, MUC-7, CoNLL02 and CoNLL03 shared tasks. ::: D10-1033_73:229
	
	For example, Animal would be mapped to Aa, G.M. would again be mapped to A.A The tagger was applied and trained in the same way as described in (Ratnaparkhi 1996). ::: P02-1062_50:174
	
	In (Bikel et al. , 1997) and (Bikel et al. , 1999), performance was plotted against training data size to show how performance improves with training data size. ::: C02-1025_176:201
	
M02,M03	These include rule-based systems \[Krupka 1998\], Hidden Markov Models (HMM) \[Bikel et al. 1997\] and Maximum Entropy Models (MaxEnt) \[Borthwick 1998\]. ::: A00-1034_8:153
	
	It has recently been shown that quasi-Newton methods, such as L-BFGS, are significantly more efficient than traditional iterative scaling and even conjugate gradient (Malouf, 2002; Sha and Pereira, 2003). ::: W03-0430_41:87
	
M02	In this paper, we will study how to adapt a general Hidden Markov Model (HMM)-based NE recognizer (Zhou and Su 2002) to biomedical domain. ::: W03-1307_25:199
	
M08,F01	Some semi-supervised approaches (Collins and Singer, 1999), (Riloff and Jones, 1999), (Pasca, 2007) have also used large available corpora to generate context patterns for named entities or for generating gazetteer lists and entity expansion using seed entities. ::: W10-2418_35:221
	
	The HMM tagger generally follows the NYMBLE model (Bikel et al, 1997), but with a larger number of states (12) to handle name prefixes and suffixes, and transliterated foreign names separately. ::: W04-0705_34:209
	
P38	They have shown to be useful in part of speech tagging (Lafferty et al. 2001), shallow parsing (Sha and Pereira 2003), and named entity recognition for Hindi newswire data (Li and McCallum 2003). ::: I08-5013_18:193
	
M06	The linear CRF chunker of Sha and Pereira (2003) is a standard near-state-of-the-art baseline chunker. ::: P10-1040_133:310
	
M02,M03,M01,M10	Recently IE systems based on supervised learning paradigms such as hidden Markov models (Bikel et al. , 1997), maximum entropy (Borthwick et al. , 1998) and decision trees (Sekine et al. , 1998) have emerged that should be easier to adapt to new domains than the dictionary-based systems of the past. ::: W00-0904_6:155
	
M06	The use of CRF sequence labeling has been increasing over the past few years (McCallum and Li, 2003; Nadeau and Sekine, 2009) with good success (Benajiba and Rosso, 2008). ::: W10-2417_49:162
	
V01,M04	MUC-7 has also seen hybrids of statistical NERs and hand-coded systems (Mikheev et al. , 1998; Borthwick, 1999), notably Mikheev's system, which achieved the best performance of 93.39% on the official NE test data. ::: C02-1025_21:201
	
M06,M05	In the present work, we show that CRFs beat all reported single-model NP chunking results on the standard evaluation dataset, and are statistically indistinguishable from the previous best performer, a voting arrangement of 24 forwardand backward-looking support-vector classi ers (Kudo and Matsumoto, 2001). ::: N03-1028_33:169
	
	Previous works have tackled NER within the biomedical domain (Settles 2004), newswire do main (Grishman and Sundheim 1996), and email domain (Minkov et al. 2005). ::: W10-0712_40:247
	
	The simplicity of our approach makes it easy to incorporate dependencies across the whole corpus, which would be relatively much harder to incorporate in approaches like (Bunescu and Mooney, 2004) and (Finkel et al. , 2005). ::: P06-1141_141:181
	
	Several learning algorithms have also been developed for named entity recognition (e.g. , (Collins and Singer, 1999; Cucerzan and Yarowsky, 1999)). ::: W02-1028_100:187
	
M05,M03	Support Vector Machines (SVMs) (Vapnik, 1995) and Maximum Entropy (ME) method (Berger et al. , 1996) are powerful learning methods that satisfy such requirements, and are applied successfully to other NLP tasks (Kudo and Matsumoto, 2000; Nakagawa et al. , 2001; Ratnaparkhi, 1996). ::: W02-0301_23:341
	
	We used the same 58 feature types as Ratnaparkhi (1996). ::: W06-1615_121:260
	
M06	Also of interest is the system presented by Settles (2004) which used CRFs with rich feature sets and suggested that one could use features from syntactic parsing with this model given their flexibility. ::: W07-1031_150:165
	
M01,M03,M06	Many discriminative methods have been applied to NER, such as decision trees (Sekine et al. , 1998), ME models (Borthwick, 1999; Chieu and Ng, 2003), and CRFs (McCallum and Li, 2003). ::: P06-1078_28:177
	
V02	A very common case of this in the CoNLL dataset is that of documents containing references to both The China Daily, a newspaper, and China, the country (Finkel et al. , 2005). ::: P06-1141_64:181
	
M09	Various supervised learning methods for Named Entity (NE) tasks were successfully applied and have shown reasonably satisfiable performance.((Zhou and Su, 2002)(Borthwick et al. , 1998)(Sassano and Utsuro, 2000)) However, most of these systems heavily rely on a tagged corpus for training. ::: P03-2031_6:83
	
F01	However, one participant (Settles, 2004) reported that their attempt to utilize gazetteers (together with other resources) had failed in gaining better overall performance. ::: W04-1213_74:117
	
M05	SVMs with a polynomial kernel of degree 2 were shown to provide state-of-the-art performance in many NLP application, see for example (Kudo and Matsumoto, 2000; Nivre et al., 2006; Isozaki and Kazawa, 2002; Goldberg et al., 2006). ::: D09-1119_64:258
	
	Several representations to encode region information are proposed and examined (Ramshaw and Marcus, 1995; Uchimoto et al. , 2000; Kudo and Matsumoto, 2001). ::: W02-0301_46:341
	
M08	In addition, we would also like to explore the semi-supervised techniques such as co-training and self-training (Collins and Singer, 1999). ::: W07-1712_167:167
	
M06	The Stanford Tagger is based on Conditional Random Fields (Finkel et al., 2005). ::: W11-0810_27:178
	
V02	His system is an extension of Kudos chunking system (Kudo and Matsumoto, 2001) that gave the best performance at CoNLL-2000 shared tasks. ::: C02-1054_42:206
	
M03	In this paper, we present an approach for extracting the named entities (NE) of natural language inputs which uses the maximum entropy (ME) framework (Berger et al. , 1996). ::: W03-0420_4:80
	
P25	Another combination of five systems (Carreras et al. , 2003b; Mayfield et al. , 2003; McCallum and Li, 2003; Munro et al. , 2003; Zhang and Johnson, 2003) obtained the best result for the German development data. ::: W03-0419_138:153
	
V01,P18,P20,P22	Both of these beliefs are questionable, as the top MUC 7 performance of 93.39% Entity Precision Recall F-Score Fully Correct protein 77.40% 68.48% 72.67% DNA 66.19% 69.62% 67.86% RNA 72.03% 65.89% 68.83% cell line 59.00% 47.12% 52.40% cell type 62.62% 76.97% 69.06% Overall 71.62% 68.56% 70.06% Left Boundary Correct protein 82.89% 73.34% 77.82% DNA 68.47% 72.01% 70.19% RNA 75.42% 68.99% 72.06% cell line 63.80% 50.96% 56.66% cell type 63.93% 78.57% 70.49% Overall 75.72% 72.48% 74.07% Right Boundary Correct protein 84.70% 74.96% 79.53% DNA 74.43% 78.29% 76.31% RNA 78.81% 72.09% 75.30% cell line 70.2% 56.07% 62.34% cell type 71.68% 88.10% 79.05% Overall 79.65% 76.24% 77.91% Table 2: Results on the evaluation data (Mikheev et al. , 1998) in the domain of newswire text used an easier performance metric where incorrect boundaries were given partial credit, while both the biomedical NER shared tasks to date have used an exact match criterion where one is doubly penalized (both as a FP and as a FN) for incorrect boundaries. ::: W04-1217_59:73
	
M02,P12	(Bikel et al. , 1997) report on Nymble, an HMM-based name tagging system operating in English and Spanish. ::: W97-0312_184:216
	
	The method we use is transformation-based learning (Brill, 1995), which requires an initial segmentation, a goal segmentation into which we wish to transform the initial segmentation and a space of allowable transformations (i.e. transformation templates). ::: P04-1059_135:233
	
M05	Table 2: Tags for positions in a word Tag Description S one-character word B first character in a multi-character word E last character in a multi-character word I intermediate character in a multi-character word (only for words longer than 2 chars) Table 3: Tags for character types Tag Description ZSPACE Space ZDIGIT Digit ZLLET Lowercase alphabetical letter ZULET Uppercase alphabetical letter HIRAG Hiragana KATAK Katakana OTHER Others (Kanji etc.) 3.3 Support Vector Machine-based Chunking We used the chunker yamcha (Kudo and Matsumoto, 2001), which is based on support vector machines (Vapnik, 1998). ::: N03-1002_79:203
	
A01	In most previous work, NER has been applied to news articles (e.g. , (Bikel et al. , 1999; McCallum and Li, 2003)), scientific articles (e.g. , (Craven and Kumlien, 1999; Bunescu and Mooney, 2004)), or web pages (e.g. , (Freitag, 1998)). ::: H05-1056_5:211
	
M08,M09,P15	NERC has been investigated using supervised(McCallum and Li, 2003), unsupervised (Etzioni et al., 2005) and semi-supervised (Pasca et al., 2006b) learning methods. ::: W10-2415_10:264
	
	As discussed above, all state-of-the-art published methods rely on lexical features for such tasks (Zhang et al., 2001; Sha and Pereira, 2003; Finkel et al., 2005; Ratinov and Roth, 2009). ::: D09-1119_45:258
	
	In an approach similar to Zhou and Su (2002), we extracted for each word a 2-byte code, as summarized in Table 1. ::: W02-2010_12:67
	
M02	Segmentation through constrained HMM Our original HMM is similar to the Nymble \[Bikel et al. 1997\] system that is based on bigram statistics. ::: A00-1034_99:153
	
M06	We use a Conditional Random Field (Lafferty et al. , 2001; Sha and Pereira, 2003) since it represents the state of the art in sequence modeling and has also been very effective at Named Entity Recognition. ::: P06-1141_34:181
	
M05,M06	Previous studies have shown that automatic named entity recognition can be performed with a reasonable level of accuracy by using various machine learning models such as support vector machines (SVMs) or conditional random fields (CRFs) (Tjong Kim Sang and De Meulder, 2003; Settles, 2004; Okanohara et al., 2006). ::: W08-0605_9:174
	
	Previous work (Cucerzan and Yarowsky, 1999) was done using the complete words as features which suffers from a low recall problem. ::: I08-5010_3:141
	
M02,V01	Generative models such as Hidden Markov Models (Bikel et al., 1997; Zhou and Su, 2001) have shown excellent performance on the Message Understanding Conference (MUC) data-set (Chinchor, 1997). ::: I08-5007_37:229
	
	Previous best results: FIJZ03 (Florian et al. , 2003), CN03 (Chieu and Ng, 2003), KSNM03 (Klein et al. , 2003). ::: P05-1001_197:242
	
M03	Max-ent taggers have been shown to be highly competitive on a number of tagging tasks, such as part-of-speech tagging (Ratnaparkhi 1996), named-entity recognition (Borthwick et. ::: P02-1062_37:174
	
M05	We use a support vector machine (SVM)based chunker yamcha (Kudo and Matsumoto, 2001) for the chunking process. ::: N03-1002_27:203
	
	Furthermore, some constraints on the boundary category and entity category between two consecutive tags are applied to filter the invalid NE tags (Zhou and Su 2002). ::: W03-1307_46:199
	
M05,V02	Table 1 presents the results of our system using three learning algorithms, the uneven margins SVM, the standard SVM and the PAUM on the CONLL2003 test set, together with the results of three participating systems in the CoNLL-2003 shared task: the best system (Florian et al. , 2003), the SVM-based system (May eld et al. , 2003) and the Perceptron-based system (Carreras et al. , 2003). ::: W05-0610_117:275
	
	As to the tasks involved in our scenario, several papers address AL for NER (Shen et al., 2004; Hachey et al., 2005; Tomanek et al., 2007) and syntactic parsing (Tang et al., 2001; Hwa, 2004; Baldridge and Osborne, 2004; Becker and Osborne, 2005). ::: P08-1098_167:197
	
	In addition, because of data sparsity (out-of-vocabulary) problem due to the long-tailed distribution of words in natural language, sophisticated unknown word models are generally needed for good performance (Klein et al. 2003). ::: D11-1144_115:236
	
	The inclusion of extra named entity recognition systems seems to have worked well (Florian et al. , 2003). ::: W03-0419_115:153
	
	Character n-gram based approach (Klein et al., 2003) using generative models, was experimented on English language and it proved to be useful over the word based models. ::: I08-5010_4:141
	
M05	SVM has given high performance in various classification tasks (Joachims, 1998; Kudo and Matsumoto, 2001). ::: W06-0110_11:142
	
M02,M03,V01	Our system is adapted from a HMM-based NE recognizer, which has been proved very effective in MUC (Zhou and Su 2002). ::: W03-1307_34:199
	
M06,M02	However, the CRF chunker in Huang and Yates (2009), which uses their HMM word clusters as extra features, achieves F1 lower than a baseline CRF chunker (Sha & Pereira, 2003). ::: P10-1040_86:310
	
	This is one of the main causes for the recent growing interest on developing language independent NERC systems, which may be trained from small training sets by taking advantage of unlabelled examples (Collins and Singer, 1999; Abney, 2002), and which are easy to adapt to changing domains (being all these aspects closely related). ::: W03-1504_9:186
	
M05,P21	In this paper, we employ an SVM-based NER method in the following way that showed good NER performance in Japanese (Isozaki and Kazawa, 2002). ::: P06-1078_29:177
	
	Sometimes, these types of features are referred to as word external and word-internal (Klein et al. , 2003) The feature set of some NER methods (Wu, 2002) also includes part-of-speech information and/or word pre xes and suf xes. ::: W07-1712_40:167
	
M05,M06	Transformation-based learning (Florian et al. , 2003), Support Vector Machines (Mayfield et al. , 2003) and Conditional Random Fields (McCallum and Li, 2003) were applied by one system each. ::: W03-0419_85:153
	
M03	Maximum entropy models (Jaynes, 1957; Berger et al. , 1996; Della Pietra et al. , 1997) are a class of exponential models which require no unwarranted independence assumptions and have proven to be very successful in general for integrating information from disparate and possibly overlapping sources. ::: W02-2019_13:62
	
	At any rate, regularized conditional loglinear models have not previously been applied to the problem of producing a high quality part-of-speech tagger: Ratnaparkhi (1996), Toutanova and Manning (2000), and Collins (2002) all present unregularized models. ::: N03-1033_178:202
	
V01,F05	As observed by participants in the MUC-6 and -7 tasks (Bikel et al. , 1997; Borthwick, 1999; Miller et 1: Capitalization information 2: Presence in dictionary first_cap, all_caps, all_lower, number, punct, other upper, lower, both, none Table 1: Capitalization information al. , 1998), an important feature for the NER task is information relative to word capitalization. ::: W02-2010_11:67
	
M05,M06	Model F score SVM combination 94.39% (Kudo and Matsumoto, 2001) CRF 94.38% Generalized winnow 93.89% (Zhang et al. , 2002) Voted perceptron 94.09% MEMM 93.70% Table 2: NP chunking F scores 5 Results All the experiments were performed with our Java implementation of CRFs,designed to handle millions of features, on 1.7 GHz Pentium IV processors with Linux and IBM Java 1.3.0. ::: N03-1028_119:169
	
	There has been some previous work on NER for SSEA languages (McCallum and Li, 2003; Cucerzan and Yarowsky, 1999), but most of the time such work was an offshoot of the work done for European languages. ::: I08-5003_60:248
	
M03,M02,M05,P04	The relevant algorithms include Maximum Entropy (Borthwick, 1999; Klein et al. , 2003), Hidden Markov Model (HMM) (Bikel et al. , 1999; Klein et al. , 2003), AdaBoost (Carreras et al. , 2003), Memory based learning (Meulder and Daelemans, 2003), Support Vector Machine (Isozaki and Kazawa, 2002), Robust Risk Minimization (RRM) Classification method (Florian et al. , 2003), etc. For Chinese NER, most of the existing approaches use hand-crafted rules with word (or character) frequency statistics. ::: W06-1660_10:202
	
V02,F01	Moreover, the experimental results shown in Tables 3 and F=1 additional resources ASO-semi 89.31 unlabeled data (27M words) (Ando and Zhang, 2005) (Florian et al. , 2003) 88.76 their own large gazetteers, 2M-word labeled data (Chieu and Ng, 2003) 88.31 their own large gazetteers, very elaborated features HySOL 88.14 unlabeled data (17M words) supplied gazetters HySOL 87.20 unlabeled data (17M words) Table 5: Previous top systems in NER (CoNLL2003) experiments F=1 additional resources ASO-semi 94.39 unlabeled data (Ando and Zhang, 2005) (15M words: WSJ) HySOL 94.30 unlabeled data (17M words: Reuters) (Zhang et al. , 2002) 94.17 full parser output (Kudo and Matsumoto, 2001) 93.91 Table 6: Previous top systems in Chunking (CoNLL-2000) experiments 4 indicate that HySOL is rather robust with respect to the hyper-parameter since we can obtain fairly good performance without a prior distribution. ::: D07-1083_174:316
	
	Previous work deals with this problem by correcting inconsistencies between the named entity classes assigned to different occurrences of the same entity (Borthwick, 1999; Mikheev et al. , 1998). ::: C02-1025_123:201
	
V03,M09	In the work presented here, we compare the segmentation boundaries of named entities in tire IREX workshop's training corpus with those of supervised learning technique mainly because it is easy to implement and quite straightibrward to extend a supervised lem'ning version to a milfimally supervised version (Collins and Singer, 1999; Cucerzan and Yarowsky, 1999). ::: C00-2102_24:126
	
F05	Features (F pos ) In the previous NER research in newswire domain, part-of-speech (POS) features were stated not useful, as POS features may affect the use of some important capitalization information (Zhou and Su 2002). ::: W03-1307_75:199
	
M03	A conditional maximum entropy model q(xjw) for p has the parametric form (Berger et al. , 1996; Chi, 1998; Johnson et al. , 1999): q(xjw) = exp T f (x) y2Y(w) exp(T f (y)) (1) where is a d-dimensional parameter vector and T f (x) is the inner product of the parameter vector and a feature vector. ::: W02-2018_17:125
	
M06	The advantage of using CRF is that they combine HMM-like generative power with classifier-like discrimination (Lafferty et al., 2001; Sha and Pereira, 2003). ::: W10-2417_52:162
	
	However, our implementation uses a quasi-Newton gradient-climber BFGS for optimization, which has been shown to converge much faster (Malouf, 2002; Sha and Pereira, 2003). ::: C04-1081_54:178
	
M01,M02,M03,M05	The latter is currently dominating in NER amongst which the most popular methods are decision tree (Sekine et al. , 1998; Pailouras et al. , 2000), Hidden Markov Model (Zhang et al. , 2003; Zhao, 2004), maximum entropy (Chieu and Ng, 2002; Bender et al. , 2003), and support vector machines (Isozaki and Kazawa, 2002; Takeuchi and Collier, 2002; Mayfield, 2003). ::: I05-3013_76:246
	
M08	Recently there have been some studies on adapting NER systems to new domains employing techniques such as active learning and semi-supervised learning (Shen et al. , 2004; Mohit and Hwa, 2005), 80 or incorporating external lexical knowledge (Ciaramita and Altun, 2005). ::: N06-1010_160:171
	
M02	Nymble (Bikel et al. , 1997), a system which uses HMMs is one of the most successflfl such systems and trains on a corpus of marked-up text, using only character features in addition to word bigrams. ::: C00-1030_23:214
	
P01	While multiple machine learning approaches have been proposed for information extraction in recent years (McCallum et al., 2000; Cohen and McCallum, 2003; Klein et al., 2003; Krishnan and Manning, 2006), manually created regexes remain a widely adopted practical solution for information extraction (Appelt and Onyshkevych, 1998; Fukuda et al., 1998; Cunningham, 1999; Tanabe and Wilbur, 2002; Li et al., 2006; DeRose et al., 2007; Zhu et al., 2007). ::: D08-1003_17:246
	
	Approaches to NER There has been a considerable amount of work on NER in English (Isozaki and Kazawa, 2002; Zhang and Johnson, 2003; Petasis et al., 2001; Mikheev et al., 1999). ::: I08-5007_18:229
	
M06	We also compare our performance against (Bunescu and Mooney, 2004) and (Finkel et al. , 2005) and find that we manage higher relative improvement than existing work despite starting from a very competitive baseline CRF. ::: P06-1141_148:181
	
	Words surrounding the current word have been occasionally used in taggers, such as (Ratnaparkhi, 1996), Brills transformation based tagger (Brill, 1995), and the HMM model of Lee et al. ::: N03-1033_129:202
	
	We take a commonly used approach and treat NER as a sequential tagging problem (Borthwick, 1999; Zhou and Su, 2002; Finkel et al. , 2005). ::: N06-1010_44:171
	
M05	A polynomial function defined as (sxi x j + r)d is popular in applications of SVMs to NLPs (Kudo and Matsumoto, 2000; Yamada et al. , 2000; Kudo and Matsumoto, 2001), because it has an intuitively sound interpretation that each dimension of the mapped space is a 3For many real-world problems where the samples may be inseparable, we allow the constraints are broken with some penalty. ::: W02-0301_69:341
	
	The techniques proposed in the literature fall under three categories: rule-based (Krupka and Hausman, 2001; Sekine and Nobata, 2004), machine learning based (O. Bender and Ney, 2003; Florian et al., 2003; McCallum and Li, 2003; Finkel and Manning, 2009; Singh et al., 2010) and hybrid solutions (Srihari et al., 2001; Jansche and Abney, 2002). ::: D10-1098_12:273
	
	Comparison with previous best results: KM01 (Kudoh and Matsumoto, 2001), CM03 (Carreras and Marquez, 2003), SP03 (Sha and Pereira, 2003), ZDJ02 (Zhang et al. , 2002). ::: P05-1001_212:242
	
M05	SVMs have shown promise when applied to chunking (Kudo and Matsumoto, 2001) and named entity recognition (Sassano and Utsuro, 2000; McNamee and Mayfield, 2002), though performance is quite sensitive to parameter choices. ::: W03-0433_21:101
	
M02,M01,M06	Statistical methods such as HMM (Bikel et al., 1997; Zhou and Su, 2001), Decision tree model (Baluja et al., 2000; Isozaki, 2001), and conditional random fields (McCallum, 2003) have been used. ::: I08-5007_36:229
	
F05,F04	Unknown word features Most of the models presented here use a set of unknown word features basically inherited from (Ratnaparkhi, 1996), which include using character n-gram prefixes and suffixes (for n up to 4), and detectors for a few other prominent features of words, such as capitalization, hyphens, and numbers. ::: N03-1033_151:202
	
M06,M05	Similarly, CRFs were employed by one system in isolation (Settles, 2004) and by another system in combination with SVMs (Song et al. , 2004). ::: W04-1213_60:117
	
	Two more systems used them in combination with other techniques (Florian et al. , 2003; Klein et al. , 2003). ::: W03-0419_71:153
	
V01	These results are achieved by training on the official MUC-6 and MUC-7 training data, which is much less training data than is used by other machine learning systems that worked on the MUC-6 or MUC-7 named entity task (Bikel et al. , 1997; Bikel et al. , 1999; Borthwick, 1999). ::: C02-1025_15:201
	
M03	ME tagger is based on Ratnaparkhi (1996)s POS tagger and is described in Curran and Clark (2003). ::: W03-0424_17:92