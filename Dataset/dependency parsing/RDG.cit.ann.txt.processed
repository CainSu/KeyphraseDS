Nivre and Nilsson (2005) showed how the restriction to projective dependency graphs could be lifted by using graph transformation techniques to preprocess training data and post-process parser output so-called pseudo-projective parsing 
However to make parsing tractable these models are forced to restrict features over a very limited history of parsing decisions (McDonald and Pereira 2006 McDonald and Nivre 2007) 
Edge-factored models have many computational benefits most notably that inference for nonprojective dependency graphs can be achieved in polynomial time (McDonald et al  2005b) 
The latest state-of-the-art statistical dependency parsers are discriminative meaning that they are based on classifiers trained to score trees given a sentence either via factored whole-structure scores (McDonald et al  2005a) or local parsing decision scores (Hall et al  2006) 
Although the higher-order MST parsing models will be slow with exact inference requiring O(nk) time (McDonald and Pereira 2006) it might be possible to use higher-order kgram subtrees with approximated parsing model in the future 
Yet they can be parsed in O(n3) time (Eisner 1996) 
Dependency-based syntactic parsing has become a widely used technique in natural language processing and many different parsing models have been proposed in recent years (Yamada and Matsumoto 2003 Nivre et al 2004 McDonald et al 2005a Titov and Henderson 2007 Martins et al 2009) 
Our first observation is that the accuracies for both systems are considerably below the 90% unlabeled and 88% labeled attachment scores for English that have been reported previously (McDonald and Pereira 2006 Hall et al 2006) 
When this analysis is coupled with the projective parsing algorithms of Eisner (1996) andPaskin (2001) we beginto get aclear picture of the complexity for data-driven dependency parsing within an edge-factored framework 
In conditional log-linear models for dependency parsing (which are closely related to conditional random fields (Lafferty et al 2001)) a distribution over dependency structures for a sentence x is defined as follows p(y|x) = 1Z(x) exp{g(xy)} (1) where Z(x) is the partition function w is a parameter vector and g(xy) = summation display (hml)y wf(xhml) Here f(xhml) is a feature vector representing the dependency (hml) in the context of the sentence x (see for example (McDonald et al 2005a)) 
In which case the parsing problem reduces to a18a37a36 a1a39a38a41a40a43a42a45a44a46a38a48a47 a49a22a50a41a51a53a52a55a54a57a56 a58 a52a60a59a62a61a5a63a64a59a66a65a43a56a67a50a26a49 sa2a5a4a29a19a22a21a23a4a25a24a28a16 (1) where the score sa2a5a4 a19 a21 a4 a24 a16 can depend on any measurable property of a4a20a19 and a4a25a24 within the tree a18  This formulation is suf ciently general to capture most dependency parsing models including probabilistic dependency models (Wang et al  2005 Eisner 1996) as well as non-probabilistic models (McDonald et al  2005 Wang et al  2006) 
The data All our data comes from the CoNLL-X Shared Task (Buchholz and Marsi 2006) specifically the 4 data sets freely available online 
McDonald and Pereira (2006) have shown that incorporating second order features relating to adjacent edge pairs improves the accuracy of maximum spanning tree parsers (MST) 
We then describe the two main paradigms for learning and inference in this years shared task as well as in last years which we call transition-based parsers (section 52) and graph-based parsers (section 53) adopting the terminology of McDonald and Nivre (2007)5 Finally we give an overview of the domain adaptation methods that were used (section 54) 
We tested this hypothesis by using the Charniak (2000) parser in n-best mode producing the top 10 trees with corresponding probabilities 
The dominant learning method in this tradition is support vector machines (Kudo and Matsumoto 2002 Yamada and Matsumoto 2003 Nivre et al 2006) but memory-based learning has also been used (Nivre Hall and Nilsson 2004 Nivre and Scholz 2004 Attardi 2006) 
For treebanks with non-projective trees we use the pseudo-projective parsing technique to transform the treebank into projective structures (Nivre and Nilsson 2005) 
The second order parsing algorithm of McDonald and Pereira (2006) uses a separate algorithm for edge labeling 
Transition Precondition NivresMethod Left-Arc (|wiwj|A) (wj|A{(wjwi)}) i = 0wk (wkwi) A Right-Arc (|wiwj|A) (|wi|wjA{(wiwj)}) Reduce (|wiA) (A) wk (wkwi) A Shift (wj|A) (|wjA) Proposed Method Left-Arc (|titj|A) (tj|A{(wjwi)}) i = 0 Right-Arc (|titj|A) (|tiA{(mphc(titj)wj)}) Shift (tj|A) (|tjA) studies taking data-driven approaches by (Kudo and Matsumoto 2002) (Yamada and Matsumoto 2003) and (Nivre 2003) the deterministic incremental parser was generalized to a state transition system in (Nivre 2008) 
It is well known that dependency trees extracted from lexicalized phrase structure parsers (Collins 1999 Charniak 2000) typically are more accurate than those produced by pure dependency parsers (Yamada and Matsumoto 2003) 
For details we refer the reader to (Nivre et al  2004) 
Statistical parsing has been one of the most active areas of research in the computational linguistics community since the construction of the Penn Treebank (Marcus et al 1993) 
For many different part factorizations and structure domains Y() it is possible to solve the above maximization efficiently and several recent efforts have concentrated on designing new maximization algorithms with increased context sensitivity (Eisner 2000 McDonald et al 2005b McDonald and Pereira 2006 Carreras 2007) 
We evaluated our classifier-based best-first parser on the Wall Street Journal corpus of the Penn Treebank (Marcus et al  1993) using the standard split sections 2-21 were used for training section 22 was used for development and tuning of parameters and features and section 23 was used for testing 
As such it follows a bottom-up strategy or bottom-up-trees as defined in Buchholz and Marsi (2006) in contrast to the shift-reduce dependency parsing algorithm described by Nivre (2003) which is a bottom-up/topdown hybrid or bottom-up-spans 
We take as our starting point a re-implementation of McDonalds state-of-the-art dependency parser (McDonald et al  2005a) 
Eisner (1996) algorithm with non-projective rewriting and second order features 
Syntactic parsing using dependency structures has become a standard technique in natural language processing with many different parsing models in particular data-driven models that can be trained on syntactically annotated corpora (Yamada and Matsumoto 2003 Nivre et al 2004 McDonald et al 2005a Attardi 2006 Titov and Henderson 2007) 
The first is simplicity given that it is based on a single transition system and makes a single pass over the input whereas the pseudo-projective parsing technique involves preprocessing of training data and post-processing of parser output (Nivre and Nilsson 2005) 
A parsing algorithm for building the dependency analyses (Eisner 1996 SekineUchimotoand Isahara 2000) 2 
For comparison we also trained two parsers using Sp one that is strictly projective and one that uses the pseudo-projective parsing technique to recover non-projective dependencies in a post-processing step (Nivre and Nilsson 2005) 
Natural language parsing with data-driven dependency-based frameworks has received an increasing amount of attention in recent years (McDonald et al 2005 Buchholz and Marsi 2006 Nivre et al 2006) 
To overcome this McDonald and Pereira (2006) use a 15We leave labeled parsing experiments to future work 
F 1 =2 precision recall/(precision + recall) the figure we find that F 1 score decreases when dependency length increases as (McDonald and Nivre 2007) found 
Although the parser only derives projective graphs the fact that graphs are labeled allows non-projective dependencies to be captured using the pseudo projective approach of Nivre and Nilsson (2005) 
These settings match the evaluation setting in previous work such as (McDonald et al 2005a Koo et al 2008) 
We use the popular online learning algorithm of structured perceptron with parameter averaging (Collins 2002) 
To train models we used projectivized versions of the training dependency trees2 1We are grateful to the providers of the treebanks that constituted the data for the shared task (Hajic et al  2004 Aduriz et al  2003 Mart et al  2007 Chen et al  2003 Bohmova et al  2003 Marcus et al  1993 Johansson and Nugues 2007 Prokopidis et al  2005 Csendes et al  2005 Montemagni et al  2003 Oflazer et al  2003) 
Parsing algorithms which search the entire space (Eisner 1996 McDonald 2006) are restricted in the features they use to score a relation 
For our experiments we use MSTParser 043b 4 with 1-best projective decoding using the algorithm of Eisner (1996) and second order features 
NP-L = non-projective list-based P-L = projective list-based PP-L = pseudo-projective list-based P-E = projective arc-eager stack-based PP-E = pseudo-projective arc-eager stack-based P-S = projective arc-standard stack-based PP-S = pseudo-projective arc-standard stack-based McD = McDonald Lerman and Pereira (2006) Niv = Nivre et al 
Parsing technologies have improved considerably in the past few years and high-performance syntactic parsers are no longer limited to PCFG-based frameworks (Charniak 2000 Klein and Manning 2003 Charniak and Johnson 2005 Petrov and Klein 2007) but also include dependency parsers (McDonald and Pereira 2006 Nivre and Nilsson 2005 Sagae and Tsujii 2007) and deep parsers (Kaplan et al 2004 Clark and Curran 2004 Miyao and Tsujii 2008) 
Global-optimization parsing methods are another common approach (Eisner 1996 McDonald et al 2005) 
A key difference with previous work on shift reduce dependency (Nivre et al 2006) and CFG (Sagae and Lavie 2006b) parsing is that for CCG there are many more shift actions a shift action for each word-lexical category pair 
Eisner (1996) made the observation that if the head of each chart item is on the left or right periphery then it is possible to parse in O(n3) 
Eisner (1996) introduced a data-driven dependency parser and compared several probability models on (English) Penn Treebank data 
Thus transition-based parsers normally run in linear or quadratic time using greedy deterministic search or fixed-width beam search (Nivre et al 2004 Attardi 2006 Johansson and Nugues 2007 Titov and Henderson 2007) and graph-based models support exact inference in at most cubic time which is efficient enough to make global discriminative training practically feasible (McDonald et al 2005a McDonald et al 2005b) 
This technique is similar to the parser voting methods used by Sagae and Lavie (2006) 
Although (McDonald et al 2005) used the prefix of each word form instead of word form itself as features character-level features here for Chinese is essentially different from that 
Graph transformations for recovering nonprojective structures (Nivre and Nilsson 2005) 
Nivre and Nilsson (2005) presented a parsing model that allows for the introduction of non-projective edges into dependency trees through learned edge transformations within their memory-based parser 
In this paper we only used bigram-subtrees and the limited form of trigram-subtrees though in theory we can use k-gram-subtrees which are limited in the same way as our trigram subtrees in (k-1)th-order MST parsing models mentioned in McDonald and Pereira (2006) or use grandparent type trigram-subtrees in parsing models of Carreras (2007) 
Our out-of-domain data is the Wall Street Journal (WSJ) portion of the Penn Treebank (Marcus et al  1993) which consists of about 40000 sentences (one million words) annotated with syntactic information 
Despite its simplicity the partial correspondence approach proves very effective and leads to parsers that achieve unlabeled attachment scores that are only 5% behind the average UAS for Dutch in the CoNLL-X Shared Task (Buchholz and Marsi 2006) 
In particular one would assume that the score of a complete spanning tree Y for a given sentence whether probabilistically motivated or not can be decomposed as a sum of local scores for each link (a word pair) (Eisner 1996 Eisner and Satta 1999 McDonald et al 2005a) 
Sagae and Lavie (2006a) and Zeman and abokrtsk (2005) have observed that reversing the direction of stepwise parsers can be beneficial in parser combinations 
There are two main dependency parsing approaches (Nivre and McDonald 2008) 
See (Charniak 2000) for details 
This becomes parsing failures in practice (Nivre and Scholz 2004) leaving more than one fragments on stack 
Like context-free grammars projective dependency trees are not sufficient to represent all the linguistic phenomena observed in natural languages but they have the advantage of being efficiently parsable their parsing problem can be solved in cubic time with chart parsing techniques (Eisner 1996 Gomez-Rodrguez et al 2008) while in the case of general non-projective dependency forests it is only tractable under strong independence assumptions (McDonald et al 2005b McDonald and Satta 2007) 
Notably using a simple arc-factored parser at level 1 we obtain an exact O(n2) stacked parser that outperforms earlier approximate methods (McDonald and Pereira 2006) 
In particular Nivre and Scholz (2004) and Attardi (2006) have developed deterministic dependency parsers with linear complexity suitable for processing large amounts of text as required for example in information retrieval applications 
Before parsing POS tags are assigned to the input sentence using our reimplementation of the POStagger from Collins (2002) 
Either they employ a non-standard parsing algorithm that can combine non-adjacent substructures (McDonald et al 2005b Attardi 2006 Nivre 2007) or they try to recover non351 ROOT0 A1 a7 a4 a63 DET hearing2 a7 a4 a63 SBJ is3 a7 a4 a63 ROOT scheduled4 a7 a4 a63 VG on5 a7 a4 a63 NMOD the6 a7 a4 a63 DET issue7 a7 a4 a63 PC today8 a7 a4 a63 ADV 9 a63 a7 a4P Figure 1 Dependency tree for an English sentence (non-projective) 
Oracles for practical parsers can be obtained by training classifiers on treebank data (Nivre et al 2004) 
Comparison to Related Work Several ensemble models have been proposed for dependency parsing (Sagae and Lavie 2006 Hall et al 2007 Nivre and McDonald 2008 Attardi and DellOrletta 2009 Surdeanu and Manning 2010) 
McDonald and Pereira (2006) define a second-order dependency parsing model in which interactions between adjacent siblings are allowed and Carreras (2007) defines a second-order model that allows grandparent and sibling interactions 
Given the need for high-quality dependency parses in applications such as statistical machine translation (Xu et al 2009) natural language generation (Wan et al 2009) and text summarization evaluation (Owczarzak 2009) there is a corresponding need for high-quality dependency annotation for the training and evaluation of dependency parsers (Buchholz and Marsi 2006) 
We instead make use of a simple two-stage approach for extending the SS-SCM approach to the second-order parsing model of (Carreras 2007) 
Things become worse still in a parser like the one described in Charniak (2000) because it conditions on (and hence splits the dynamic programming states according to) features of the grandparent node in addition to the parent thus multiplying the number of possible dynamic programming states even more 
Models for data-driven dependency parsing can be roughly divided into two paradigms Graph-based and transition-based models (McDonald and Nivre 2007) 
Our probability model uses the parsing order proposed in (Nivre et al  2004) but instead of performing deterministic parsing as in (Nivre et al  2004) this ordering is used to define a generative history based model by adding word prediction to the Shift parser action 
For more information on the data sets see Buchholz and Marsi (2006) 
Practically all data-driven models that have been proposed for dependency parsing in recent years can be described as either graph-based or transition based (McDonald and Nivre 2007) 
Given the lattice and Gss lattice parsing is a straightforward generalization of the standard arc-factored dynamic programming algorithm from Eisner (1996) 
The second order algorithm of Carreras (2007) uses in addition to McDonald and Pereira (2006) the child of the dependent occurring in the sentence between the head and the dependent and the an edge to a grandchild 
Both English and Chinese sentences are tagged by the implementations of the POS tagger of Collins (2002) which trained on WSJ and CTB 50 respectively 
ULISSE was tested against the output of two really different datadriven parsers the firstorder Maximum Spanning Tree (MST) parser (McDonald et al 2006) and the DeSR parser (Attardi 2006) using Support Vector Machine as learning algorithm 
Carreras (2007) employs his own extension of Eisners algorithm for the case of projective trees and second-order models that include head grandparent relations 
Most previous dependency parsing models have focused on projective trees including the work of Eisner (1996) Collins et al 
Unsupervised dependency parsing has seen rapid progress recently with error reductions on English (Marcus et al 1993) of about 15% in six years (Klein and Manning 2004 Spitkovsky et al 2010) and better and better results for other languages (Gillenwater et al 2010 Naseem et al 2010) but results are still far from what can be achieved with small seeds language-specific rules (Druck et al 2009) or using cross-language adaptation (Smith and Eisner 2009 Spreyer et al 2010) 
In turn those features were inspired by successful previous work in firstorder dependency parsing (McDonald et al  2005) 
Then we substitute Step 3 as a supervised learning such as MIRA with a second-order parsing model (McDonald et al 2005a) which incorporates q1 as a real-values features 
The best phrase-structure parsing models represent generatively the joint probability P(xy) of sentence x having the structure y (Collins 1999 Charniak 2000) 
One of the surprising things discovered by this evaluation is that the best results are achieved by methods which are quite different from state-of-the-art models for constituent parsing eg the deterministic parsing method of (Nivre et al  2006) and the minimum spanning tree parser of (McDonald et al  2006) 
In this vein the CoNLL 2008 shared task sets the challenge of learning jointly both syntactic dependencies (extracted from the Penn Treebank (Marcus et al 1993) ) and semantic dependencies (extracted both from PropBank (Palmer et al 2005) c2008 
Deterministic dependency parsers which run in linear time have also been developed (Nivre & Scholz 2004 Attardi 2006) 
We conducted experiments with two data-driven parsers MaltParser (Nivre et al 2007b) and MSTParser (McDonald et al 2006) 
These results are similar in spirit to (Nivre and McDonald 2008) but with the following novel contributions a stacking interpretation a richer feature set that includes non-local features (shown here to improve performance) and a variety of stacking architectures 
Note that Models 1 and 2 have the same complexity as Carreras (2007) but strictly greater expressiveness for each sibling or grandchild part used in the Carreras (2007) factorization Model 1 defines an enclosing grand-sibling while Model 2 defines an enclosing tri-sibling or grand-sibling 
However current statistical dependency parsers provide worse results if the dependency length becomes longer (McDonald and Nivre 2007) 
Table 1 gives additional statistics for treebanks from the CoNLL-X shared task (Buchholz and Marsi 2006) 
In dependency reparsing we focus on unlabeled dependencies as described by Eisner (1996) 
Our labeled data comes from the Penn Treebank (Marcus et al  1993) and consists of about 40000 sentences from Wall Street Journal (WSJ) articles 153 annotated with syntactic information 
Comparing with the baselines we observe that our full model outperforms that of McDonald and Pereira (2006) and is in line with the most accurate dependency parsers (Nivre and McDonald 2008 Martins et al 2008) obtained by combining transition-based and graph-based parsers14 Notice that our model compared with these hybrid parsers has the advantage of not requiring an ensemble configuration (eliminating for example the need to tune two parsers) 
R O O T 
We evaluated the ISBN parser on all the languages considered in the shared task (Hajic et al  2004 Aduriz et al  2003 Mart et al  2007 Chen et al  2003 Bohmova et al  2003 Marcus et al  1993 Johansson and Nugues 2007 Prokopidis et al  2005 Csendes et al  2005 Montemagni et al  2003 Oflazer et al  2003) 
Freund and Schapire (1999) originally proposed the averaged parameter method it was shown to give substantial improvements in accuracy for tagging tasks in Collins (2002) 
There are some algorithms 1149 Figure 1 This is an example of dependency tree to determine these relations of each word to another words for instance the modified CKY algorithm (Eisner 1996) is used to define these relations for a given sentence 
Collins (2002b) gives convergence proofs for the methods Collins (2002a) directly compares the boosting and perceptron approaches on a named entity task and Collins and Duffy (2001 2002) use a reranking approach with kernels which allow representations of parse trees or labeled sequences in very-high-dimensional spaces 
First we implement a chart-based dynamic programming parser for the 2nd-ordered MST model and develop a training procedure based on the perceptron algorithm with averaged parameters (Collins 2002) 
For handling non-projective relations Nivre and Nilsson (2005) suggested applying a preprocessing step to a dependency parser which consists in lifting non-projective arcs to their head repeatedly until the tree becomes pseudo-projective 
However since most previous studies instead use the mean attachment score per word (Eisner 1996 Collins et al  1999) we will give this measure as well 
By contrast the nave CKY algorithm for this model is O(n5) which can be improved to O(n3) (Eisner 1996)6 The higher complexity of our algorithm is due to two factors first we have to maintain both h and h in one state because the current shift-reduce model can not draw features across different states (unlike CKY) and more importantly we group states by step in order to achieve increment ality and linear runtime with beam search that is not (easily) possible with CKY or MST 
Again since neither MIRA nor BPM outperforms the other on all measures we conclude that the results constitute a valiation of the results reported in (McDonald et al  2005a) 
For English we used the Penn Treebank version 30 (Marcus et al  1993) and extracted dependency relations by applying the head-finding rules of (Yamada and Matsumoto 2003) 
Then we will define the generative parsing model based on the algorithm of (Nivre et al  2004) and propose an ISBN for this model 
Then 42 will describe the application of three types of tree transformations while subsection 43 will examine the application of propagating syntactic features through a first-stage dependency tree a process that can also be seen as an application of stacked learning as tested in (Nivre and McDonald 2008 Martins et al 2008) 41 Feature engineering The original CoNLL-X format uses 10 different columns (see Figure 12) grouping the full set of morpho syntactic features in a single column 
As for overall performance both the exact and relaxed full model outperform the arcfactored model and the second order model of McDonald and Pereira (2006) with statistical significance (p < 001) according to Dan Bikels randomized method (http//wwwcisupennedu/dbikel/softwarehtml) 
We report experiments on twelve languages from the CoNLL-X shared task (Buchholz and Marsi 2006)5 All experiments are evaluated using the labeled attachment score (LAS) using the default settings6 Statistical significance is measured using Dan Bikels randomized parsing evaluation comparator with 10000 iterations7 The additional features used in the level 1 parser are enumerated in Table 1 and their various subsets are depicted in Table 2 
Dependency Parsing Algorithms For simplicity of implementation we use a standard CKY parser in the experiments although Eisners algorithm (Eisner 1996) and the Spanning Tree algorithm (McDonald et al 2005b) are also applicable 
For this we use the parsing strategy for projective dependency parsing introduced in (Nivre et al  2004) which is similar to a standard shift-reduce algorithm for context-free grammars (Aho et al  1986) 
These structures are equivalent to non-projective dependency parses (McDonald et al  2005b) and more generally could be relevant to any task that involves learning a mapping from a graph to an underlying spanning tree 
In graph-based parsing dependency trees are scored by factoring the tree into its arcs and parsing is performed by searching for the highest scoring tree (Eisner 1996 McDonald et al 2005b) 
A probabilistic shift reduce LR-like model such as the one used in our parser is different in many ways from a lexicalized PCFG-like model (using markov a grammar) such as those used in the Collins (1999) and Charniak (2000) parsers 
We will extend these experiments with the use of different parser features (Nivre and McDonald 2008 Martins et al 2008) 
In addition they used the parsing model by Carreras (2007) that applied second-order features on both sibling and grandparent interactions 
UAS means unlabeled attachment score (Buchholz and Marsi 2006) 
Recently dependency-based parsing has been applied to 13 different languages in the shared task of the 2006 Conference on Computational Natural Language Learning (CoNLL) (Buchholz and Marsi 2006) 
Consequently recent work in dependency parsing has been restricted to applications of second order parsers the most powerful of which (Carreras 2007) requires O(n4) time and O(n3) space while being limited to second-order parts 
McDonald and Pereira (2006) and McDonald and Satta (2007) describe complexity results for nonprojective parsing showing that parsing for a variety of models is NP-hard 
The second baseline is represented by the Sentence Length (SL) starting from the assumption demonstrated by McDonald and Nivre (2007) that long sentences are harder to analyse using statistical dependency parsers than short ones 
As a general result our experiments confirm previous studies on non-projective dependency parsing (Nivre and Nilsson 2005 Hall and Novk 2005 512 McDonald and Pereira 2006) The phenomenon of non-projectivity cannot be ignored without also ignoring a significant portion of real-world data (around 15a37for DDT and 23a37for PDT) 
Carreras (2007) extends the first-order model to incorporate a sum over scores for pairs of adjacent arcs in the tree yielding a second-order model 
The graph-based parser Following MSTParser (McDonald et al 2005 McDonald and Pereira 2006) we define the graph Variables agenda the beam for state items item partial parse tree output a set of output items indexprev word indexes Input x POS-tagged input sentence 
Nivre and Nilsson (2005) uses tree rewriting which is the most common technique 
With a projectivity constraint and arc factorization the parsing problem can be solved in cubic time by dynamic programming (Eisner 1996) and with a weaker tree constraint (permitting nonprojective parses) and arc factorization a quadratic-time algorithm exists (Chu and Liu 1965 Edmonds 1967) as shown by McDonald et al 
Unlike the original definition in (Nivre et al  2004) the Right-Arcr decision does not shift wj to the stack 
NLP community has recently seen a surge of interest in dependency parsing with several CoNLL shared tasks focusing on it (Buchholz and Marsi 2006 Nivre et al 2007) 
As a result graph-based parsers (including MSTParser) often limit the scope of their features to a small number of adjacent arcs (usually two) and/or resort to approximate inference (McDonald and Pereira 2006) 
A bottom-up algorithm proposed by (Nivre and Scholz 2004) is use for a deterministic dependency structure analysis 
The experimental parsing results presented in this paper have been obtained using MaltParser a freely available system for data-driven dependency parsing with state-of-the-art accuracy for most languages in previous evaluations (Buchholz and Marsi 2006 Nivre et al 2007a Nivre et al 2007b) 
We write act(i) to denote the feature representation extracted for action act at location i The model is trained using a variant of the structured perceptron (Collins 2002) similar to the algorithm of (Shen et al 2007 Shen and Joshi 2008) 
A is a replication of (Nivre and McDonald 2008) except for the modifications described in footnote 4 
The Berkeley parser (Petrov et al 2006) is a latent-variable PCFG parser MSTParser (McDonald et al 2006) is a graph-based dependency parser and MaltParser (Nivre et al 2006) is a transition-based dependency parser 
The parsers used in the experiments are MaltParser (Nivre et al  2004) and MSTParser (McDonald et al  2005) 
There has been extensive work on data-driven dependency parsingfor both projective parsing(Eisner 1996 Paskin 2001 Yamadaand Matsumoto 2003 Nivre and Scholz 2004 McDonaldet al 2005a) and non-projective parsing systems(Nivre and Nilsson2005Halland Novak 2005McDonald et al 2005b) 
Instead of performing exact inference by dynamic programming we incorporated the linear model and feature templates from McDonald and Pereira (2006) into our beam-search framework while adding new global features 
The three baselines are the second order model of McDonald and Pereira (2006) and the hybrid models of Nivre and McDonald (2008) and Martins et al 
Although the contributions of this paper are mainly theoretical we also present an empirical evaluation of the 2planar parser showing that it outperforms the projective parser on four data sets from the CoNLL-X shared task (Buchholz and Marsi 2006) 
It is easy to see that the main difference between the PA algorithms and the Perceptron algorithm (PC) (Collins 2002) as well as the MIRA algorithm (McDonald et al  2005a) is in line 9 
Kudo and Matsumoto (2002) report a two week learning time on a Japanese corpus of about 8000 sentences with SVM 
McDonald and Pereira (2006) developed a technique to rearrange edges in the tree in a postprocessing step after the projective parsing has taken place 
We employ the second-order projective graphbased parsing model of Carreras (2007) which is an extension of the projective parsing algorithm of Eisner (1996) 
A major benefit of this choice is computational efficiency an exhaustive search over all projective structures can be done in cubic greedy parsing in linear time (Eisner 1996 Nivre 2003) 
All of the convergence and generalization results in Collins (2002) depend on notions of separability rather than the size of GEN Two questions come to mind 
McDonald and Nivre (2007) describe how the systems behavioral differences are due to the different parsing algorithms implemented by the Shift Reduce and the MST parsing models 
We participated in the multilingual track of the CoNLL 2007 shared task (Nivre et al  2007) and evaluated the system on data sets of 10 languages (Hajic et al  2004 Aduriz et al  2003 Mart et al  2007 Chen et al  2003 Bohmova et al  2003 Marcus et al  1993 Johansson and Nugues 2007 Prokopidis et al  2005 Csendes et al  2005 Montemagni et al  2003 Oflazer et al  2003) 
The input for both systems is projectivized using the head+path schema (Nivre and Nilsson 2005) 
For direct comparison with the approach by Nivre and McDonald (2008) we present the results on the CoNLL-X corpora (Table 3) MST and MSTMalt are the results achieved by the MST parser and the MST parser using hints from Maltparser Malt and 04 05 06 07 08 09 1 0 5 10 15 20 25 30 35 F-Measure Dependency Length Left-to-Right_DeSR Revision_DeSR Figure 1 English 
Looking rst at learning times it is obvious that learning time depends primarily on the number of training instances which is why we can observe a difference of several orders of magnitude in learning time between the biggest training set (Czech) and the smallest training set (Slovene) 14 This is shown by Nivre and Scholz (2004) in comparison to the iterative arc-standard algorithm of Yamada and Matsumoto (2003) and by McDonald and Nivre (2007) in comparison to the spanning tree algorithm of McDonald Lerman and Pereira (2006) 
We created a dependency training corpus based on the Penn Treebank (Marcus et al  1993) or more specifically on the HPSG Treebank generated from the Penn Treebank (see section 22) 
Sagae and Lavie (2006) demonstrated that a simple combination scheme of the outputs of different parsers can obtain substantially improved accuracies 
The table also reports the scores obtained on the same data set by by the shift reduce parsers of Nivre and Scholzs (2004) and Yamada andMatsumoto(2003) andMcDonaldandPereiras second-order maximum spanning tree parser (McDonald & Pereira 2006) 
As expected Malt and MST have very similar accuracy for short sentences but Malt degrades more rapidly with increasing sentence length because of error propagation (McDonald and Nivre 2007) 
Although supervised learning methods bring stateof-the-art outcome for dependency parser inferring (McDonald et al 2005 Hall et al 2007) a large enough data set is often required for specific parsing accuracy according to this type of methods 
The approach was extended to labeled dependency parsing by Nivre Hall and Nilsson (2004) (for Swedish) and Nivre and Scholz (2004) (for English) using a different parsing algorithm rst presented in Nivre (2003) 
Bengoetxea and Gojenola (2010) discuss non-projective dependencies in Basque and show that the pseudo-projective transformation of (Nivre and Nilsson 2005) improves accuracy for dependency parsing of Basque 
The best result on this dataset to date (927% UAS) is that of Sagae and Lavie (Sagae & Lavie 2006) who use a parser which combines the predictions of several pre-existing parsers including McDonalds and Nivres parsers 
As a final note following Collins (2002) we used the averaged parameters from the training algorithm in decoding test examples in our experiments 
To overcome this Eisner (1996a) proposed a back-off strategy which reduces the conditioning of a model 
The dependency probability can then be defined as C(ij) = exp(w f(ij+))summation text r exp(w f(ijr)) = exp( summation text k wk fk(ij+))summation text r exp( summation text k wk fk(ijr)) (5) 22 Features for Classification The feature templates for the classifier are similar to those of 1st-ordered MST model (McDonald et al 2005a) 
For comparison the table also includes results for the two best performing systems in the original CoNLL-X shared task Malt-06 (Nivre et al 2006) and MST-06 (McDonald et al 2006) as well as the integrated system MSTMalt which is a graph-based parser guided by the predictions of a transition-based parser and currently has the best reported results on the CoNLL-X data sets (Nivre and McDonald 2008) 
All our experimental settings match previous work (Yamada and Matsumoto 2003 McDonald et al 2005 Koo et al 2008) 
Many of these are even in standardized formats (Buchholz and Marsi 2006 Nivre et al 2007) 
Moreover the deterministic dependency parser of Yamada and Matsumoto (2003) when trained on the Penn Treebank gives a dependency accuracy that is almost as good as that of Collins (1997) and Charniak (2000) 
We can reduce the time complexity to O(n3q3) by strictly adopting the DP structures in the parsing algorithm of Eisner (1996) 
The Danish and Dutch datasets were prepared for the CoNLL 2006 shared task (Buchholz and Marsi 2006) Arabic and Czech are from the 2007 shared task 
In supervised dependency parsing second order edge features provide improvements (McDonald and Pereira 2006 Riedel and Clarke 2006) moreover the feature-based approach is not limited to dependency parsing 
To combine the outputs of each parser we used the method of Sagae and Lavie (2006) 
Tournament All 9196 5744 SR algorithm (Sassano 2004) All 9148 5567 CC algorithm (Kudo and Matsumoto 2002) All 9147 5565 Combination CC and Relative preference Kudos (2005) 9166 5630 Relative preference (Kudo and Matsumoto 2005) Kudos (2005) 9137 5600 CC algorithm (Kudo and Matsumoto 2002) Kudos (2005) 9123 5559 Table 4 Dependency and sentence accuracy [%] using 24263 sentences as training data with all features comparison with Kudo(2005)s experiments 
The two dominating approaches have been graph-based parsing eg MST-parsing (McDonald et al 2005b) and transition-based parsing eg the MaltParser (Nivre et al 2006a) 
Dependency parsing can be used to provide a bare bones syntactic structure that approximates semanticsand it hast he additional advantage of admitting fast parsing algorithms (Eisner 1996 McDonald et al  2005b) with a negligible grammar constant in many cases 
Factorizations like that of Carreras (2007) obtain grandchild parts by augmenting spans with the indices of modifiers leading to limitations on 4The reason for the restriction is that in Model 2 grand siblings can only be derived via Figure 7(b) which does not recursively copy the grandparent index for reuse in smaller g-spans as Model 1 does in Figure 6(b) 
Though syntactic parsers for English are reported to have accuracies over 90% on the Wall Street Journal (WSJ) section of the Penn Treebank (PTB) (McDonald et al 2005 Sagae and Lavie 2006 Huang 2008 Carreras et al 2008) broad-coverage parsing is still far from being a solved problem 
Ensemble systems The error analysis presented in this paper could be used as inspiration for more refined weighting schemes for ensemble systems of the kind proposed by Sagae and Lavie (2006) making the weights depend on a range of linguistic and graph-based factors 
The second extension is to apply the approach to second-order parsing models more specifically the model of (Carreras 2007) using a two-stage semi-supervised learning approach 
For English we use the automatically-assigned POS tags produced by an implementation of the POS tagger of Collins (2002) 
English we used the Wall Street Journal section of the Penn Treebank (Marcus et al  1993) 
Finally Table 2 compares loopy BP to a previously proposed hill-climbing method for approximate inference in non-projective parsing McDonald and Pereira (2006) 
Tractability is usually ensured by strong factorization assumptions like the one underlying the arc-factored model (Eisner 1996 McDonald et al 2005) which forbids any feature that depends on two or more arcs 
Some of the more popular and more accurate of these approaches to data-driven parsing (Charniak 2000 Collins 1997 Klein and Manning 2002) have been based on generative models that are closely related to probabilistic context free grammars 
It is worth noting that the parsing units in this treebank are sometimes smaller than conventional sentence units which partly explains the low average number of tokens per sentence (Buchholz and Marsi 2006) 
Therehas been extensive work on data-driven dependency parsingfor both projective parsing(Eisner 1996 Paskin2001 Yamadaand Matsumoto 2003 Nivre and Scholz2004 McDonaldet al 2005a)and non-projective parsing systems(Nivre andNilsson2005HallandNovak 2005McDonald et al 2005b) 
This is done by performing higher order parsing which is shown to improve parsing accuracy but also increase parsing complexity (Carreras 2007 Koo and Collins 2010)2 Transition based parsing is attractive because it can use parse information without increasing complexity (Nivre 2006) 
Subsequent research began to focus more on conditional models of parse structure given the input sentence which allowed discriminative training techniques such as maximum conditional likelihood (ie maximum entropy ) to be applied (Ratnaparkhi 1999 Charniak 2000) 
Indirect support for this assumption can be gained from previous experiments with Swedish data where almost the same accuracy (85% unlabeled attachment score) has been achieved with a treebank which is much smaller but which contains proper dependency annotation (Nivre et al  2004) 
We report experiments on seven languages six (Danish Dutch Portuguese Slovene Swedish and Turkish) from the CoNLL-X shared task (Buchholz and Marsi 2006) and one (English) from the CoNLL-2008 shared task (Surdeanu et al 2008)8 All experiments are evaluated using the unlabeled attachment score (UAS) using the default settings9 We used the same arc-factored featuresasMcDonaldetal(2005)(include dint he MSTParser toolkit10) for the higher-order models described in 3335 we employed simple higher order features that look at the word part-of-speech tag and (if available) morphological information of the words being correlated through the indicator variables 
Transition based parsers model the sequence of decisions of a shift-reduce parser given previous decisions and current state and parsing is performed by greedily choosing the highest scoring transition out of each successive parsing state or by searching for the best sequence of transitions (Ratnaparkhi et al 1994 Yamada and Matsumoto 2003 Nivre et al 2004 Sagae and Lavie 2005 Hall et al 2006) 
In order to get a better understanding of these matters we replicate parts of the error analysis presented by McDonald and Nivre (2007) where parsing errors are related to different structural properties of sentences and their dependency graphs 
Much of this work has been fueled by the availability of large corpora annotated with syntactic structures especially the Penn Treebank (Marcus et al  1993) 
Dependency treebanks are becoming available in many languages and several approaches to dependency parsing on multiple languages have been evaluated in the CoNLL 2006 and 2007 shared tasks (Buchholz & Marsi 2006 Nivre et al  2007) 
This corresponds to the Baseline condition in Nivre and Nilsson (2005) 
McDonald and Pereira (2006) define this as a second-order Markov assumption 
Our results yield relative error reductions of roughly 27% (English) and 20% (Czech) over McDonald and Pereira (2006)s second-order supervised dependency parsers and roughly 9% (English) and 7% (Czech) over the previous best results provided by Koo et 
We use a global linear model to score candidate items trained discriminatively with the averaged perceptron (Collins 2002) 
R O O T I a t e t h e f i s h w i t h a f o r k  Figure 1 Example for dependency structure 21 Parsing approach For dependency parsing there are two main types of parsing models (Nivre and McDonald 2008) graph-based model and transition-based model which achieved state-of-the-art accuracy for a wide range of languages as shown in recent CoNLL shared tasks (Buchholz et al 2006 Nivre et al 2007) 
The algorithms of Kudo and Matsumoto (2002) Yamada and Matsumoto (2003) and Nivre (2003 2006b) all belong to this family 
The two main approaches to data-driven dependency parsing are transition based dependency parsing (Nivre 2003 Yamada and Matsumoto 2003 Titov and Henderson 2007) and maximum spanning tree based dependency parsing (Eisner 1996 Eisner 2000 McDonald and Pereira 2006) 
As shown by McDonald and Nivre (2007) the Single Malt parser tends to suffer from two problems error propagation due to the deterministic parsing strategy typically affecting long dependencies more than short ones and low precision on dependencies originating in the artificial root node due to fragmented parses9 The question is which of these problems is alleviated by the multiple views given by the component parsers in the Blended system 
We will focus on tree transformations that combine preprocessing with post-processing and where the parser is treated as a black box such as the pseudo-projective parsing technique proposed by Nivre and Nilsson (2005) and the tree transformations investigated in Nilsson et al 
Right-to-left parsing has been used as part of ensemble-based parsers (Sagae & Lavie 2006 Hall et al 2007) 
We test our parsing models on the CONLL-2007 (Hajic et al  2004 Aduriz et al  2003 Mart et al  2007 Chen et al  2003 Bohmova et al  2003 Marcus et al  1993 Johansson and Nugues 2007 Prokopidis et al  2005 Csendes et al  2005 Montemagni et al  2003 Oflazer et al  2003) data set on various languages including Arabic Basque Catalan Chinese English Italian Hungarian and Turkish 
Dependency Parsing the task of inferring a dependency structure over an input sentence has gained a lot of research attention in the last couple of years due in part to to the two CoNLL shared tasks (Nivre et al 2007 Buchholz and Marsi 2006) in which various dependency parsing algorithms were compared on various data sets 
In fact our approach can also be applied to other parsers such as (Yamada and Matsumoto 2003)s parser (McDonald et al 2006)s parser and so on 
We use the discriminative perceptron learning algorithm (Collins 2002 McDonald et al 2005) to train the values of vectorw 
A look at the performance sheet in the contest shows that two systems with quite different approaches (one using deterministic parsing with SVM and the other using MIRA with nondeterministic and dynamic programming based MST approach ) performed with good results (McDonald et al  2006 Nivre et al  2006) 
We present a statistical parser that is based on a shift-reduce algorithm like the parsers of Sagae and Lavie (2005) and Nivre and Scholz (2004) but performs a best-first search instead of pursuing a single analysis path in deterministic fashion 
Although the best published results for the Collins parser is 80% UAS (Collins 1999) this parser reaches 82% when trained on the entire training data set and an adapted version of Charniaks parser (Charniak 2000) performs at 84% (Jan Hajic pers 
Corresponding manipulations in the form of tree transformations for dependency-based parsers have recently gained more interest (Nivre and Nilsson 2005 Hall and Novak 2005 McDonald and Pereira 2006 Nilsson et al  2006) but are still less studied partly because constituency-based parsing has dominated the field for a long time and partly because dependency structures have less structure to manipulate than constituent structures 
The results on Chinese are obtained on two different data sets Chinese Treebank 40 and Chinese Treebank 50 as noted3 Table 1 shows that the results I am able to achieve on English are competitive with the state of the art but are still behind the best results of (McDonald and Pereira 2006) 
An advantage of graph-based methods is that tractable inference enables the use of standard structured learning techniques that globally set parameters to maximize parsing performance on the training set (McDonald et al 2005a) 
In our experiments we implemented our systems on the MSTParser6 and extended with the parent-child-grandchild structures (McDonald and Pereira 2006 Carreras 2007) 
The first stage is based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages 
Deterministic parsing algorithms for building dependency graphs (Kudo and Matsumoto 2002 Nivre 2003 Yamada and Matsumoto 2003) Table3 Unlabeled attachment scores for different choices for morphological features 
The Turkish Treebank has recently been parsed by 17 research groups in the CoNLLX shared task on multilingual dependency parsing (Buchholz and Marsi 2006)where it was seen as the most difcult language by the organizers and most of the groups 
In order to make the comparison more fair we therefore also evaluate pseudo-projective versions of the latter algorithms making use of graph transformations in pre-and post-processing to recover nonprojective dependency arcs following Nivre and Nilsson (2005) 
Consider for instance algorithms for converting the phrase-structure trees in the Penn Treebank (Marcus et al 1993) into dependency structures 
In the feature selection we follow a bit more McDonald and Pereira (2006) since we have in addition the lemmas morphologic features and the distance between the word forms 
Such models are commonly referred to as edge-factored since their parameters factor relative to individual edges of the graph (Paskin 2001 McDonald et al 2005a)Edge-factored model shave many computational benefitsmost notably that inference for nonprojective dependency graphs can be achieved in polynomial time(McDonaldet al 2005b)Theprimary problem in treating each dependency as independent is that it is not a realistic assumption 
Interestingly though the two systems have similar accuracies overall there is a clear distinction between the kinds of errors each system makes which we argue is consistent with observations by McDonald and Nivre (2007) 
Again we find the clearest patterns in the graphs for precision where Malt has very low precision near the root but improves with increasing depth while MST shows the opposite trend (McDonald and Nivre 2007) 
The baseline features capture information about the lexical items and their part of speech (POS) tags (as defined in (McDonald et al 2005)) 
McDonald and Nivre (2007) showed that the MSTParser and MaltParser produce different errors 
The rst strictly incremental parser of this kind was described in Nivre (2003) and used for classier-based parsing of Swedish by Nivre Hall and Nilsson (2004) and English by Nivre and Scholz (2004) 
For the second-order parsing experiments we used the Carreras (2007) parser 
We can then state the following theorem (see (Collins 2002) for a proof) Theorem 1 For any training sequence (xiyi) that is separable with margin for any value of T then for the perceptron algorithm in figure 1 Ne R 2 2 where R is a constant such that 8i8z 2 GEN(xi) jj (xiyi) (xiz)jj R This theorem implies that if there is a parameter vector U which makes zero errors on the training set then after a finite number of iterations the training algorithm will converge to parameter values with zero training error 
Unfortunately it is not possible to directly compare the parsers accuracy with most popular constituent parsers such as the Charniak (2000) and Berkeley (Petrov et al 2006 Petrov and Klein 2007) parsers9 both because they do not produce functional tags for subjects direct objects etc which are required for the final script of the constituent-to-dependency conversion routine and because they determine part-of-speech tags in conjunction with the parsing 
As a concrete example Figure 4 simulates an edge-factored model (Eisner 1996 McDonald et al 2005a) using shift-reduce with dynamic programming which is similar to bilexical PCFG parsing using CKY (Eisner and Satta 1999) 
The parsing algorithms used in Carreras (2007) independently find the left and right dependents of a word and then combine them later in a bottomup style based on Eisner (1996) 
As learning algorithm we use Perceptron tailored for structured scenarios proposed by Collins (2002) 
Table 1 shows the feature templates from the MSTParser (McDonald and Pereira 2006) which are defined in terms of the context of a word its parent and its sibling 
Classier-based dependency parsing was pioneered by Kudo and Matsumoto (2002) for unlabeled dependency parsing of Japanese with head-nal dependencies only 
Ensemble-based methods have attracted a lot of attention in dependency parsing recently (Sagae and Lavie 2006 Hall et al 2007 Nivre and McDonald 2008 Martins et al 2008 Fishel and Nivre 2009 Surdeanu and Manning 2010) 
To learn arc scores these models use large-margin structured learning algorithms (McDonald et al  2005a) which optimize the parameters of the model to maximize the score margin between the correct dependency graph and all incorrect dependency graphs for every sentence in a training set 
Conversion to Dependencies 321 Syntactic Dependencies There exists no large-scale dependency treebank for English and we thus had to construct a dependency-annotated corpus automatically from the Penn Treebank (Marcus et al 1993) 
Follow the edge based factorization method (Eisner 1996) we factorize the score of a dependency tree s(xy) into its dependency edges and design a dynamic programming algorithm to search for the candidate parse with maximum score 
Previous work (McDonald and Pereira 2006 Carreras 2007) has shown that second-order parsing models which include information from sibling or grandparent relationships between dependencies can give significant improvements in accuracy over first-order parsing models 
This algorithm has a runtime of O(n3) and has been employed successfully in both generative and discriminative parsing models (Eisner 1996 McDonald et al  2005) 
Since deterministic dependency parsing has previously been shown to be competitive in terms of parsing accuracy (Yamada and Matsumoto 2003 Nivre et al  2004) we believe that this is a promising approach for situations that require parsing to be robust efficient and (almost) incremental 
The most popular strategy for capturing nonprojective structures in data-driven dependency parsing is to apply some kind of post-processing to the output of a strictly projective dependency parser as in pseudo-projective parsing (Nivre and Nilsson 2005) corrective modeling (Hall and Novak 2005) or approximate non-projective parsing (McDonald and Pereira 2006) 
Deterministic dependency parsing (Nivre et al  2004 Yamada and Matsumoto 2003) can apply global constraints by conditioning attachment decisions on the intermediate parse built 
We will briefly review the perceptron algorithm and its convergence properties see Collins (2002) for a full description 
We used the MSTParser (McDonald et al 2006) which achieved top results in the CoNLL 2006 (CoNLL-X) shared task as a base dependency parser 
The table also confirms the commonly known fact (Yamada and Matsumoto 2003 McDonald et al 2005) that constituency parsers are more accurate at producing dependencies than dependency parsers (at least when the dependencies were produced by a deterministic transformation of a constituency treebank as is the case here) 
This is in contrast to other non-projective methods such as that of Nivre and Nilsson (2005) who implement non-projectivity in a pseudo-projective parser with edge transformations 
Our baseline features (baseline) are very similar to those described in (McDonald et al 2005a Koo et al 2008) these features track word and POS bigrams contextual features surrounding dependencies distance features and so on 
Although the parser only derives projective graphs the fact that these graphs are labeled allows non-projective dependencies to be captured using the pseudo-projective approach of Nivre and Nilsson (2005) (section 34) 
Both models have been used to achieve state-of-the-art accuracy for a wide range of languages as shown in the CoNLL shared tasks on dependency parsing (Buchholz and Marsi 2006 Nivre et al 2007) but McDonald and Nivre (2007) showed that a detailed error analysis reveals important differences in the distribution of errors associated with the two models 
The model is enhanced for non-projective languages by Nivre and Nilsson (2005) 
The score of a tree for second order parsing is now s(xy) = summation display (ikj)y s(ikj) where k and j are adjacent same-side children of i in the tree y The second-order model allows us to condition onthe mostrecent parsing decision thatis the last dependent picked up by a particular word which is analogous to the the Markov conditioning of in the Charniak parser (Charniak 2000) 
We adapted this system to first perform unlabeled parsing then label the arcs using a log-linear classifier with access to the full unlabeled parse (McDonald et al 2005a McDonald et al 2005b McDonald and Pereira 2006) 
These are passed into a fast algorithm for maximum spanning tree (Tarjan 1977) or maximum projective spanning tree (Eisner 1996) 
In our experiments we use the 1-best MIRA algorithm (McDonald and Pereira 2006)1 as a 1We used a slightly modified version of 1-best MIRA whose difference can be found in the third line in Eq 
An online learning algorithm considers a single training instance for each update to the weight vector w We use the common method of setting the final weight vector as the average of the weight vectors after each iteration (Collins 2002) which has been shown to alleviate overfit ting 
Figure 1 gives an example dependency graph for the sentence Mr Tomash will remain as a director emeritus which has been extracted from the Penn Treebank (Marcus et al  1993) 
Most of the graph-based parsers were trained using an online inference-based method such as passive aggressive learning (Nguyen et al  2007 Schiehlen and Spranger 2007) averaged perceptron (Carreras 2007) or MIRA (Shimizu and Nakagawa 2007) while some systems instead used methods based on maximum conditional likelihood (Nakagawa 2007 Hall et al  2007b) 
Givenann-wordinput sentence theparser begins by scoring each of the O(n2) possible edges and then seeks the highest-scoring legal dependency tree formed by any n1 of these edges using an O(n3) dynamic programming algorithm (Eisner 1996) for projective trees 
The projectivity constraint also leads to favourable parsing complexities chart-based parsing of projective dependency grammars can be done in cubic time (Eisner 1996) hard-wiring projectivity into a deterministic dependency parser leads to linear-time parsing in the worst case (Nivre 2003) 
Weal so report an evaluation on all thirteen languages of the CoNLL-X shared task(Buchholz&Marsi 2006) for comparison with the results by Nivre and McDonald (2008) 
We have used the 10 smallest data sets from CoNNL-X (Buchholz and Marsi 2006) in our experiments 
Syntactic representation The dependency treebank we use is a conversion of the English WSJ treebank (Marcus et al 1993) to dependency structure using the procedure described in (Johansson and Nugues 2007) 
Local models without global constraints are therefore mislead into deadend interpretations from which they cannot recover (McDonald and Nivre 2007) 
For PTREE (projective) it is the inside-outside version of a dynamic programming algorithm (Eisner 1996) 
A more recent approach (Nivre and McDonald 2008) combined MSTParser and MaltParser by using the output of one parser for features in the other 
Table 2 shows the results for English projective dependency trees extracted from the Penn Treebank (Marcus et al  1993) using the rules of Yamada and Matsumoto (2003) 
We test on two stateof-the art parsers MST We modified the publicly-available MST parser (McDonald et al 2005)6 to employ our filters before carrying out feature extraction 
(Kromann 2003) the Alpino Treebank of Dutch (van der Beek et al  2002) the TIGER Treebank of German (Brants et al  2002) the Floresta Sintactica of Portuguese (Afonso et al  2002) and the Slovene Dependency Treebank (Dzeroski et al  2006)4 The data sets used are the training sets from the CoNLLX Shared Task on multilingual dependency parsing (Buchholz and Marsi 2006) with 20% of the data reserved for testing using a pseudo-random split 
This includes work that integrates second and even third order features (McDonald et al 2006 Carreras 2007 Koo and Collins 2010) 
Both the graph-based (McDonald et al 2005a McDonald and Pereira 2006 Carreras et al 2006) and the transition-based (Yamada and Matsumoto 2003 Nivre et al 2006) parsing algorithms are related to our word-pair classification model 
This type of domain adaptation is reminiscent of self-training (McClosky et al 2006a Huang and Harper 2009) and co-training (Blum and Mitchell 1998 Sagae and Lavie 2006) except that the goal here is not to further improve the performance of the very best model 
The observed time complexity of our DP parser is in fact linear compared to the super linear complexity of Charniak MST (McDonald et al 2005b) and Berkeley parsers 
We applied the above two marginal-based training algorithms to six languages with varying degrees of non-projectivity using datasets obtained from the CoNLL-X shared task (Buchholz and Marsi 2006) 
Averaging has been shown to help reduce overfit ting (McDonald et al  2005a Collins 2002) 
Combinations of graph-based and transition-based models for data-driven dependency parsing have previously been explored by Sagae and Lavie (2006) who report improvements of up to 17 percentage points over the best single parser when combining three transition-based models and one graph-based model for unlabeled dependency parsing evaluated on data from the Penn Treebank 
English UAS Complete Y&M2003 903 384 CO2006 908 376 Hall2006 894 364 Wang2007 892 344 Z&C2008 921 454 KOO08-dep1c 9223 KOO08-dep2c 9316 Carreras2008 935 Ord1 9095 3745 Ord1s 9176 4068 Ord1c 9188 4071 Ord1i 9168 4143 Ord1sc 9220 4298 Ord1sci 9260 4428 Ord2 9171 4288 Ord2s 9251 4619 Ord2c 9240 4408 Ord2i 9212 4437 Ord2sc 9270 4656 Ord2sci 9316 4715 Table 2 Dependency parsing results for English for our parsers and previous work To demonstrate that our approach and other work are complementary we thus implemented a system using all the techniques we had at hand that used subtreeand cluster-based features and applied the integrating method of Nivre and McDonald (2008) 
Parsing method This chapter presents a basic parsing algorithm proposed by (Nivre and Scholz 2004) 
This feature template is almost the same as the one used by McDonald and Pereira (2006) 
See for example (Lombardi 1996 Eisner 1996) who also discuss Early-style parsers for projective dependency grammars 
The fact that our model defines a probability model over parse trees unlike the previous state-ofthe-art methods (Nivre et al  2006 McDonald et al  2006) makes it easier to use this model in applications which require probability estimates eg in language processing pipelines 
Pseudo-projective parsing was proposed by Nivre and Nilsson (2005) as a way of dealing with nonprojective structures in a projective data-driven parser 
The two main approaches to dependency parsing are transition based dependency parsing (Nivre 2003 Yamada and Matsumoto 2003 Titov and Henderson 2007) and maximum spanning tree based dependency parsing (Eisner 1996 Eisner 2000 McDonald and Pereira 2006) 
That algorithm in turn is similar to the dependency parsing algorithm of Nivre and Scholz (2004) but it builds a constituent tree and a dependency tree simultaneously 
In this work we use the averaged perceptron algorithm (Collins 2002) since it is an online algorithm much simpler and orders of magnitude faster than Boosting and MaxEnt methods 
Recent work has successfully developed dependency parsing models for many languages using supervised learning algorithms (Buchholz and Marsi 2006 Nivre et al 2007) 
This is the best automatically learned part-of-speech tagging result known to us representing an error reduction of 44% on the model presented in Collins (2002) using the same data splits and a larger error reduction of 121% from the more similar best previous loglinear model in Toutanova and Manning (2000) 
As a result the dependency parsing problem is written G = argmax G=(VA) summation display (ijl)A s(ijl) This problem is equivalent to finding the highest scoring directed spanning tree in the complete graph over the input sentence which can be solved in O(n2) time (McDonald et al 2005b) 
In general dependency parsers (McDonald et al 2006 Nivre et al 2007) seem to suffer more from this domain change than constituency parsers (Charniak and Johnson 2005 Petrov et al 2006) 
We will also explore ways of combining graph-based and transition-based parsers along the lines of Nivre and McDonald (2008) 
Therehas been extensive work on data-driven dependency parsingfor both projective parsing(Eisner 1996Paskin2001Yamadaand Matsumoto 2003 Nivre and Scholz2004 McDonaldet al 2005a)and non-projective parsing systems(Nivre andNilsson2005HallandNovak2005McDonald et al 2005b) 
We present a comparison of three statistical parsing architectures that output typed dependencies for French one constituency-based architecture featuring the Berkeley parser (Petrov et al 2006) and two dependency-based systems using radically different parsing methods MSTParser (McDonald et al 2006) and MaltParser (Nivre et al 2006) 
The system with online learning and Nivres parsing algorithm was trained on the data released by CoNLL Shared Task Organizers for all the ten languages (Hajic et al  2004 Aduriz et al  2003 Mart et al  2007 Chen et al  2003 Bohmova et al  2003 Marcus et al  1993 Johansson and Nugues 2007 Prokopidis et al  2005 Csendes et al  2005 Montemagni et al  2003 Oflazer et al  2003) 
This includes work on phrase structure parsing (Collins 1997 Charniak 2000 Petrov et al 2006) dependency parsing (McDonald et al 2005 Nivre et al 2006) as well as a number of other formalisms (Clark and Curran 2004 Wang and Harper 2004 Shen and Joshi 2008) 
Except these three languages we use software of projectivization/deprojectivization provided by Nivre and Nilsson (2005) for other languages 
However evaluations on the widely used WSJ corpus of the Penn Treebank (Marcus et al  1993) show that the accuracy of these parsers still lags behind the state-of-theart 
We will evaluate the parser on both the full PTB (Marcus et al 1993) and on a sense annotated subset of the Brown Corpus portion of PTB in order to investigate the upper bound performance of the models given gold-standard sense information as in Agirre et al 
We evaluate the accuracy of HPSG parsing with dependencyconstraintsontheHPSGTreebank(Miyao et al  2003) which is extracted from the Wall Street Journal portion of the Penn Treebank (Marcus et al  1993)1 
Compared to graph-based dependency parsing it typically offers linear time complexity and the comparative freedom to define non-local features as exemplified by the comparison between MaltParser and MSTParser (Nivre et al 2006b McDonald et al 2005 McDonald and Nivre 2007) 
Averaging has been shown to help reduce overfit ting (Collins 2002) 
Nivre and Scholz (2004) proposed a variant of the model of Yamada and Matsumoto that reduces the complexity from the worst case quadratic to linear 
The recent advances in parsing have achieved parsers with 3 ()On time complexity without the grammar constant (McDonald et al  2005) 
On the other hand the best available parsers trained on the Penn Treebank those of Collins (1997) and Charniak (2000) use statistical models for disambiguation that make crucial use of dependency relations 
Algorithm 2 Average multiclass perceptron input S= (xiyi)N0k =vector0 kY for t = 1 to T do choose j Et ={rY xjtrxjtyj} if|Et|> 0 then t+1r = tr xj|Et| rEt t+1yj = tyj +xj output k = 1T summationtextt tk kY 34 Higher-order feature spaces Yamada and Matsumoto (2003) and McDonald and Pereira (2006) have shown that higher-order feature representations and modeling can improve parsing accuracy although at significant computational costs 
If the trees are constrained to be projective EM is easily applied using the inside-outside variant of the parsing algorithm described by Eisner (1996) to compute the marginal probability 
This is a pattern that has been observed in previous evaluations of the parsers and can be explained by the global learning and inference strategy of MSTParser and the richer feature space of MaltParser (McDonald and Nivre 2007) 
To train the model we use the averaged perceptron algorithm (Collins 2002) 
The parsing methodology investigated here has previously been applied to Swedish where promising results were obtained with a relatively small treebank (approximately 5000 sentences for training) resulting in an attachment score of 847% and a labeled accuracy of 806% (Nivre et al  2004)1 However since there are no comparable results available for Swedish it is difficult to assess the significance of these findings which is one of the reasons why we want to apply the method to a benchmark corpus such as the the Penn Treebank even though the annotation in this corpus is not ideal for labeled dependency parsing 
MST uses Chu-LiuEdmonds (Chu and Liu 1965 Edmonds 1967) Maximum Spanning Tree algorithm for nonprojective parsing and Eisner's algorithm for projective parsing (Eisner 1996) 
The data are from 2006/7 CoNLL shared tasks (Buchholz and Marsi 2006 Nivre et al 2007a) where punctuation was identified by the organizers who also furnished disjoint train/test splits 
Combinatorial algorithms (Chu and Liu 1965 Edmonds 1967) can solve this problem in cubic time4 If the dependency parse trees are restricted to be projective cubic-time algorithms are available via dynamic programming (Eisner 1996) 
There has been extensive work on data-driven dependency parsingfor both projective parsing(Eisner 1996 Paskin 2001 Yamada and Matsumoto 2003 Nivre and Scholz 2004 McDonaldet al 2005a) and non-projective parsing systems (Nivre and Nilsson2005Hall and Novak 2005McDonald et al 2005b) 
For example Sagae and Lavie (2006) displayed that combining the predictions of both parsing models can lead to significantly improved accuracies 
For the baseline systems we used the firstand second-order (parent-sibling) features that were used in McDonald and Pereira (2006) and other second-order features (parent-child-grandchild) that were used in Carreras (2007) 
Classifier-based dependency parsers (Yamada and Matsumoto 2003 Nivre and Scholz 2004) learn from an annotated corpus how to select an appropriate sequence of Shift/Reduce actions to construct the dependency tree for a sentence 
In the experiments below we employ a data-driven deterministic dependency parser producing labeled projective dependency graphs3 previously tested on Swedish (Nivre et al  2004) and English (Nivre and Scholz 2004) 
Nivre and McDonald (2008) instead use hints from one parse as features in a second parse exploiting the complementary properties of graph-based parsers (Eisner 1996 McDonald et al 2005) and transition-based dependency parsers (Yamada & Matsumoto 2003 Nivre & Scholz 2004) 
To that end there are two dominant approaches graph-based methods characterized by arc features in an exhaustive search and transition-based methods characterized by operational features in a greedy search (McDonald and Nivre 2007) 
Following last years shared task practice (Buchholz and Marsi 2006) we use the following definition of projectivity An arc (i j) is projective iff all nodes occurring between i and j are dominated by i (where dominates is the transitive closure of the arc relation) 
With a linear scoring function the parser solves parse(s) = argmaxts summation display [hm]t w f(hms) The weights w are typically learned using an online method such as an averaged perceptron (Collins 2002) or MIRA (Crammer and Singer 2003) 
All three of the models are based on versions of the Carreras (2007) parser so modifying these methods to work with our new third-order parsing algorithms would be an interesting topic for future research 
The feature function is a second-order edge factored representation (McDonald and Pereira 2006 Carreras 2007) 
This is part of the explanation of why (Charniak 2000) finds that early generation of head tags as in (Collins 1999) is so beneficial 
Unlike the deterministic parsers above this parser uses a dynamic programming algorithm (Eisner 1996) to determine the best tree so there is no difference between presenting the input from left-to-right or right-to-left 
For parsing features we follow standard practice for graph-based dependency parsing (McDonald 2006 Carreras 2007 Koo and Collins 2010) 
Note that the approximations to maximizing over spanning trees with second-order features proposed by McDonald and Pereira (2006) do not permit estimating the clusters as part of the same process as weight estimation (at least not without modification) 
Shift-reduce parsers have become popular for dependency parsing building on the initial work of Yamada and Matsumoto (2003) and Nivre and Scholz (2004) 
This algorithm can thus be viewed as a large-margin version of the perceptron algorithm for structured outputs Collins (2002) 
To regularize the model we take as the final model the average of all weight vectors posited during training (Collins 2002) 
To match previous work (McDonald and Pereira 2006 Koo et al 2008) we split the data into a training set (sections 2-21) a development set (Section 22) and a test set (section 23) 
Length of dependency links McDonald and Nivre (2007) report that statistical parsers have a drop in 116 accuracy when analysing long distance dependencies 
The experimental results in (McDonald and Nivre 2007) show a negative impact on the parsing accuracy from too long dependency relation 
Similar observations regarding the effect of model order have also been made by Carreras (2007) 
This contrasts with the online learning algorithms used in previous work with spanning-tree models (McDonald et al  2005b) 
The reasons for this interest are manifold the availability of shared task data from various CoNLL conferences (among others (Buchholz and Marsi 2006 Hajic et al 2009)) comprising collections of languages based on a single representation format has certainly been instrumental 
After all iterations the algorithm computes the average of v  which reduces the effect of overfit ting (Collins 2002) 
For experiment on English we used the English Penn Treebank (PTB) (Marcus et al 1993) and the constituency structures were converted to dependency trees using the same rules as (Yamada and Matsumoto 2003) 
The second extension is to apply the approach to second order parsing models such as those described in (Carreras 2007) using a twostage semi-supervised learning approach 
The following treebanks were used for training the parser (Aduriz et al  2003 Bhmov et al  2003 Chen et al  2003 Haji et al  2004 Marcus et al  1993 Mart et al  2002 Montemagni et al 2003 Oflazer et al  2003 Prokopidis et al  2005 Csendes et al  2005) 
For example the parser of McDonald and Pereira (2006) defines parts for sibling interactions such as the trio plays Elianti and  in Figure 1 
The data sets derived from the original treebanks (section 3) were in the same column-based format as for the 2006 shared task (Buchholz and Marsi 2006) 
Most subsequent works on shift-reduce or transition-based dependency parsing followed arc-eager (Nivre and Scholz 2004 Zhang and Clark 2008) which now becomes the dominant style 
Thus the Penn Treebank of American English (Marcus et al  1993) has been used to train and evaluate the best available parsers of unrestricted English text (Collins 1999 Charniak 2000) 
For languages with treebanks supervised models give the state-of-the-art performance in dependency parsing (McDonald and Pereira 2006 Nivre et al 2006 Koo and Collins 2010 Martins et al 2010) and constituent parsing (Collins 2003 Charniak and Johnson 2005 Petrov et al 2006) 
Several approaches to dependency parsing on multiple languages have been evaluated in the CoNLL-X Shared Task (Buchholz & Marsi 2006) 
In many dependency parsing models such as (Eisner 1996) and (MacDonald et al  2005) the score of a dependency tree is the sum of the scores of the dependency links which are computed independently of other links 
The base parsing models are either independently trained (Sagae and Lavie 2006 Hall et al 2007 Attardi and DellOrletta 2009 Surdeanu and Manning 2010) or their training is integrated eg using stacking (Nivre and McDonald 2008 Attardi and DellOrletta 2009 Surdeanu and Manning 2010) 
Parsers that attempt to disambiguate the input completely full parsing typically first employ some kind of dynamic programming algorithm to derive a packed parse forest and then applies a probabilistic top-down model in order to select the most probable analysis (Collins 1997 Charniak 2000) 
More importantly when this classifier is integrated into a 2nd-ordered maximum spanning tree (MST) dependency parser (McDonald and Pereira 2006) in a weighted average manner significant improvement is obtained over the MST baselines 
For the non-directional parser we projectivize the training set prior to training using the procedure described in (Carreras 2007) 
We show that stacking methods outperform the approximate second-order parser of McDonald and Pereira (2006) on twelve languages and can be used within that approximation to achieve even better results 
These SVM settings are the same as previous research (Kudo and Matsumoto 2002 Sassano 2004) 
To overcome a minor shortcoming of the parsing algorithm of (Nivre et al  2004) we introduce a simple language independent post-processing step 
Several works have tested the effect of using a two-stage parser (Nivre and McDonald 2008 Martins et al 2008) where the second parser takes advantage of features obtained by the first one 
Best For the CoNLL-X languages only the best UAS for any parser in the original shared task (Buchholz and Marsi 2006) or in any column of Martins et al 
Examples of this include McDonald and Pereiras (2006) rewriting of projective trees produced by the Eisner (1996) algorithm and Nivre and Nilssons (2005) pseudo projective approach that creates projective trees with specially marked arcs that are later transformed into non-projective dependencies 
As our baseline we take the strictly projective arc-eager transition system proposed by Nivre (2003) as implemented in the freely available MaltParser system (Nivre et al 2006a) with and without the pseudo-projective parsing technique for recovering non-projective dependencies (Nivre and Nilsson 2005) 
However we have found this not to be a problem when measuring multi planarity in natural language treebanks since the effective problem size can be reduced by noting that each connected component of the crossings graph can be treated separately and that nodes that are not part of a cycle need not be considered5 Given that non-projective sentences in natural language tend to have a small proportion of non-projective links (Nivre and Nilsson 2005) the connected components of their crossings graphs are very small and k-colourings for them can quickly be found by brute-force search 
At this point we have many different parsing models that reach and even surpass 90% dependency or constituency accuracy on this test set (McDonald et al 2006 Nivre et al 2007 Charniak and Johnson 2005 Petrov et al 2006 Carreras et al 2008 Koo and Collins 2010) 
The optimal parse can be found using a spanning tree algorithm (Eisner 1996 McDonald et al  2005) 
Carreras (2007) presents a second-order parser that can score both sibling and grandchild parts with complexities of O(n4) time and O(n3) space 
N&N2005 The pseudo-projective parser of Nivre and Nilsson (2005) 
Our experiments are based on five data sets from the CoNLL-Xshared task Arabic Czech Danish Slovene and Turkish (Buchholz and Marsi 2006) 
Columns 24 enumerate LAS for Malt MST2O and Malt + MST2O as in Nivre and McDonald (2008) 
The features are designed over edges of dependency trees and the weights are given by model parameters (McDonald and Pereira 2006 Carreras 2007) 
We evaluate these methods on the Prague Dependency Treebank using online large-margin learning techniques (Crammer et al  2003 McDonald et al  2005) and show that MST parsing increases efficiency and accuracy for languages with non-projective dependencies 
Exact inference for parsing models that allow non-projective trees is NP hard except under very restricted independence assumptions (Neuhaus and Broker 1997 McDonald and Pereira 2006 McDonald and Satta 2007) 
We bootstrapped non-projective parsers for languages assembled for the CoNLL dependency parsing competitions (Buchholz and Marsi 2006) 
In the second category are those that employ exhaustive inference algorithms usually by making strong independence assumptions as is the case for edge-factored models (Paskin 2001 McDonald et al  2005a McDonald et al  2005b) 
However recent results in non-projective dependency parsing especially using data-driven methods indicate that most non-projective structures required for the analysis of natural language are very nearly projective differing only minimally from the best projective approximation (Nivre and Nilsson 2005 Hall and Novk 2005 McDonald and Pereira 2006) 
The features used to score while based on the previous work in dependency parsing (McDonald et al  2005) introduce some novel concepts such as better codification of context and surface distances and runtime information from dependencies previously parsed 
Beam Search Several researchers dealt with the early-commitment and error propagation of deterministic parsers by extending the greedy decisions with various flavors of beam-search (Sagae and Lavie 2006a Zhang and Clark 2008 Titov and Henderson 2007) 
In our dependency parsing experiments we used unlabeled dependencies extracted from the Penn 130 Treebank using the same head-table as Yamada and Matsumoto (2003) using sections 02-21 as training data and section 23 as test data following (McDonald et al  2005 Nivre & Scholz 2004 Yamada & Matsumoto 2003) 
Mc06 The best UAS reported by McDonald and Pereira (2006) 
Specifically we view stacked learning as a way of approximating non-local features in a linear model rather than making empirically dubious independence (McDonald et al 2005b) or structural assumptions (eg projectivity Eisner 1996) using search approximations (Sagae and Lavie 2005 Hall et al 2006 McDonald and Pereira 2006) solving a (generally NP-hard) integer linear program (Riedel and Clarke 2006) or adding latent variables (Titov and Henderson 2007) 
The model in Charniak (2000) is quite different however 
Furthermore it made the system homogeneous in terms of learning algorithms since that is what is used to train our unlabeled parser (McDonald and Pereira 2006) 
We use all 23 train/test splits from the 2006/7 CoNLL shared tasks (Buchholz and Marsi 2006 Nivre et al 2007)6 which cover 19 different languages7 We splice out all punctuation labeled in the data as is standard practice (Paskin 2001 Klein and Manning 2004) introducing new arcs from grandmothers to grand-daughters where necessary both in trainand test-sets 
This type of classier has been used successfully in deterministic parsing by Kudo and Matsumoto (2002)Yamada and Matsumoto (2003) and Sagae and Lavie (2005)among others 
For the baseline system (1) we used the system of McDonald and Pereira (2006) on a MacPro 28 Ghz as well for our implementation (2) 
We adopted the second order MST parsing algorithm as outlined by Eisner (1996) 
This model is related to the averaged perceptron algorithm of Collins (2002) 
For example Nivre and McDonald (2008) present the combination of two state of the art dependency parsers feeding each another showing that there is a significant improvement over the simple parsers 
Overall the accuracy of the DeSR parser with semantic information is slightly inferior to that of the second-order MST parser (McDonald & Pereira 2006) (915% UAS) 
Over the past decade there has been tremendous progress on learning parsing models from treebank data (Magerman 1995 Collins 1999 Charniak 1997 Ratnaparkhi 1999 Charniak 2000 Wang et al  2005 McDonald et al  2005) 
The first thing to note is that pseudo-projective parsing gives a significant improvement for PDT as previously reported by Nivre and Nilsson (2005) but also for Alpino where the improvement is even larger presumably because of the higher proportion of non-projective dependencies in the Dutch treebank 
To illustrate how this framework allows for improvements in the accuracy of dependency parsing to be used directly to improve the accuracy of HPSG parsing we showed that by combining the results of different dependency parsers using the search-based parsing ensemble approach of (Sagae and Lavie 2006) we obtain improved HPSG parsing accuracy as a result of the improved dependency accuracy 
These are due to the limitations of the parsing algorithm of (McDonald and Pereira 2006) which does not allow the features defined on those types of trigram-subtrees 
Features Used for Selecting Reduce The features used in (Nivre and Scholz 2004) to define a state transition are basically obtained from the two target words wi and wj and their related words 
In 2stage we can provide features specific to each stage which cant be done in a single stage approach (McDonald et al 2006) 
In fact the largest source of English dependency trees is automatically generated from the Penn Treebank (Marcus et al  1993) and is by convention exclusively projective 
Some authors propose to solve it by techniques for recovering non-projectivity from the output of a projective parser in a post-processing step (Hall and Novk 2005 Nivre and Nilsson 2005) others extend projective parsers by heuristics that allow at least certain non-projective constructions to be parsed (Attardi 2006 Nivre 2007) 
In the case of dependency parsers it is also possible to use grammars (Eisner and Satta 1999) but many algorithms use a data-driven approach instead making individual decisions about which dependencies to create by using probabilistic models (Eisner 1996) or classifiers (Yamada and Matsumoto 2003) 
The majority of graph-based parsers in the shared task were based on what McDonald and Pereira (2006) call the first-order model where the score of each arc is independent of every other arc but there were also attempts at exploring higher-order models either with exact inference limited to projective dependency graphs (Carreras 2007) or with approximate inference (Nakagawa 2007) 
This paper describes a simple yet novel method for constructing sets of 50-best parses based on a coarse-to-fine generative parser (Charniak 2000) 
Averaging parameters is a way to reduce overfit ting for perceptron training (Collins 2002) and is applied to all our experiments 
By and large transition-based models use a greedy inference strategy whereas graph-based models used different Maximum Spanning Tree (MST) algorithms Carreras (2007) MSTC Eisner (2000) MSTE or Chu-Liu/Edmonds (McDonald et al 2005 Chu and Liu 1965 Edmonds 1967) MSTCL/E More interestingly most of the best systems used some strategy to mitigate parsing errors 
While we see improvements over the single-parser baseline 4We made other modifications to MSTParser implementing many of the successes described by (McDonald et al 2006) 
Supervised structured large margin training approaches have been applied to parsing and produce promising results (Taskar et al 2004 McDonald et al 2005a Wang et al 2006) 
This allows us to ef ciently use ILP for dependency parsing and add constraints which provide a signi cant improvement over the current stateof-the-art parser (McDonald et al  2005b) on the Dutch Alpino corpus (see bl row in Table 1) 
CoNLL-X shared task (Buchholz and Marsi 2006) 
Our features are in many ways similar to those of Charniak (2000) 
There has been much recent work on dependency parsing using graph-based transition-based and hybrid methods see Nivre and McDonald (2008) for an overview 
This is supported by the fact that the accuracy of short dependencies is in general greater than that of long dependencies as reported in McDonald and Nivre (2007) for graph-based models 
These include the perceptron (Collins 2002) and its large-margin variants (Crammer and Singer 2003 McDonald et al  2005a) 
Indeed as for the voted perceptron of Collins (2002) we can get performance gains by reducing the support threshold for features to be included in the model 
On the WSJ corpus this parser achieves the same performance as that of McDonald and Pereira (2006) 
NO-RERANK Charniak (2000)s parser based on a lexicalized PCFG model of phrase structure trees3 The probabilities of CFG rules are parameterized on carefully hand-tuned extensive information such as lexical heads and symbols of ancestor/sibling nodes 
We show how Nivres (2009) swap-based reordering technique for non-projective shift-reduce-style parsing can be integrated into the non-directional easy-first framework of Goldberg and Elhadad (2010) to support non-projectivity and we report the results of our parsing experiments on the standard test section of the PTB providing comparisons with several freely available parsers including Goldberg and Elhadads (2010) implementation MALTPARSER (Nivre et al 2006) MSTPARSER (McDonald et al 2005 McDonald and Pereira 2006) the Charniak (2000) parser and the Berkeley parser (Petrov et al 2006 Petrov and Klein 2007) 
Recently dependency parsing has received renewed interest both in the parsing literature (Buchholz and Marsi 2006) and in applications like translation (Quirk et al  2005) and information extraction (Culotta and Sorensen 2004) 
This is true of the widely used link grammar parser for English (Sleator and Temperley 1993) which uses a dependency grammar of sorts the probabilistic dependency parser of Eisner (1996) and more recently proposed deterministic dependency parsers (Yamada and Matsumoto 2003 Nivre et al  2004) 
Parsers For graph-based parsers we used the projective first-order (MST1) and second order (MST2) variants of the freely available MST parser4 (McDonald et al 2005 McDonald and Pereira 2006) 
We parse the English sentences with the Charniak Parser (Charniak and Johnson 2005) and tag the Chinese sentences with a POS tagger implemented faithfully according to (Collins 2002) and trained on the Penn Chinese Treebank 50 (Xue et al 2005) 
This approach is one of those described in Eisner (1996) 
For this paper we used POS tags that were provided either by the Treebank itself (gold standard tags) or by the perceptron POS tagger3 presented in Collins (2002) 
The best projective parse tree is obtained using the Eisner algorithm (Eisner 1996) with the scores and the best non-projective one is obtained using the ChuLiu-Edmonds (CLE) algorithm (McDonald et al  2005b) 
The second-order model of McDonald and Pereira (2006) considers h m ch 
We know that Maltparser is good at short distance labeling and MST is good at long distance labeling (McDonald and Nivre 2007) 
The terms graph-based and transition-based were used by McDonald and Nivre (2007) to describe the difference between MSTParser (McDonald and Pereira 2006) which is a graph-based parser with an exhaustive search decoder and MaltParser (Nivre et al 2006) which is a transition-based parser with a greedy search decoder 
The evaluation metric traditionally associated with dependency parsing is based on scoring labeled or unlabeled attachment decisions whereby each correctly identified pair of head-dependent words is counted towards the success of the parser (Buchholz and Marsi 2006) 
With both these classifiers we use the same top-1 approach as with the CW-classifers and also averaging which has been shown to alleviate overfit ting (Collins 2002) 
Additionally following Sagae and Lavie (2006) we extend the basic deterministic LR algorithm with a bestfirst search which results in a parsing strategy similar to generalized LR parsing (Tomita 1987 1990) except that we do not perform Tomitas stack-merging operations 
The treebank data in our experiments are from the CoNLL shared-tasks on dependency parsing (Buchholz and Marsi 2006 Nivre et al 2007) 
The parser of McDonald and Pereira (2006) had been applied to English Czech and Danish and the parser of Nivre et al 
Now it is easy to see why the original decision Right-Arcr (Nivre et al  2004) had to be decomposed into two distinct decisions the decision to construct a labeled arc and the decision to shift the word 
Carreras (2007) and Titov and Henderson (2007) obtained the second and third positions respectively 
The algorithms are commonly used in other online-learning dependency parsing such as in (McDonald et al  2005a) 
Table 5 Comparison against the state-of-the-art full up to 40 (McDonald and Pereira 2006)a 0825 (Wang et al 2007) 0866 (Chen et al 2008) 0852 0884 Ours 0861 0889 aThis results was reported in (Wang et al 2007) 
An alternative method used by Charniak in the adaptation of his parser for Czech 6 and used by Nivre and Nilsson (2005) alters the dependency links by raising the governor to a higher node in the tree whenever 5 Bilexical dependencies are components of both the Collins and Charniak parsers and effectively model the types of syntactic subordination that we wish to extract in a dependency tree 
This paper explores an alternative approach to parsing based on the perceptron training algorithm introduced in Collins (2002) 
The shared tasks of multi-lingual dependency parsing took place at CoNLL-2006 (Buchholz and Marsi 2006) and CoNLL-2007 (Nivre et al 2007) 
It is straight-forward to extend the algorithms of Eisner (1996) and Paskin (2001) to the labeled case adding only a factor of O(|L|n2) 
McDonald and Pereira (2006) it was shown that non-projective dependency parsing with horizontal Markovization is FNP-hard 
Dependency grammar has proven to be a very useful syntactic formalism due in no small part to the development of efficient parsing algorithms (Eisner 2000 McDonald et al 2005b McDonald and Pereira 2006 Carreras 2007) which can be leveraged for a wide variety of learning methods such as feature-rich discriminative models (Lafferty et al 2001 Collins 2002 Taskar et al 2003) 
For baseline systems we used the firstand second-order basic features which were the same as the features used by McDonald and Pereira (2006) and we used the default settings of MSTParser throughout the paper iters=10 training-k=1 decode-type=proj 
Exact nonprojective inference and estimation become intractable if we break edge factoring (McDonald and Pereira 2006) 
We projectivize training data by a minimal transformation lifting non-projective arcs one step at a time and extending the arc label of lifted arcs using the encoding scheme called HEAD by Nivre and Nilsson (2005) which means that a lifted arc is assigned the label rh where r is the original label and h is the label of the original head in the nonprojective dependency graph 
Ablation studies In order to better understand the contributions of the various feature types we ran additional ablation experiments the results are listed in Table 3 in addition to the scores of Model 0 and the emulated Carreras (2007) parser (see Section 43) 
McDonald and Pereira (2006) expanded their first-order spanning tree model to be second-order by factoring the score of the tree into the sum of adjacent edge pair scores 
A dependency parser is trained on a corpus annotated with lexical dependencies which are easier to produce by annotators without deep linguistic knowledge and are becoming available in many languages (Buchholz & Marsi 2006) 
The second model is similar to that of McDonald and Pereira (2006) a factor consists of a main labeled dependency and the head child closest to the modifier (ch) 
This behavior is somewhat similar to parser stacking (Nivre and McDonald 2008 Martins et al 2008) in which a first-order parser derives some of its input features from the full 1-best output of another parser 
We use feature sets that are very similar to those described in Carreras (2007) 
In this short paper we extend the baseline feature templates with the following Distance between S0 and N0 Direction and distance between a pair of head and modifier have been used in the standard feature templates for maximum spanning tree parsing (McDonald et al 2005) 
From the fact that neither MIRA nor BPM clearly outperforms the other we conclude that we have successfully replicated the results reported in (McDonald et al  2005a) for English 
The definition of 2(x h m c) is dir cpos(xh) cpos(xm) cpos(xc) dir cpos(xh) cpos(xc) dir cpos(xm) cpos(xc) dir form(xh) form(xc) dir form(xm) form(xc) dir cpos(xh) form(xc) dir cpos(xm) form(xc) dir form(xh) cpos(xc) dir form(xm) cpos(xc) 3 Experiments and Results We report experiments with higher-order models for the ten languages in the multilingual track of the CoNLL-2007 shared task (Nivre et al  2007)1 In all experiments we trained our models using the averaged perceptron (Freund and Schapire 1999) following the extension of Collins (2002) for structured prediction problems 
(McDonald et al 2005a) x is used to denote the sentence to be parsed and xi to denote the i-th word in the sentence 
Eisner (1996) proposed a CKY-like O(n3) algorithm 
The performance of MIRA based parsing achieves the state-ofthe-art performance in English data (McDonald et al  2005a McDonald et al  2006) 
This paper makes two contributions 1) We combine together multiple word representations based on semantic and syntactic clusters in order to improve discriminative dependency parsing in the MSTParser framework (McDonald et al 2005) and 2) We provide an ensemble method for combining diverse clustering algorithms that is the discriminative parsing analog to the generative product of experts model for parsing described in (Petrov 2010) 
One of the MSTPARSER models used the Chu-Liu-Edmonds maximum spanning tree approach and the other used the Eisner (1996) algorithm with second order features and a nonprojective rewriting post-processing step 
Eisner (1996) proposes an O(n3) decoding algorithm for dependency parsing 
The method gives very similar accuracy to the model of Charniak (2000) which also uses a rich set of initial features in addition to Charniaks (1997) original model 
Features For simplicity in current work we only used two sets of features word-pair and tag-pair indicator features which are a subset of features used by other researchers on dependency parsing (McDonald et al 2005a Wang et al 2007) 
McDonald and Pereira (2006) adopted an approximation based on O(n3) projective parsing followed by rearrangement to permit crossing arcs achieving higher performance 
This corresponds to the Head condition in Nivre and Nilsson (2005) 
Discriminative classifiers for mapping histories to parser actions (Kudo and Matsumoto 2002 Yamada and Matsumoto 2003) 
Using this representation the parsing algorithm of Eisner (1996) is sufficient for searching over all projective trees in O(n3) time 
To study the influence of parsing methodology we will compare two different parsers MaltParser (Nivre et al  2004) and MSTParser (McDonald et al  2005) 
The best results have been achieved using Support-Vector Machines placing the MaltParser very high in both the CoNNL shared tasks on dependency parsing in 2006 and 2007 (Buchholz and Marsi 2006 Nivre et al 2007) and it has been shown that SVMs are better for the task than Memory-based learning (Hall et al 2006) 
At a recent evaluation of data-driven systems for dependency parsing with data from 13 different languages (Buchholz and Marsi 2006) the deterministic classier-based parser of Nivre et al 
However in this work we use forests from a Treebank parser (Charniak 2000) whose grammar is often flat in many productions 
In the grandparent models in our experiments we use a similar definition with feature vectors L(xiklk1lk) and R(xikrk1rk) where k is the parent for word i under y|i We train the model using the averaged perceptron for structured problems (Collins 2002) 
Wetrained the models onprojectivized graphs following Nivre and Nilsson (2005) method 
For a more complete definition see the CoNLLX shared task description paper (Buchholz and Marsi 2006)
Deterministic methods for dependency parsing have now been applied to a variety of languages including Japanese (Kudo and Matsumoto 2000) English (Yamada and Matsumoto 2003) Turkish (Oflazer 2003) and Swedish (Nivre et al  2004) 
Figure 7 illustrates the cubic parsing actions of the Eisners parsing algorithm (Eisner 1996) in the right direction where s r and t refer to the start and end indices of the chart items 
In order to get a first estimate of the empirical accuracy that can be obtained with transition-based 2-planar parsing we have evaluated the parser on four data sets from the CoNLL-X shared task (Buchholz and Marsi 2006) Czech Danish German and Portuguese 
They use a variant of Eisners generative model C (Eisner 1996b Eisner 1996a) for reranking and extend it to capture higher-order information than Eisners second-order generative model 
Many models have been successfully applied to sequence labeling problems such as maximum entropy (Ratnaparkhi 1996) conditional random fields (CRF) (Lafferty et al 2001) and perceptron (Collins 2002) 
In this paper we use the following baseline parsers MaltParser (Nivre et al 2007) for transition-based parsing MSTParser (McDonald et al 2005) (with sibling 2-edge factors) and BohnetParser (Bohnet 2010) (with general 2-edge factors) for graph-based parsing and Berkeley Parser (Petrov et al 2006) for constituency-based parsing 
The two algorithms we employed in our dependency parsing model are the Eisner parsing (Eisner 1996) and Chu-Lius algorithm (Chu and Liu 1965) 
While the MSTParser uses exact-inference (Eisner 1996) we apply beam-search to decoding 
Sagae and Lavies parsing algorithm is similar to the one used by Nivre and Scholz (2004) for deterministic dependency parsing (using kNN) 
The (Eisner 1996) algorithm is typically used for projective parsing 
Like a variation of Eisners generative model C (Eisner 1996b Eisner 1996a) 1In Figure 1 according to custom of dependency tree description the direction of hyperedge is written as from head to tail nodes 
Finally we note that 50-best parsing is only a fac1Charniak in (Charniak 2000) cites an accuracy of 895% 
Using the full set of features described in (McDonald et al 2005a Wang et al 2007) and comparing the corresponding dependency parsing 537 English PTB-10 Training(l/ul) 3026/1016 Dev 163 Test 270 PTB-15 Training 7303/2370 Dev 421 Test 603 PTB-20 Training 12519/4003 Dev 725 Test 1034 Chinese CTB4-10 Training(l/ul) 642/347 Dev 61 Test 40 CTB4-15 Training 1262/727 Dev 112 Test 83 CTB4-20 Training 2038/1150 Dev 163 Test 118 CTB4-40 Training 4400/2452 Dev 274 Test 240 CTB4 Training 5314/2977 Dev 300 Test 289 Table 1 Size of Experimental Data (# of sentences) results with previous work remains a direction for future work 
Graph-based parsing models (McDonald and Pereira 2006 Carreras 2007) have achieved state-of-the-art accuracy for a wide range of languages as shown in recent CoNLL shared tasks (Buchholz et al 2006 Nivre et al 2007) 
Recent work by Nivre and Nilsson introduces a technique where the projectivization transformation is encoded in the non-terminals of constituents during parsing (Nivre and Nilsson 2005) 
The two dependency parsers use radically different parsing approaches but have achieved very similar performance for a wide range of languages (McDonald and Nivre 2007) 
However instead of performing deterministic parsing as in (Nivre et al  2004) we use this ordering to define a generative history-based model by integrating word prediction operations into the set of parser actions 
Our experiments involve data from two treebanks the Wall Street Journal Penn treebank (Marcus et al 1993) and the Chinese treebank (Xue et al 2004) 
We evaluate its performance on the standard Penn English Treebank (PTB) dependency parsing task ie train on sections 02-21 and test on section 23 with automatically assigned POS tags (at 972% accuracy) using a tagger similar to Collins (2002) and using the headrules of Yamada and Matsumoto (2003) for conversion into dependency trees 
These independence assumptions are unwarranted as it has already been established that modeling non-local information such as arity and nearby parsing decisions improves the accuracy of dependency models (Klein and Manning 2002 McDonald and Pereira 2006) 
We also demonstrate that our approach and other improvement techniques (Koo et al 2008 Nivre and McDonald 2008) are complementary and that we can achieve very high accuracies when we combine our method with other improvement techniques 
Rows MSTParser 1/2 show the first-order (using feature templates 1 5 from Table 1) (McDonald et al 2005) and second order (using all feature templates from Table 1) (McDonald and Pereira 2006) MSTParsers as reported by the corresponding papers 
Since the parsing algorithm only produces projective dependency graphs we may use pseudo-projective parsing to recover non-projective dependencies ie projectivize training data and encode information about these transformations in extended arc labels to support deprojectivization of the parser output (Nivre and Nilsson 2005) 
In the multilingual parsing track participants train dependency parsers using treebanks provided for ten languages Arabic (Hajic et al  2004) Basque (Aduriz et al 2003) Catalan (Mart et al  2007) Chinese (Chen et al  2003) Czech (Bhmova et al  2003) English (Marcus et al  1993 Johansson and Nugues 2007) Greek (Prokopidis et al  2005) Hungarian (Czendes et al  2005) Italian (Montemagni et al  2003) and Turkish (Oflazer et al  2003) 
First the graph-based models have better precision than the transition-based models when predicting long arcs which is compatible with the results of McDonald and Nivre (2007) 
But the denominator Zi is a normalizing constant that sums over all parses it is found by a dependency-parsing variant of the inside algorithm following (Eisner 1996) 
If the parse has to be projective Eisners bottom-up-span algorithm (Eisner 1996) can be used for the search 
Eisner (1996) gave a generative model with a cubic parsing algorithm based on an edge factorization of trees 
Another use of bottom-up is due to Eisner (1996) who introduced the notion of a span 
The boosting approach to ranking has been applied to named entity segmentation (Collins 2002a) and natural language generation (Walker Rambow and Rogati 2001) 
In the first category are those methods that employ approximate inference typically through the use of linear time shift-reduce parsing algorithms (Yamada and Matsumoto 2003 Nivre and Scholz 2004 Nivre and Nilsson 2005) 
The constraints are chosen based on the two criteria (1) adding them to the base constraints (those added in advance) would result in an extremely large program and (2) it must be efcient to detect whether the constraint is violated in y No Cycles (T2) For every possible cycle c for the sentence x we have a constraint which forbids the case where all edges in c are active simultaneously summation display (ij)c dij |c|1 Comma Coordination (C3) For each symmetric conjunction token i which forms a symmetric coordination and each set of tokens A in x to the left of i with no comma between each pair of successive tokens we add summation display aA dia |A|1 which forbids con gurations where i has the argument tokens A Compatible Coordination Arguments (C4) For each conjunction token i and each set of tokens A in x with incompatible POS tags we add a constraint to forbid con gurations where i has the argument tokens A summation display aA dia |A|1 Selective Projective Parsing (P1) For each pair of triplets (ijl1) and (mnl2) we add the constraint eijl1 + emnl2 1 if l1 or l2 is in P 32 Training For training we use single-best MIRA (McDonald et al  2005a) 
It uses online large margin learning as the learning algorithm (McDonald et al 2005) 
The second order algorithm of Carreras (2007) uses in addition to McDonald and Pereira (2006) the child of the dependent occurring in the sentence between the head and the dependent as well as the edge from the dependents to a grandchild 
Indeed the highest scoring parsers trained using the MSTPARSER (McDonald and Pereira 2006) and MALTPARSER (Nivre et al 2006) parsing suites achieved only 788 and 811 labeled attachment F1 respectively 
Their model 1 is defined by an enclosing grand sibling for each sibling or grandchild part used in Carreras (2007) 
Dependency Features fdep(xthm) Unigram Features whth dir wmtm dir Bigram Features whth wmtm dir dist In Between Features th tb tm dir dist Surrounding Features th1 th th+1 tm1 tm tm+1 dir dist Sibling Features fsib(xthsm) wh th ws ts wm tm dir Grandparent Features fgrd(xtghm) wg tg wh th wm tm dir gdir Grand-sibling Features fgsib(xtghsm) wg tg wh th ws ts wm tm dir gdir 2This second-order model incorporates grandparent features composed of all grandchildren rather than just outermost ones and outperforms the one of Carreras (2007) according to the results in Koo and Collins (2010) 
Performance of Alternative Models 157 5 Related Work Previous parsing models (eg  Collins 1997 Charniak 2000) maximize the joint probability P(S T) of a sentence S and its parse tree T We maximize the conditional probability P(T | S) 
Another unexploited connection is that probabilistic approaches pay closer attention to the individual errors made by each component of a parse whereas the training error minimized in the large margin approach the structured margin loss (McDonald et al  2005) is a coarse measure that only assesses the total error of an entire parse rather than focusing on the error of any particular component 
The model parameters are trained using a discriminative learning algorithm eg averaged perceptron (Collins 2002) or MIRA (Crammer and Singer 2003) 
This averaging effect has been shown to reduce overfit ting and produce much more stable results (Collins 2002) 
For the parser stacking we follow the approach of Nivre and McDonald (2008) using MaltParser as a guide for the MST parser with the hash kernel ie providing the arcs and labels assigned by MaltParser as features 
There are now several approaches for multilingual dependency parsing as demonstrated in the CoNLL 2006 shared task (Buchholz and Marsi 2006) 
Whereas Ratnaparkhi (1996) used feature support cutoffs and early stopping to stop overfit ting of the model and Collins (2002) contends that including low support features harms a maximum entropy model our results show that low support features are useful in a regularized maximum entropy model 
Graph-based dependency parsing finds the highest-scoring tree according to a scoring function that decomposes under an exhaustive search (McDonald et al 2005) 
Journal Penn TreeBank (Marcus et al 1993) 
Most algorithms in this tradition are restricted to projective dependency graphs but it is 549 Computational Linguistics Volume 34 Number 4 possible to recover non-projective dependencies using pseudo-projective parsing (Nivre and Nilsson 2005) 
Representative of each method MSTParser and MaltParser gave comparable accuracies in the CoNLL-X shared task (Buchholz and Marsi 2006) 
For the MST-parsing MIRA (McDonald et al 2005a McDonald and Pereira 2006) and for transition-based parsing Support-Vector Machines (Hall et al 2006 Nivre et al 2006b) 
In this paper we work with both first-order and second-order models we train the models using MIRA and we use the (Eisner 1996) algorithm for inference 
The edge template was developed on development data from the English Penn-III treebank (Marcus et al 1993) 
The CoNLL-X shared task (Buchholz and Marsi 2006) made a wide selection of standardized treebanks for different languages available for the research community and allowed for easy comparison between various statistical methods on a standardized benchmark 
It is worth noting that both these systems combine transition based base parsers with a graph-based method for parser combination as first described by Sagae and Lavie (2006) 
McDonald and Nivre (2007) analyze the difference between graph-based and transition-based parsers (specifically the MALT and MST parsers) by comparing the different kinds of errors made by both parsers 
Czech dependency structures may contain nonprojective edges so we employ a maximum directed spanning tree algorithm (Chu and Liu 1965 Edmonds 1967 McDonald et al 2005b) as our firstorder parser for Czech 
Assuming that the calls o(c) and t(c) can both be performed in constant time the worst-case time complexity of a deterministic parser based on a transition system S is given by an upper bound on the length of transition sequences in S When building practical parsing systems the oracle can be approximated by a classifier trained on treebank data a technique that has been used successfully in a number of systems (Yamada and Matsumoto 2003 Nivre et al 2004 Attardi 2006) 
Graph-based (McDonald et al 2005 McDonald and Pereira 2006 Carreras et al 2006) and transition-based (Yamada and Matsumoto 2003 Nivre et al 2006) parsing algorithms offer two different approaches to data-driven dependency parsing 
The PredEdge features are exactly the six features used by Nivre and McDonald (2008) in their MSTMalt parser therefore feature set A is a replication of this parser except for modifications noted in footnote 4 
First in supervised models a head outward process is modeled (Eisner 1996 Collins 1999) 
The learning methods using in discriminative parsing are Perceptron (Collins 2002) and online large-margin learning (MIRA) (Crammer and Singer 2003) 
Differences with our approach are that we use a beam rather than best-first search we use a global model rather than local models chained together and finally our results surpass the best published results on the CCG parsing task whereas Sagae and Lavie (2006a) matched the best PTB results only by using a parser combination 
Such models are commonly referred to as edge-factored since their parameters factor relative to individual edges of the graph (Paskin 2001 McDonald et al 2005a)Edge-factored model shave many computational benefitsmost notably that inference for nonprojective dependency graphs can be achieved in polynomial time(McDonaldet al2005b)Theprimary problemin treating each dependency as independent is that it is not a realistic assumption 
For dependency parsing McDonald and Pereira (2006) proposed a method which can incorporate some types of global features and Riedel and Clarke (2006) studied a method using integer linear programming which can incorporate global linguistic constraints 
Although the accuracy of our method did not reach that of (McDonald and Pereira 2006) the scores were competitive even though our method is deterministic 
McDonald and Nivre (2007) compared the accuracy of MSTParser and MaltParser along a number of structural and linguistic dimensions 
Table 7 gives the results for both development (cross-validation for SDT PADT and Alpino 974 development set for PDT) and final test compared to the two top performing systems in the shared task MSTParser with approximate second-order non-projective parsing (McDonald et al  2006) and MaltParser with pseudo-projective parsing (but no coordination or verb group transformations) (Nivre et al  2006) 
It is well known that projective dependency parsing using edge based factorization can be handled with the Eisner algorithm (Eisner 1996) 
However they make different types of errors which can be seen as a reflection of their theoretical differences (McDonald and Nivre 2007) 
Pseudo-projective parsing for recovering nonprojective structures (Nivre and Nilsson 2005) 
We use the MIRA 217 online learner to set the weights (Crammer and Singer 2003 McDonald et al  2005a) since we found it trained quickly and provide good performance 
Nivre and McDonald (2008) were first to introduce stacking in the context of dependency parsing 
Ax = {(ijl) | ij Vx and l L} Let D(Gx) represent the subgraphs of graph Gx that are valid dependency graphs for the sentence x Since Gx contains all possible labeled arcs the set D(Gx) must necessarily contain all valid dependency graphs for x Assume that there exists a dependency arc scoring function s  V V L R Furthermore define the score of a graph as the sum of its arc scores s(G = (VA)) = summation display (ijl)A s(ijl) The score of a dependency arc s(ijl) represents the likelihood of creating a dependency from word wi to word wj with the label l If the arc score function is known a priori then the parsing problem can be stated as 123 G = argmax GD(Gx) s(G) = argmax GD(Gx) summation display (ijl)A s(ijl) This problem is equivalent to finding the highest scoring directed spanning tree in the graph Gx originatingoutoftherootnode0 which can be solved for both the labeled and unlabeled case in O(n2) time (McDonald et al  2005b) 
Statistical parsers reap dramatic gains from punctuation (Engel et al 2002 Roark 2001 Charniak 2000 Johnson 1998 Collins 1997 inter alia) 
We used the method proposed by (Carreras 2007) for our second-order parsing model 
Another advantage of generative models is that they do not suffer from the label bias problems (Bottou 1991) which is a potential problem for conditional or deterministic history-based models such as (Nivre et al  2004) 
At any rate regularized conditional loglinear models have not previously been applied to the problem of producing a high quality part-of-speech tagger Ratnaparkhi (1996) Toutanova and Manning (2000) and Collins (2002) all present unregularized models 
This extends the experiments of Nivre and McDonald (2008) replicated in our feature subset A Table 4 enumerates the results 
This approach has been further developed in particular by Ryan McDonald and his colleagues (McDonald Crammer and Pereira 2005 McDonald et al 2005 McDonald and Pereira 2006) and is now known as spanning tree parsing because the problem of nding the most probable tree under this type of model is equivalent to nding an optimum spanning tree in a dense graph containing all possible dependency arcs 
Compared with the parsing algorithms of Carreras (2007) Algorithm 2 uses history information by adding line 8 10 and 11 
The majority of previous work on dependency parsing has focused on local (ie  classification of individual edges) discriminative training methods (Yamada and Matsumoto 2003 Nivre et al  2004 Y Cheng 2005) 
However global constraints cannot be incorporated into the CLE algorithm (McDonald et al  2005b) 
This limited look-ahead window leads to error propagation and worse performance on root and long distant dependencies relative to graphbased parsers (McDonald and Nivre 2007) 
Our bottom-up deterministic analyzer adopt Nivres algorithm (Nivre and Scholz 2004) 
In the first-order parsing models the parts are individual head-modifier arcs in the dependency tree (McDonald et al 2005) 
Y&M2003 is the SVM-shift reduce parsing model of Yamada and Matsumoto (2003) N&S2004 is the memory-based learner of Nivre and Scholz (2004) and MIRA is the the system we have described 
For non-projective languages the algorithm is NP-hard and McDonald and Pereira (2006) introduce an approximate algorithm to handle such cases 
In particular metrics like attachment score for dependency parsers (Buchholz and Marsi 2006) and Parseval for constituency parsers (Black et al 1991) suffer from being an average over a highly skewed distribution of different grammatical constructions 
As for the second-order features we again base our features with those of McDonald and Pereira (2006) who reported successful experiments with second-order models 
This feature set forces us to adopt the expensive search procedure by Carreras (2007) which extends Eisners span-based dynamic programming algorithm (1996) to allow second-order feature dependencies 
Nivre and McDonald (2008) first showed how the MSTParser (McDonald et al 2005) and the MaltParser (Nivre et al 2007) could be improved by stacking each parser on the predictions of the other 
It was already known that the two systems make different errors through the work of Sagae and Lavie (2006) 
We provide additional evidence that the parser ensemble approach proposed by Sagae and Lavie (2006a) can be used to improve parsing accuracy even when only a single parsing algorithm is used as long as variation can be obtained for example by using different learning techniques or changing parsing direction from forward to backward (of course even greater gains may be achieved when different algorithms are used although this is not pursued here) and finally 4 
The feature sets we used are similar to other feature sets in the literature (McDonald et al 2005a Carreras 2007) so we will not attempt to give a exhaustive description of the features in this section 
The term no-3rd indicates a parser that was trained and tested with the thirdorder feature mappings fgsib and ftsib deactivated though lower-order features were retained note that Model 2 no-3rd is not identical to the Carreras (2007) parser as it defines grandchild parts for the pair of grandchildren 
As with the graph-based parser we use the discriminative perceptron (Collins 2002) to train the transition-based model (see Figure 5) 
For neighbouring parse decisions we extend the work of McDonald and Pereira (2006) and show that modeling vertical neighbour hoods makes parsing intractable in addition to modeling horizontal neighbour hoods 
An existing method to combine multiple parsing algorithms is the ensemble approach (Sagae and Lavie 2006a) which was reported to be useful in improving dependency parsing (Hall et al 2007) 
Finally in contrast to the results reported by Nivre and Nilsson (2005) simply projectivizing the training data (without using an inverse transformation) is not beneficial at all except possibly for Alpino 
Determining constraints with dependency parser combination Parser combination has been shown to be a powerful way to obtain very high accuracy in dependency parsing (Sagae and Lavie 2006) 
Buchholz and Marsi (2006) report that [f]or most parsers their ranking differs at most a few places from their overall ranking 
We also implemented an averaged perceptron system (Collins 2002) (another online learning algorithm) for comparison 
Labeled attachment score (LAS) The proportion of words that are assigned the correct head and dependency type (or no head if the word is a root) (Nivre et al  2004) 
Nivres parser has been tested for Swedish (Nivre et al  2004) English (Nivre and Scholz 2004) Czech (Nivre and Nilsson 2005) Bulgarian (Marinov and Nivre 2005) and Chinese Cheng et al 
The first-stage 50-best parser The first stage of our parser is the lexicalized probabilistic context-free parser described in (Charniak 2000) and (Charniak and Johnson 2005) 
We implemented the tournament model the CC algorithm (Kudo and Matsumoto 2002) SR algorithm (Sassano 2004) and CLE algorithm (McDonald et al 2005) with SVM classifiers 
The search for the best parse can then be formalized as the search for the maximum spanning tree (MST) (McDonald et al  2005b) 
For dependency parsing domain McDonald et al (2005a) modified the MIRA learning algorithm (McDonald et al  2005a) for structured domains in which the optimization problem can be solved by using Hidreths algorithm (Censor and Zenios 1997) which is faster than the quadratic programming technique 
For English we used the Penn Treebank (Marcus et al 1993) in our experiments and the tool Penn2Malt3 to convert the data into dependency structures using a standard set of head rules (Yamada and Matsumoto 2003a) 
At test time each input sentence is parsed using each of the three LR models and the three resulting dependency structures are combined according to the maximum-spanning-tree parser combination scheme6 (Sagae and Lavie 2006a) where each dependency proposed by each of the models has the same weight (it is possible that one of the more sophisticated weighting schemes proposed by Sagae and Lavie may be more effective but these were not attempted) 
The state-ofthe-art accuracy of Chinese POS tagging is about 935% which is much lower than that of English (about 97% (Collins 2002)) 
Research on dependency parsing is mainly based on machine learning methods which can be called history-based (Yamada and Matsumoto 2003 Nivre et al  2006) and discriminative learning methods (McDonald et al  2005a Corston-Oliver et al  2006) 
However unrestricted global inference for graph-based dependency parsing is NP-hard and graph-based parsers like MSTParser therefore limit the scope of their features to a small number of adjacent arcs (usually two) and/or resort to approximate inference (McDonald and Pereira 2006) 
An alternative to this approach is to use transition based parsing (Yamada and Matsumoto 2003 Nivre and Nilsson 2005 Attardi 2006 Nivre 2009 Gomez-Rodrguez and Nivre 2010) where there is an incremental processing of a string with a model that scores transitions between parser states conditioned on the parse history 
Support vector machines for mapping histories to parser actions (Kudo and Matsumoto 2002) 
Previous studies (McDonald and Pereira 2006 Yamada and Matsumoto 2003 Zhang and Clark 2008) show that the accuracies of complete trees are about 40% for English and about 35% for Chinese while the accuracies of relations between two words are much higher about 90% for English and about 85% for Chinese 
Nivre and Scholz (2004) uses this term with reference to Yamada and Matsumoto (2003) whose parser has to find all children of a token before it can attach that token to its head 
Table 2 shows the dependency accuracy root accuracy and complete match scores for our best parser (Model 2 with label set B) in comparison with Collins (1997) (Model 3) Charniak (2000) and Yamada and Matsumoto (2003)5 It is clear that with respect to unlabeled accuracy our parser does not quite reach state-of-the-art performance even if we limit the competition to deterministic methods such as that of Yamada and Matsumoto (2003) 
More precisely parsing accuracy is measured by the attachment score which is a standard measure used in studies of dependency parsing (Eisner 1996 Collins et al  1999) 
Stage 1 Unlabeled Parsing The first stage of our system creates an unlabeled parse y for an input sentence x This system is primarily based on the parsing models described by McDonald and Pereira (2006) 
This has been found to work well in previous work on dependency parser combination (Zeman and Zabokrtsky 2005 Sagae and Lavie 2006) 
The structured large margin approach on the other hand uses a global scoring function by minimizing a training loss the structured margin loss (McDonald et al  2005) which is directly coordinated with the global tree 
Though some of the parsing algorithms are language independent and show state-of-the-art performance on multilingual dependency Treebanks (Nivre et al 2007 Buchholz and Marsi 2006) they are often too slow for online purpose 
The edge model was developed on development data from the English Penn-III treebank (Marcus et al 1993) and we evaluate on Sect 
Note that the best-performing stacked configuration for each and every language outperforms MST2O corroborating results reported by Nivre and McDonald (2008) 
The usage of reductions list is identical to Eisner (1996a) and readers may refer to it for further details 
German was included in the CoNLL shared tasks in 2006 (Multilingual Dependency Parsing (Buchholz and Marsi 2006)) and in 2009 (Syntactic and Semantic Dependencies in Multiple Languages (Hajic et al 2009)) with data based on the TIGER 1123 corpus (Brants et al 2002) in both cases 
Note that the (incorrect) alignment between heeft and You will not be pursued because it would lead to heeft being a dependent of itself and thus violating the wellformed3Ie single headedness and acyclicity we do not require the trees to be projective but instead train pseudo-projective models (Nivre and Nilsson 2005) on the projected data (cf 
To assign probabilities to these actions previous work has proposed memory-based classifiers (Nivre et al 2004) SVMs (Nivre et al 2006b) and Incremental Sigmoid Belief Networks (ISBN) (Titov and Henderson 2007b) 
We choose the 2nd-ordered MST model (McDonald and Pereira 2006) as the baseline 
Models similar to model 2 have been found to work well for datasets with a rich annotation of dependency types such as the Swedish dependency treebank derived from Einarsson (1976) where the extra part-of-speech features are largely redundant (Nivre et al  2004) 
To learn these structures we used online large-margin learning (McDonald et al  2005) that empirically provides state-of-the-art performance for Czech 
We include in the table results from the pure transition-based parser of Zhang and Clark (2008) (row Z&C08 transition) the dynamic-programming arc-standard parser of Huang and Sagae (2010) (row H&S10) and graphbased models including MSTParser (McDonald and Pereira 2006) the baseline feature parser of Koo et al 
However this algorithm cannot be generalized to the second-order setting McDonald and Pereira (2006) proved that this problem is NPhard and described an approximate greedy search algorithm 
Another example is the second-order approximate spanning tree parser developed by McDonald and Pereira (2006) 
Subsequently researchers have begun to look at both porting these parsers to new domains (Gildea 2001 McClosky et al 2006 Petrov et al 2010) and constructing parsers for new languages (Collins et al 1999 Buchholz and Marsi 2006 Nivre et al 2007) 
In computational linguistics dependency based syntactic representations have in recent years been used primarily in data-driven models which learn to produce dependency structures for sentences solely from an annotated corpus as in the work of Eisner (1996) Yamada and Matsumoto (2003) Nivre Hall and Nilsson (2004) and McDonald Crammer and Pereira (2005) among others 
Currently the work on conditional parsing models appears to have culminated in large margin training approaches (Taskar et al  2004 McDonald et al  2005) which demonstrates the state of the art performance in English dependency parsing 
It is also worth noting that the parsing units in this treebank are in many cases larger than conventional sentences which partly explains the high average number of tokens per sentence (Buchholz and Marsi 2006) 
The fact that our model defines a probability model over parse trees unlike the previous stateof-the-art methods (Nivre et al  2006 McDonald et al  2006) makes it easier to use this model in applications which require probability estimates such as in language processing pipelines or for language modeling 
Related Work Data-driven dependency parsing using supervised machine learning was pioneered by Eisner (1996) who showed how traditional chart parsing techniques could be adapted for dependency parsing to give efcient parsing with exact inference over a probabilistic model where the score of a dependency tree is the sum of the scores of individual arcs 
It should be noted that unlike Model 1 Model 2 produces grand-sibling parts only for the outermost pair of grandchildren4 similar to the behavior of the Carreras (2007) parser 
This is a very simple approach but provided significant performance improvements comparing with the stateof-the-art supervised dependency parsers such as (McDonald and Pereira 2006) 
In fact the resemblance is more than passing as Model 2 can emulate the Carreras (2007) algorithm by demoting each third-order part into a second-order part SCOREGS(xghms) = SCOREG(xghm) SCORETS(xhmst) = SCORES(xhms) where SCOREG SCORES SCOREGS and SCORETS are the scoring functions for grandchildren siblings grand-siblings and tri-siblings respectively 
Using stacking with rich features we obtain results competitive with Nivre and McDonald (2008) while preserving the fast quadratic parsing time of arcfactored spanning tree algorithms 
McDonald and Pereira (2006) consider a method for recovering non-projective attachments from a graph representation of a sentence in which an optimal projective parse tree has been identified 
System LAS Nivre et al 2007b (MaltParser combined) 7694% Carreras 2007 7575% Titov and Henderson 2007 7549% C o N L L 07 Hall et al 2007 (MaltParser (single parser) + pseudo projective transformation) 7499% BDT I MaltParser (single parser) 7452% BDT I size 7483% BDT II MaltParser (single parser) Baseline 7708% Table 1 
An important part of this research effort are the CoNLL 2006 and 2007 shared tasks (Buchholz and Marsi 2006 Nivre et al 2007) which allowed for a comparison of many algorithms and approaches for this task on many languages 
But it keeps all the parsing histories of a head which is different from only keeping adjacent two modifiers in (McDonald and Pereira 2006) 
We find that partial correspondence projection gives rise to parsers that outperform parsers trained on aggressively filtered data sets and achieve unlabeled attachment scores that are only 5% behind the average UAS for Dutch in the CoNLL-X Shared Task on supervised parsing (Buchholz and Marsi 2006) 
The lower-order feature mappings fdep fsib and fgch are based on feature sets from previous work (McDonald et al 2005a McDonald and Pereira 2006 Carreras 2007) to which we added lexicalized versions of several features 
The dependency parsers that we compare are the deterministic shift-reduce MaltParser (Nivre et al 2007) and the second-order minimum spanning tree algorithm based MstParser (McDonald et al 2006) 
The approach of deterministic classier-based parsing was rst proposed for Japanese by Kudo and Matsumoto (2002) and for English by Yamada and Matsumoto (2003) 
CoNLL-X shared task (Buchholz and Marsi 2006)wasa large-scale evaluation of data-driven dependency parsers with data from 13 different languages and 19 participating systems 
They used a method of combining several parsers outputs in the framework of MST parsing (Sagae and Lavie 2006) 
We combined this parsing algorithm with the passive-aggressive perceptron algorithm (Crammer et al 2003 McDonald et al 2005 Crammer et al 2006) 
We compare the performance of our forest reranker against n-best reranking on the Penn English Treebank (Marcus et al 1993) 
For example it must not contain dependency chains such as en kom ik en  For a more formal definition see previous work (Nivre et al  2004) 
The second-order model of McDonald and Pereira (2006) incorporates sibling parts and also needs O(n3) parsing time 
Notice that the strategy just described to handle sibling features is not fully compatible with the features proposed by Eisner (1996) for projective parsing as the latter correlate only consecutive siblings and are also able to place special features on the first child of a given word 
Preliminary experiments using a different dependency parser MSTParser (McDonald et al 2005) resulted in similar empirical observations 
Specifically these approaches considered sibling relations of the modifier token (Eisner 1996 McDonald and Pereira 2006) 
There has been extensive work on data-driven dependency parsing for both projective parsing (Eisner 1996 Paskin 2001 Yamada and Matsumoto 2003 Nivre and Scholz 2004 McDonald et al  2005a) and non-projective parsing systems (Nivre and Nilsson 2005 Hall and Novak 2005 McDonald et al  2005b) 
Table 4 compares our baseline against the state-of-the-art graph-based (McDonald et al 2005) and transition-based (Zhang and Clark 2008) approaches and confirms that our system performs at the same level with those stateof-the-art and runs extremely fast in the deterministic mode (k=1) and still quite fast in the beamsearch mode (k=16) 
English main results In our English experiments we tested eight different parsing configurations representing all possible choices between baseline or cluster-based feature sets first-order (Eisner 2000) or second-order (Carreras 2007) factorizations and labeled or unlabeled parsing 
Introduce through post-processing eg through reattachment rules (Bick 2006) or if the change increases overall parse tree probability (McDonald et al  2006) 
In 2006 the shared task was multilingual dependency parsing where participants had to train a single parser on data from thirteen different languages which enabled a comparison not only of parsing and learning methods but also of the performance that can be achieved for different languages (Buchholz and Marsi 2006) 
Following (Nivre et al  2006) the encoding scheme called HEAD in (Nivre and Nilsson 2005) was used to encode the original non-projective dependencies in the labels of the projectivized dependency tree 
The second order parsing algorithm of McDonald and Pereira (2006) uses a separate algorithm for edge labeling 
For projective parsing it is significantly faster than exact dynamic programming at the cost of small amounts of search error We are interested in extending these ideas to phrase-structure and lattice parsing and in trying other higher-order features such as those used in parse reranking (Charniak and Johnson 2005 Huang 2008) and history-based parsing (Nivre and McDonald 2008) 
Perceptron 829 880 303 (inc punc) MIRA 833 886 313 Bayes Point Machine 840 888 309 Table 3 Comparison to previous best published results reported in (McDonald et al  2005a) 
Refer to (McDonald et al  2005b) for a detailed treatment of both algorithms 
According to (McDonald and Nivre 2007) all data-driven models for dependency parsing that have been proposed in recent years can be described as either graph-based or transition-based 
We use the CoNLL-X data format for dependency trees (Buchholz and Marsi 2006) to encode partial structures 
Previous work showed that rich features over a wide range of decision history can lead to significant improvements in accuracy for transition-based models (Yamada and Matsumoto 2003a Nivre et al 2004) 
More precisely dependency arcs (or pairs of arcs) are first represented by a high dimensional feature vector f(ijl) Rk where f is typically a binary feature vector over properties of the arc as well as the surrounding input (McDonald et al 2005a McDonald et al 2006) 
We use five datasets from the CONLL-X Shared Task (Buchholz and Marsi 2006)1 Lemmas and morphological features (FEATS) are ignored since we only add POS and CPOS tags to unlabeled data 
Parsing experiments 51 Data and setup We used the standard partitions of the Wall Street Journal Penn Treebank (Marcus et al  1993) ie sections 2-21 for training section 22 for development and section 23 for evaluation 
The reason may be that shorter dependencies are often modifier of nouns such as determiners or adjectives or pronouns modifying their direct neighbors while longer dependencies typically represent modifiers of the root or the main verb in a sentence(McDonald and Nivre 2007) 
Then we present results using the graph-based parser MSTParser (McDonald and Pereira 2006) again with default settings to test the methods across parsers 
The pchemtb-closed shared task (Marcus et al  1993 Johansson and Nugues 2007 Kulick et al  2004) is used to illustrate our models 
The resulting algorithm is projective and nonprojectivity is handled by pseudo-projective transformations as described in (Nivre and Nilsson 2005) 
This was shown to be the case for both MaltParser (Nivre et al 2007c) and MST (McDonald et al 2005) two of the best performing parsers on the whole 
Despite the realization that maximum margin training is closely related to maximum conditional likelihood for conditional models (McDonald et al  2005) a suf ciently uni ed view has not yet been achieved that permits the easy exchange of improvements between the probabilistic and nonprobabilistic approaches 
In contrast to previous work where this was constrained to sibling relations of the dependent (McDonald and Pereira 2006) here head-grandchild relations can be taken into account 
But the lack of corpora has led to a situation where much of the current work on parsing is performed on a single domain using training data from that domain the Wall Street Journal (WSJ) section of the Penn Treebank (Marcus et al  1993) 
We employ as a basis for our parser the second order maximum spanning tree dependency parsing algorithm of Carreras (2007) 
Finally our work is similar to the comparison of the chart-based MSTParser (McDonald et al 2005) and shift-reduce MaltParser (Nivre et al 2006) for dependency parsing 
This enables search by either minimum spanning tree (West 2001) or by Eisners (1996) projective parser 
Nivre and McDonald (2008) explore a parser stacking approach in which the output of one parser is fed as an input to a different kind of parser 
Related work Our graph-based parser is derived from the work of McDonald and Pereira (2006) 
The verb krijg is incorrectly coordinated with the preposition om  work is ef cient and has also been extended to non-projective trees (McDonald et al  2005b) 
Bilexical statistics (Eisner 1996) as represented by the maximal context of the P L w and P R w parameters serve as a proxy for such semantic preferences where the actual modifier word (as opposed to say merely its part of speech) indicates the particular semantics of its head 
The CoNLL shared tasks on multilingual dependency parsing in 2006 and 2007 (Buchholz and Marsi 2006 Nivre et al 2007a) demonstrated that dependency parsing for MRLs is quite challenging 
Kudo and Matsumoto (2002) applied the 361 cascaded chunking algorithm (hereafter CC algorithm) to Japanese dependency parsing 
Freund and Schapire (1999) discuss how the theory for classification problems can be extended to deal with both of these questions Collins (2002) describes how these results apply to NLP problems 
We have successfully replicated the state-of-the-art results for dependency parsing (McDonald et al  2005a) for both Czech and English using Bayes Point Machines 
Given this assumption the parsing problem reduces to find Y = arg max Y(X) score(Y|X) (1) = arg max Y(X) summation display (xixj)Y score(xi xj) where the score(xi xj) can depend on any measurable property of xi and xj within the sentence X This formulation is sufficiently general to capture most dependency parsing models including probabilistic dependency models (Eisner 1996 Wang et al 2005) as well as non-probabilistic models (McDonald et al 2005a) 
Such models are commonly referred to as edge-factored since their parameters factor relative to individual edges of the graph (Paskin 2001 McDonald et al  2005a) 
In the probabilistic LR model probabilities are assigned to tree 696 Precision Recall F-score Time (min) Best-First Classifier-Based (this paper) 881 878 879 17 Deterministic (MaxEnt) (this paper) 854 848 851 < 1 Charniak & Johnson (2005) 913 906 910 Unk Bod (2003) 908 907 907 145* Charniak (2000) 895 896 895 23 Collins (1999) 883 881 882 39 Ratnaparkhi (1997) 875 863 869 Unk Tsuruoka & Tsujii (2005) deterministic 865 812 838 < 1* Tsuruoka & Tsujii (2005) search 868 850 859 2* Sagae & Lavie (2005) 860 861 860 11* Table 1 Summary of results on labeled precision and recall of constituents and time required to parse the test set 
Johansson and Nugues (2008) reported training times of 24 days for English with the high-order parsing algorithm of Carreras (2007) 
The results reported here for English and Czech are comparable to the previous best published numbers in (McDonald et al  2005a) as Table 3 shows 
Exhaustive nonprojective dependency parsing with more powerful models is intractable (McDonald and Satta 2007) and one has to resort to approximation algorithms (McDonald and Pereira 2006) 
Recently classifier-based dependency parsing (Nivre and Scholz 2004 Yamada and Matsumoto 2003) has showed that deterministic parsers are capable of high levels of accuracy despite great simplicity 
Sagae and Lavie (2006b) extend this model to alternate between left-to-right and right-to-left passes 
However just as it has been noted that most non-projective structures appearing in practice are only slightly nonprojective (Nivre and Nilsson 2005) we characterise a sense in which the structures appearing in treebanks can be viewed as being only slightly ill-nested 
Discriminative classiers to map histories to parser actions (Kudo and Matsumoto 2002 Yamada and Matsumoto 2003 NivreHalland Nilsson 2004) A system of this kind employs no grammar but relies completely on inductive learning from treebank data for the analysis of new sentencesand on deterministic parsing for disambiguation 
In the multilingual track of the CoNLL 2007 shared task on dependency parsing a single parser must be trained to handle data from ten different languages Arabic (Hajic et al  2004) Basque (Aduriz et al  2003) Catalan (Mart et al  2007) Chinese (Chen et al  2003) Czech (Bohmova et al  2003) English (Marcus et al  1993 Johansson and Nugues 2007) Greek (Prokopidis et al  2005) Hungarian (Csendes et al  2005) Italian (Montemagni et al  2003) and Turkish (Oflazer et al  2003)1 Our contribution is a study in multilingual parser optimization using the freely available MaltParser system which performs 1For more information about the task and the data sets see Nivre et al 
Even before the 2006 shared task the parsers of Collins (1997) and Charniak (2000) originally developed for English had been adapted for dependency parsing of Czech and the parsing methodology proposed by Kudo and Matsumoto (2002) and Yamada and Matsumoto (2003) had been evaluated on both Japanese and English 
In order to get at least a preliminary answer to this question we extracted LCFRS productions from the data used in the 2006 CoNLL shared task on data-driven dependency parsing (Buchholz and Marsi 2006) and evaluated how large a portion of these productions could be binarized using our algorithm 
Parsing history has been used to improve parsing accuracy by many researchers (Yamada and Matsumoto 2003 McDonald and Pereira 2006) 
Several efficient accurate and robust approaches to data-driven dependency parsing have been proposed recently (Nivre and Scholz 2004 McDonald et al  2005 Buchholz and Marsi 2006) for syntactic analysis of natural language using bilexical dependency relations (Eisner 1996) 
To reduce the data sparseness problem we use the back-off strategy proposed in (Eisner 1996a) 
We argue that bootstrapping a parser is most promising when the model uses a rich set of redundant features as in recent models for scoring dependency parses (McDonald et al  2005) 
Dependency parsers have been tested on parsing sentences in English (Yamada and Matsumoto 2003 Nivre and Scholz 2004 McDonald et al 2005) as well as many other languages (Nivre et al 2007a) 
Charniak (2000) shows the value his parser gains from parent annotation of nodes suggesting that this information is at least partly complementary to information derivable from lexicalization and Collins (1999) uses a range of linguistically motivated and carefully hand-engineered subcategorizations to break down wrong context-freedom assumptions of the naive Penn treebank covering PCFG such as differentiating base NPs from noun phrases with phrasal modifiers and distinguishing sentences with empty subjects from those where there is an overt subject NP 
Third-order features of S0 and N0 Higher-order context features have been used by graph-based dependency parsers to improve accuracies (Carreras 2007 Koo and Collins 2010) 
The pseudo-projective transformation proposed by Nivre and Nilsson (2005) is such an approach which is compatible with different parser engines 
We experiment on two popular treebanks the Wall Street Journal (WSJ) portion of the Penn English Treebank (Marcus et al 1993) and the Penn Chinese Treebank (CTB) 50 (Xue et al 2005) 
Nivre and Nilsson (2005) observe that most non-projective dependency structures appearing in practice are close to being projective since they contain only a small proportion of nonprojective arcs 
Dependency trees are usually assumed to be projective (no crossing arcs) which means that if there is an arc a2a5a4a20a19a15a21a27a4a25a24a28a16 then a4a29a19 is an ancestor of all the words between a4a30a19 and a4a25a24  Let a31a7a2a32a0a33a16 denote the set of all the directed projective trees that span a0  From an input sentence a0 one would like to be able to compute the best parse that is a projective tree a18a35a34 a31a7a2a32a0a33a16 that obtains the highest score  In particular I follow Eisner (1996) and McDonald et al 
The best performing system (McDonald et al 2006 note this system is different to our baseline) achieves 792% labelled accuracy while our baseline system achieves 786% and our constrained version 798% 
Table 6 contrasts our results with those from Collins (2002) 
Algorithm 2 Parsing algorithm 1 Initialization V [ssdirI/C] = 00sdir 2 for k = 1 to n do 3 for s = 0 to nk do 4 t = s + k 5 % Create incomplete items 6 Lst=V [stI]= maxsr<tV I(r) 7 Rst=V [stI]= maxsr<tV I(r) 8 Calculate Ldfst and Rdfst 9 % Update the scores of incomplete chart items 10 V [stI]=L+st=Lst + Ldfst 11 V [stI]=R+st=Rst + Rdfst 12 % Create complete items 13 V [stC]= maxsr<tV C(r) 14 V [stC]= maxs<rtV C(r) 15 end for 16 end for Algorithm 2 is the parsing algorithm with thehistory-based features whereV [stdirI/C] refers to the score of chart item [stdirI/C] V I(r) is a function to search for the optimal sibling and grandchild nodes for the incomplete items (line 6 and 7) (Carreras 2007) given the 130 splitting point r and return the score of the structure and V C(r) is a function to search for the optimal grandchild node for the complete items (line 13and14) 
This is done by training a classifier to determine parser actions based on local features that represent the current state of the parser (Nivre and Scholz 2004 Sagae and Lavie 2005) 
Dependency parsing has been applied to a fairly broad range of languages especiallyintheCoNLLsharedtasksin2006and2007 (Buchholz and Marsi 2006 Nivre et al 2007) 
While large factors are desirable for capturing sophisticated linguistic constraints they come at the cost of time complexity for the projective case adaptations of Eisners algorithm (Eisner 1996) are O(n3) for 1-edge factors (McDonald et al 2005) or sibling 2-edge factors (McDonald and Pereira 2006) and O(n4) for general 2-edge factors (Carreras 2007) or 3-edge factors (Koo and Collins 2010) 
In the second approach combination of 3Downloadedfromhttp//source forgenet/projects/mstparser the three dependency parsers is done according to the maximum spanning tree combination scheme of Sagae and Lavie (2006) which results in high accuracy of surface dependencies 
For handling non-projective relations Nivre and Nilsson (2005) suggested applying a preprocessing step to a dependency parser which consists in lifting non-projective arcs to their head repeatedly until the tree becomes pseudo-projective 
The averaged perceptron (Collins 2002) is a variant which averages the w across all iterations it has demonstrated good generalization especially with data that is not linearly separable as in many natural language processing problems 
Compared with the algorithm of Carreras (2007) that only considers the furthest children of s and t Algorithm 2 considers all the children 
To avoid too small training sets we pool together categories that have a frequency below a certain threshold t 24 Pseudo-Projective Parsing Pseudo-projective parsing was proposed by Nivre and Nilsson (2005) as a way of dealing with non-projective structures in a projective data-driven parser 
Decision Trees(Haruno et al  1998) and Maximum Entropy models(Ratnaparkhi 1997 Uchimoto et al  1999 Charniak 2000) have been applied to dependency or syntactic structure analysis 
For example it would be easy to enforce such constraints in the Eisner (1996) algorithm or using Integer Linear Programming approaches (Riedel and Clarke 2006 Martins et al 2009) 
The deterministic shift-reduce parsing algorithm of (Nivre & Scholz 2004) was used to create two parsers2 one that processes the input sentence from left-to-right (LR) and one that goes from right-toleft (RL) 
Occasionally in 59 sentences out of 2416 on section 23 of the Wall Street Journal Penn Treebank (Marcus et al  1993) the shift-reduce parser fails to attach a node to a head producing a disconnected graph 
This permits us to make exact comparisons with the parser of Yamada and Matsumoto (2003) but also the parsers of Collins (1997) and Charniak (2000) which are evaluated on the same data set in Yamada and Matsumoto (2003) 
While we expect a longer runtime than using the Chu-Liu-Edmonds as in previous work (McDonald et al  2005b) we are interested in how large the increase is Table 2 shows the average solve time (ST) for sentences with respect to the number of tokens in each sentence for our system with constraints (cnstr) and the Chu-Liu-Edmonds (CLE) algorithm 
The pseudo-projective approach (Nivre and Nilsson 2005) Transform non-projective training trees to projective ones but encode the information necessary to make the inverse transformation in the DEPREL so that this inverse transformation can also be carried out on the test trees (Nivre et al  2006) 
We used our implementation of the Collins (2002) tagger (with 973% accuracy on a standard Penn Treebank test) to perform POS-tagging 
The comparison reported in this section is similar to the comparison between the chartbased MSTParser (McDonald et al 2005) and shift reduce MaltParser (Nivre et al 2006) for dependency parsing 
In 2006 the shared task was multilingual dependency parsing where participants had to train and test a parser on data from thirteen different languages (Buchholz and Marsi 2006) 
Also we chose to average each individual perceptron (Collins 2002) prior to Bayesian averaging 
As mentioned previously the source data is drawn from a corpus of news specifically the Wall Street Journal section of the Penn Treebank (Marcus et al  1993) 
Our main training set consists of Sections 02-21 of the Wall Street Journal portion of the Penn Treebank (Marcus et al 1993) with Section 22 serving as development set for source domain comparisons 
We generalize the standard deterministic stepwise framework to probabilistic parsing with the use of a best-first search strategy similar to the one employed in constituent parsing by Ratnaparkhi (1997) and later by Sagae and Lavie (2006) 3 
In this work we use two well-known publicly available dependency parsers MSTParser (McDonald et al 2005b)1 which implements ex1http//source forgenet/projects/mstparser 159 act first-order arc-factored nonprojective parsing (212) and approximate second-order nonprojective parsing and MaltParser (Nivre et al 2006) which is a state-of-the-art transition-based parser2 We do not alter the training algorithms used in prior work for learning these two parsers from data 
In the higher-order models the parts consist of arcs together with some context eg the parent or the sister arcs (McDonald and Pereira 2006 Carreras 2007 Koo and Collins 2010) 
To facilitate comparisons with previous work (McDonald et al 2005b McDonald and Pereira 2006) we used the training/development/test partition defined in the corpus and we also used the automatically-assigned part of speech tags provided in the corpus10 Czech word clusters were derived from the raw text section of the PDT 10 which contains about 39 million words of newswire text11 We trained the parsers using the averaged perceptron (Freund and Schapire 1999 Collins 2002) which represents a balance between strong performance and fast training times 
They will be based solely on different dependency tree configurations (see Figure 5) similarly to (Nivre and McDonald 2008 Martins et al 2008) 
Different ways have been proposed to deal with non-projectivity (Nivre and Nilsson 2005 McDonald et al 2005 McDonald and Pereira 2006 Nivre 2009) 
In addition in English parsing we ignore the parent-predictions of punctuation tokens13 and in Czech parsing we retain the punctuation tokens this matches previous work (Yamada and Matsumoto 2003 McDonald et al 2005a McDonald and Pereira 2006) 
The behavior of MSTParser and Malt Pars erin this respect is consistent with the results of McDonald and Nivre (2007) 
The labeled attachment scores for the ISBN with tuned features (TF) and local features (LF) and ISBN with tuned features and no edges connecting latent variable vectors (TF-NA) are presented in table 1 along with results for the MALT parser both with tuned and local feature the MST parser (McDonald et al  2006) and the average score (Aver) across all systems in the CoNLL-X shared task 
For higher-order non-projective parsing however computational complexity results (McDonald and Pereira 2006 McDonald and Satta 2007) indicate that exact solutions to the three inference problems of Section 22 will be intractable 
This can be seen in state-of-the-art constituency-based parsers such as Collins (1999) Charniak (2000) and Petrovetal(2006)and the effects of different transformations have been studied by Johnson (1998) KleinandManning(2003)andBikel(2004) 
We then discuss the application of pseudo-projective transformations (Nivre and Nilsson 2005) and an additional arc-reversing transform to dependency DAGs 
This approach corresponds to the training problem posed in (McDonald et al 2005a) and has yielded the best published results for English dependency parsing 
We trained on the standard Penn Treebank WSJ corpus (Marcus et al 1993) 
Supervised learning algorithms still represent the state of the art approach for inferring dependency parsers from data (McDonald et al 2005a McDonald and Pereira 2006 Wang et al 2007) 
This feature was particularly helpful for nouns identifying their parent (McDonald et al  2005a) 
We follow the edge based factorization method of Eisner (Eisner 1996) and define the score of a dependency tree as the sum of the score of all edges in the tree s(xy) = summation display (ij)y s(ij) = summation display (ij)y w (ij) (1) where (ij) is a high-dimensional binary feature representation of the edge from xwi to xwj  For example in Figure 1 we can present an example (ij) as follows (ij) = brace left Bigg 1if xwi =prime hitprimeandxwj =prime ballprime 0otherwise The basic question must be answered for models of this form how to find the dependency tree y with the highest score for sentence x? 
A first-order (or edge-factored) parsing model (McDonald et al 2005) contains only LINK factors along with a global TREE or PTREE factor 
Supervised dependency parsing achieves the stateof-the-art in recent years (McDonald et al 2005a McDonald and Pereira 2006 Nivre et al 2006) 
In the domain adaptation track participants were provided with English training data from the Wall Street Journal portion of the Penn Treebank (Marcus et al  1993) converted to dependencies (Johansson and Nugues 2007) to train parsers to be evaluated on material in the biological (development set) and chemical (test set) domains (Kulick et al  2004) and optionally on text from the CHILDES database (MacWhinney 2000 Brown 1973) 
For regularization purposes we adopt an average perceptron (Collins 2002) which returns for each y y = 1T summationtextTt=1 ty the average of all weight vectors ty posited during training 
One of the reasons are CoNLL shared tasks for syntactic dependency parsing in the years 2006 2007 (Buchholz and Marsi 2006 Nivre et al 2007) and the CoNLL shared task for joint parsing of syntactic and semantic dependencies in the year 2008 and of cause this shared task in 2009 cf 
Method # Step Time[s] # Example # Feature Tournament 26396 371 374579 56165 SR algorithm (Sassano 2004) 15641 80 94669 37183 CC algorithm (Kudo and Matsumoto 2002) 18922 99 112759 37183 Table 3 Parsing time and the size of the training examples 
Although both parser models show a similar accuracy McDonald and Nivre (2007) demonstrate that the two types of models exhibit different behaviors 
Thus table 4 shows labeled attachment scores the main evaluation metric used in the shared task in comparison to the two highest scoring systems from the original evaluation (McDonald et al  2006 Nivre et al  2006) 
Approximate algorithms have been employed to handle models that are not arc-factored (although features are still fairly local) McDonald and Pereira (2006) adopted an approximation based on O(n3) projective parsing followed by a hill climbing algorithm to rearrange arcs and Smith and Eisner (2008) proposed an algorithm based on loopy belief propagation 
These results show that the discriminative spanning tree parsing framework (McDonald et al  2005b McDonald and Pereira 2006) is easily adapted across all these languages 
Supported by the Lynn and William Frankel Center for Computer Sciences Ben Gurion University Current dependency parsers can be categorized into three families local-and-greedy transition based parsers (eg MALTPARSER (Nivre et al 2006)) globally optimized graph-based parsers (eg MSTPARSER (McDonald et al 2005)) and hybrid systems (eg (Sagae and Lavie 2006b Nivre and McDonald 2008)) which combine the output of various parsers into a new and improved parse and which are orthogonal to our approach 
A solution that leverages the complementary strengths of these two approaches described in detail by McDonald and Nivre (2007)was recently and successfully explored by Nivre and McDonald (2008) 
For comparison we also include the results of the two top scoring systems in the CoNLL-X shared task those of McDonald Lerman and Pereira (2006) and Nivre et al 
Alternatively discriminative parsing is tractable with exact and efficient search based on dynamic programming (DP) if all features are restricted to be local that is only looking at a local window within the factored search space (Taskar et al 2004 McDonald et al 2005) 
The second order model of Carreras (2007) incorporates both sibling and grandparent parts and needs O(n4) parsing time 
We follow the edge based factorization method of Eisner (1996) and define the score of a dependency tree as the sum of the score of all edges in the tree s(xy) = summation display (ij)y s(ij) = summation display (ij)y w f(ij) where f(ij) is a high-dimensional binary feature representation of the edge from xi to xj 
Sen T/S Lem CPoS PoS MSF Dep NPT NPS Danish 94 52 182 no 10 24 47 52 10 156 Dutch 195 133 146 yes 13 302 81 26 54 364 German 700 392 178 no 52 52 0 46 23 278 Portuguese 207 91 228 yes 15 21 146 55 13 189 Slovene 29 15 187 yes 11 28 51 25 19 222 Table 1 Data sets Tok = number of tokens (*1000) Sen = number of sentences (*1000) T/S = tokens per sentence (mean) Lem = lemmatization present CPoS = number of coarse-grained part-of-speech tags PoS = number of (fine-grained) part-of-speech tags MSF = number of morpho syntactic features (split into atoms) Dep = number of dependency types NPT = proportion of non-projective dependencies/tokens (%) NPS = proportion of non-projective dependency graphs/sentences (%) The history-based classifier can be trained with any of the available supervised methods for function approximation butin the experiments below we will rely on SVM which has previously shown good performance for this kind of task (Kudo and Matsumoto 2002 Yamada and Matsumoto 2003) 
The deterministic shift/reduce classifier-based dependency parsing approach (Nivre and Scholz 2004) has been shown to offer state-of-the-art accuracy (Nivre et al  2006) with high efficiency due to a greedy search strategy 
To match previous work (McDonald et al 2005 McDonald and Pereira 2006 Koo et al 2008) we split the data into a training set (sections 2-21) a development set (Section 22) and a test set (section 23) 
Blended parser is an ensemble system based on the methodology proposed by Sagae and Lavie (2006) 
Most recent English dependency parsers produce one of three sets of dependency types unlabeled some variant of the coarse labels used by the CoNLL dependency parsing shared-tasks (Buchholz and Marsi 2006 Nivre et al 2007) (eg ADV NMOD PMOD) or Stanfords dependency labels (de Marneffe and Manning 2008) 
Nivre and Scholz (2004) proposed a variant of the model of Yamada and Matsumoto that reduces the complexity from the worst case quadratic to linear 
If projectivity (no crossing branches) is desired Eisners (1996) dynamic programming algorithm (similar to CYK) for dependency parsing can be used instead 
As evident from the CoNLL-X shared task on dependency parsing (Buchholz and Marsi 2006) there are currently two dominant models for data-driven dependency parsing 
We reimplemented Eisners decoder (Eisner 1996) which searches among all projective parse trees and the Chu-Liu-Edmonds decoder (Chu and Liu 1965 Edmonds 1967) which searches in the space of both projective and non-projective parses 
It consists of the second order parsing algorithm of Carreras (2007) the non-projective approximation algorithm (McDonald and Pereira 2006) the passive aggressive support vector machine and a feature extraction component 
The English sentences are then parsed by an implementation of 2nd-ordered MST model of McDonald and Pereira (2006) which is trained on dependency trees extracted from WSJ 
We used the pseudo-projective transformation introduced in (Nivre and Nilsson 2005) to cast non-projective parsing tasks as projective 
The data partition and head rules were chosen to match previous work (Yamada and Matsumoto 2003 McDonald et al 2005a McDonald and Pereira 2006) 
We obtain k-best list sand forests generated from the baseline discriminative model which has the same feature set as provided in (McDonald et al 2005) using the second order Eisner algorithms 
All non-projective dependencies in the training and gold sets were projectivized prior to training and parsing using the algorithm of Nivre and Nilsson (2005) 
All systems are evaluated using unlabeled attachment score (UAS) which is the percentage of words (ignoring punctuation tokens) in a corpus that modify the correct head (Buchholz and Marsi 2006) 
To simplify implementation we instead opted for the pseudo-projective approach (Nivre and Nilsson 2005) in which nonprojective links are lifted upwards in the tree to achieve projectivity and special trace labels are used to enable recovery of the nonprojective links at parse time 
That work extends the maximum spanning tree dependency parsing framework (McDonald et al  2005a McDonald et al  2005b) to incorporate features over multiple edges in the dependency graph 
Any details concerning the conversion from the original formats of the various treebanks to the CoNLLformata pure dependency based formatare found in documentation referred to in Buchholz and Marsi (2006) 
Similarly to Carreras (2007) we see that these features have a very large impact on parsing accuracy but also that the parser pays dearly in terms of efficiency as the search complexity increases fromO(n3)toO(n4) 
An example of a stacked feature is a binary feature f2a(xg(x)) that fires if and only if the arc a was predicted by g ie if aAg(x) such a feature was used by Nivre and McDonald (2008) 
The corresponding unlabeled figures are 733 and 3343 This confirms the results of previous studies showing that the pseudo-projective parsing technique used by MaltParser tends to give high precision given that non-projective dependencies are among the most difficult to parse correctly but rather low recall (McDonald and Nivre 2007) 
MST McDonald and Pereira (2006)s dependency parser1 based on the Eisner algorithm for projective dependency parsing (Eisner 1996) with the second order factorization 
Non-local information such as arity (or valency) and neighbouring dependencies can be crucial to obtaining high parsing accuracies (Klein and Manning 2002 McDonald and Pereira 2006) 
Decoding can be carried out using Viterbistyle dynamic-programming algorithms for example the O(n3) algorithm of Eisner (1996) 
The Perceptron style for natural language processing problems as initially proposed by (Collins 2002) can provide state of the art results on various domains including text chunking syntactic parsing etc The main drawback of the Perceptron style algorithm is that it does not have a mechanism for attaining the maximize margin of the training data 
Data-driven dependency parsing has been shown to give accurate and efficient parsing for a wide range of languages such as Japanese (Kudo and Matsumoto 2002) English (Yamada and Matsumoto 2003) Swedish (Nivre et al  2004) Chinese (Cheng et al  2004) and Czech (McDonald et al  2005) 
For non-projective parsing we use the NonProjective Approximation Algorithm of McDonald and Pereira (2006) 
Note that it is straightforward to calculate these expected counts using a variant of the inside-outside algorithm (Baker 1979) applied to the (Eisner 1996) dependency-parsing data structures (Paskin 2001) for projective dependency structures or the matrix-tree theorem (Koo et al 2007 Smith and Smith 2007 McDonald and Satta 2007) for nonprojective dependency structures 
Let R(TTprime) be the Hamming distance between two dependency graphs for an input sentence x = x0x1 xn R(TTprime) = n summation display (ij)kET I((ij)kTprime) This is a common definition of risk between two graphs as it corresponds directly to labeled dependency parsing accuracy (McDonald et al  2005a Buchholz et al  2006) 
This type of model has been used by among others Eisner (1996) McDonald et al 
Most of these features are inspired by previous work in dependency parsing (McDonald et al  2005 Collins 1999) 
Stepwise approaches can use an explicit probability model over next steps eg a generative one (Eisner 1996 Dreyer et al  2006) or train a machine learner to predict those 
The work of Eisner (1996) showed that the argmax problem for digraphs could be solved in O(n3) using a bottom up dynamic programming algorithmsimilartoCKY 
Nivre and Scholz (2004) developed a history-based learning model 
Transition based parsers typically have a linear or quadratic complexity (Nivre et al 2004 Attardi 2006) 
The data sets used are taken from the CoNLL-X shared task on multilingual dependency parsing (Buchholz and Marsi 2006) 
Unlabeled attachment score (UAS) The proportion of words that are assigned the correct head (or no head if the word is a root) (Eisner 1996 Collins et al  1999) 
In this paper we employ the MSTParser (McDonald et al 2006) and MaltParser (Nivre 2003) for comparison 
Some support for this view can be found in the results from the CoNLL shared tasks on dependency parsing in 2006 and 2007 where a variety of data-driven methods for dependency parsing have been applied with encouraging results to languages of great typological diversity (Buchholz and Marsi 2006 Nivre et al 2007a) 
Best-performing constituency parsers like Charniak (2000) and Berkeley (Petrov and Klein 2007) do outperform our parser since they consider more information during parsing but they are at least 5 times slower 
Following standard practice for higher-order dependency parsing (McDonald and Pereira 2006 Carreras 2007) Models 1 and 2 evaluate not only the relevant third-order parts but also the lower-order parts that are implicit in their third-order factorizations 
It uses graph transformation to handle non-projective trees (Nivre and Nilsson 2005) 
A typical approach in graph-based dependency parsing has been to assume a factorized model where local features are used but a global function is optimized (McDonald et al 2005b) 
Nivre and McDonald (2008) present an application of stacked learning to dependency parsing in which a second predictor is trained to improve the performance of the first 
The problem occurs most often on punctuations (66/84 on WSJ section 23) so it affects only marginally the accuracy scores (UAS LAS) as computed in the CoNLL-X evaluation (Buchholz & Marsi 2006) 
We follow McDonald and Nivre (2007) and characterize the errors of the two parsers by sentence and dependency length and dependency type 
We used data from the CoNLL-X shared task on multilingual dependency parsing (Buchholz and Marsi 2006) 
This formalization generalizes standard projective parsing models based on the Eisner algorithm (Eisner 1996) to yield efficient O(n2) exact parsing methods for nonprojective languages like Czech 
We projectivize training data by a minimal transformation lifting non-projective arcs one step at a time and extending the arc label of lifted arcs using the encoding scheme called HEAD by Nivre and Nilsson (2005) which means that a lifted arc is assigned the label rh where r is the original label and h is the label of the original head in the non-projective dependency graph 
The Setup We use two different parsers (i) MaltParser (Nivre et al 2007b) with the arc eager algorithm as optimized for English in (Nivre et al 2010) and (ii) MSTParser with the second-order projective model of McDonald and Pereira (2006) 
In the following decade great success in terms of parse disambiguation and even language modeling was achieved by various lexicalized PCFG models (Magerman 1995 Charniak 1997 Collins 1999 Charniak 2000 Charniak 2001) 
We extracted tagged sentences from the parse trees5 We split the data into training development and test sets as in (Collins 2002) 
In another study Blaheta and Charniak (2000) report an F-measure of 989% for the assignment of Penn Treebank grammatical role labels (our G set) to phrases that were correctly parsed by the parser described in Charniak (2000) 
The 50-best parser is a probabilistic parser that on its own produces high quality parses the maximum probability parse trees (according to the parsers model) have an f-score of 0897 on section 23 of the Penn Treebank (Charniak 2000) which is still state-of-the-art 
As expected we see that MST does better than Malt for all categories except nouns and pronouns (McDonald and Nivre 2007) 
Given a feature representation for edges and a weight vector w we seek the dependency tree or 92 h1 h1 h2 h2 s h1 h1 r r+1 h2 h2 t h1 h1 h2 h2 s h1 h1 h2 h2 t h1 h1 s h1 h1 t Figure 2 O(n3) algorithm of Eisner (1996) needs to keep 3 indices at any given stage 
Nivre and McDonald (2008) and Zhang and Clark (2008) proposed stacking methods to combine graph-based parsers with transition-based parsers 
Recently statistical dependency parsing techniques have been proposed which are deterministic and/or linear (Yamada and Matsumoto 2003 Nivre and Scholz 2004) 
A simple example is shown in Figure 1 where the arc between a and hat indicates that hat is the head of a Current statistical dependency parsers perform better if the dependency lengthes are shorter (McDonald and Nivre 2007) 
As far as we know this makes it different from all previous systems for dependency parsing applied to the Penn Treebank (Eisner 1996 Yamada and Matsumoto 2003) although there are systems that extract labeled grammatical relations based on shallow parsing eg Buchholz (2002) 
Because the frequency of non-projective dependencies in the Turkish Treebank is not high enough to learn such dependencies and mostly due to the unconnected punctuations with which we are dealing by adding an extra dependency labelwe did not observe any improvement when applying the pseudo-projective processing of Nivre and Nilsson (2005)which is reported to improve accuracy for other languages 
Since the number of non-projective dependencies is much smaller than the number of projective dependencies (Nivre and Nilsson 2005) it is not efficient to perform non-projective parsing for all cases 
In order to compare the performance to the state of the art in dependency parsing we have retrained the non-projective parser on the entire training data set for each language and evaluated it on the final test set from the CoNLL-X shared task (Buchholz and Marsi 2006) 
MaltParser (Nivre et al 2007b) is a language independent system for data-driven dependency parsing based on a transition-based parsing model (McDonald and Nivre 2007) 
This averaging effect has been shown to help overfit ting (Collins 2002) 
For the comparison of English dependency accuracy excluding punctuation MIRA and BPM are both statistically significantly better than the averaged perceptron result reported in (McDonald et al  2005a) 
Examples include the margin perceptron (Duda et al  2001) ALMA (Gentile 2001) and MIRA (which is used to train the parser in (McDonald et al  2005a)) 
Unlike most previous work on data-driven dependency parsing (Eisner 1996 Collins et al  1999 Yamada and Matsumoto 2003 Nivre 2003) we assume that dependency graphs are labeled with dependency types although the evaluation will give results for both labeled and unlabeled representations 
Therefore the integration of both techniques as in Nivre and McDonald (2008) seems to be very promising 
Table 5 reports experimental results by using the first order decoding method in which an MST parsing algorithm (McDonald et al  2005b) is applied for non-projective parsing and the Eisners method is used for projective language data 
We briefly describe the tagger (see (Ciaramita & Altun 2006) for more details) a Hidden Markov Model trained with the perceptron algorithm introduced in (Collins 2002) 
To study the influence of lan968 guage and treebank specific properties we will use data from Arabic Czech Dutch and Slovene taken from the CoNLL-X shared task on multilingual dependency parsing (Buchholz and Marsi 2006) 
Parsers and settings All experiments were performed using two datadriven parsers MaltParser6 (Nivre et al 2007b) and MSTParser7 (McDonald et al 2006) 
The feature types are essentially those described in (McDonald et al  2005a) 
This distinction roughly corresponds to the distinction made by Buchholz and Marsi (2006) between stepwise and all-pairs approaches 
The baseline parser resembles the architecture of McDonald and Pereira (2006) 
Table 3 shows the results achieved by our method and other researchers (UAS with p) where Wang05 refers to (Wang et al 2005) Wang07 refers 92 to (Wang et al 2007) and McDonald&Pereira06 5 refers to (McDonald and Pereira 2006) 
We build an ISBN model of dependency parsing using the parsing order proposed in (Nivre et al  2004) 
Previous work has shown that conditioning on neighboring decisions can lead to significant improvements in accuracy (Yamada and Matsumoto 2003 Charniak 2000) 
We extend the projective parsing algorithm of Eisner (1996) for our case and train models using the averaged perceptron 
Greedy local search (Yamada and Matsumoto 2003 Sagae and Lavie 2005 Nivre and Scholz 2004) has typically been used for decoding in shift-reduce parsers while beam-search has recently been applied as an alternative to reduce error-propagation (Johansson and Nugues 2007 Zhang and Clark 2008 Zhang and Clark 2009 Huang et al 2009) 
Specifically we make the following contributions we develop a baseline shift-reduce dependency parser using the less popular but classical arc-standard style (Section 2) and achieve similar state-of-the-art performance with the the dominant but complicated arceager style of Nivre and Scholz (2004) we propose bilingual features based on word alignment information to prefer target-side contiguity in resolving shift-reduce conflicts (Section 3) we verify empirically that shift-reduce conflicts are the major source of errors and correct shift-reduce decisions strongly correlate with the above bilingual contiguity conditions even with automatic alignments (Section 53) finally with just three bilingual features we improve dependency parsing accuracy by 06% for both English and Chinese over the state-of-the-art baseline with negligible (6%) efficiency overhead (Section 54) 
We demonstrate the application of our procedure to comparing dependency parsing results on different versions of the Penn Treebank (Marcus et al 1993) 
Computation of the marginals and partition function can also be achieved in O(n3) time using a variant of the inside-outside algorithm (Baker 1979) applied to the Eisner (1996) data structures (Paskin 2001) 
Sagae and Lavie (2006) later used weighted voting and reparsing ie using CLE to find the dependency tree that reflects the maximum number of votes 
Indeed the result of Collins (2002) that including low support features helps a voted perceptron model but harms a maximum entropy model is undone once the weights of the maximum entropy model are regularized 