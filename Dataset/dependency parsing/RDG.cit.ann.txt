P29,M23	Nivre and Nilsson (2005) showed how the restriction to projective dependency graphs could be lifted by using graph transformation techniques to preprocess training data and post-process parser output, so-called pseudo-projective parsing. ::: D07-1013_57:290
	
	However, to make parsing tractable, these models are forced to restrict features over a very limited history of parsing decisions (McDonald and Pereira, 2006; McDonald and Nivre, 2007). ::: C10-2015_9:184
	
P38	Edge-factored models have many computational benefits, most notably that inference for nonprojective dependency graphs can be achieved in polynomial time (McDonald et al. , 2005b). ::: W07-2216_27:259
	
	The latest state-of-the-art statistical dependency parsers are discriminative, meaning that they are based on classifiers trained to score trees, given a sentence, either via factored whole-structure scores (McDonald et al. , 2005a) or local parsing decision scores (Hall et al. , 2006). ::: D07-1014_10:189
	
	Although the higher-order MST parsing models will be slow with exact inference, requiring O(nk) time (McDonald and Pereira, 2006), it might be possible to use higher-order kgram subtrees with approximated parsing model in the future. ::: D09-1060_112:253
	
M26	Yet, they can be parsed in O(n3) time (Eisner, 1996). ::: P05-1012_14:209
	
	Dependency-based syntactic parsing has become a widely used technique in natural language processing, and many different parsing models have been proposed in recent years (Yamada and Matsumoto, 2003; Nivre et al., 2004; McDonald et al., 2005a; Titov and Henderson, 2007; Martins et al., 2009). ::: P10-1151_5:201
	
E02	Our first observation is that the accuracies for both systems are considerably below the 90% unlabeled and 88% labeled attachment scores for English that have been reported previously (McDonald and Pereira, 2006; Hall et al., 2006). ::: C10-1094_133:185
	
M01	When this analysis is coupled with the projective parsing algorithms of Eisner (1996) andPaskin (2001) we beginto get aclear picture of the complexity for data-driven dependency parsing within an edge-factored framework. ::: W07-2216_244:259
	
	In conditional log-linear models for dependency parsing (which are closely related to conditional random fields (Lafferty et al., 2001)), a distribution over dependency structures for a sentence x is defined as follows: p(y|x) = 1Z(x) exp{g(x,y)}, (1) where Z(x) is the partition function, w is a parameter vector, and g(x,y) = summation display (h,m,l)y wf(x,h,m,l) Here f(x,h,m,l) is a feature vector representing the dependency (h,m,l) in the context of the sentence x (see for example (McDonald et al., 2005a)). ::: D09-1058_33:216
	
	In which case, the parsing problem reduces to a18a37a36 a1a39a38a41a40a43a42a45a44a46a38a48a47 a49a22a50a41a51a53a52a55a54a57a56 a58 a52a60a59a62a61a5a63a64a59a66a65a43a56a67a50a26a49 sa2a5a4a29a19a22a21a23a4a25a24a28a16 (1) where the score sa2a5a4 a19 a21 a4 a24 a16 can depend on any measurable property of a4a20a19 and a4a25a24 within the tree a18 . This formulation is suf ciently general to capture most dependency parsing models, including probabilistic dependency models (Wang et al. , 2005; Eisner, 1996) as well as non-probabilistic models (McDonald et al. , 2005; Wang et al. , 2006). ::: N07-3002_20:74
	
V01	The data All our data comes from the CoNLL-X Shared Task (Buchholz and Marsi, 2006), specifically the 4 data sets freely available online. ::: P10-1075_138:213
	
S01	McDonald and Pereira (2006) have shown that incorporating second order features relating to adjacent edge pairs improves the accuracy of maximum spanning tree parsers (MST). ::: W07-2217_18:259
	
M24	We then describe the two main paradigms for learning and inference, in this years shared task as well as in last years, which we call transition-based parsers (section 5.2) and graph-based parsers (section 5.3), adopting the terminology of McDonald and Nivre (2007).5 Finally, we give an overview of the domain adaptation methods that were used (section 5.4). ::: D07-1096_230:410
	
	We tested this hypothesis by using the Charniak (2000) parser in n-best mode, producing the top 10 trees with corresponding probabilities. ::: P06-2089_148:169
	
M09,M10	The dominant learning method in this tradition is support vector machines (Kudo and Matsumoto 2002; Yamada and Matsumoto 2003; Nivre et al. 2006) but memory-based learning has also been used (Nivre, Hall, and Nilsson 2004; Nivre and Scholz 2004; Attardi 2006). ::: J08-4003_571:595
	
P29,M12,M23	For treebanks with non-projective trees we use the pseudo-projective parsing technique to transform the treebank into projective structures (Nivre and Nilsson, 2005). ::: D11-1006_71:303
	
	The second order parsing algorithm of McDonald and Pereira (2006) uses a separate algorithm for edge labeling. ::: C10-2129_31:191
	
M02,M21,M08	Transition Precondition NivresMethod Left-Arc (|wi,wj|,A) (,wj|,A{(wj,wi)}) i = 0wk (wk,wi) A Right-Arc (|wi,wj|,A) (|wi|wj,,A{(wi,wj)}) Reduce (|wi,,A) (,,A) wk (wk,wi) A Shift (,wj|,A) (|wj,,A) Proposed Method Left-Arc (|ti,tj|,A) (,tj|,A{(wj,wi)}) i = 0 Right-Arc (|ti,tj|,A) (|ti,,A{(mphc(ti,tj),wj)}) Shift (,tj|,A) (|tj,,A) studies taking data-driven approaches, by (Kudo and Matsumoto, 2002), (Yamada and Matsumoto, 2003), and (Nivre, 2003), the deterministic incremental parser was generalized to a state transition system in (Nivre, 2008). ::: P10-2035_28:126
	
	It is well known that dependency trees extracted from lexicalized phrase structure parsers (Collins, 1999; Charniak, 2000) typically are more accurate than those produced by pure dependency parsers (Yamada and Matsumoto, 2003). ::: P05-1012_174:209
	
	For details we refer the reader to (Nivre et al. , 2004). ::: W07-2218_99:279
	
	Statistical parsing has been one of the most active areas of research in the computational linguistics community since the construction of the Penn Treebank (Marcus et al., 1993). ::: D11-1006_7:303
	
	For many different part factorizations and structure domains Y(), it is possible to solve the above maximization efficiently, and several recent efforts have concentrated on designing new maximization algorithms with increased context sensitivity (Eisner, 2000; McDonald et al., 2005b; McDonald and Pereira, 2006; Carreras, 2007). ::: P08-1068_38:218
	
	We evaluated our classifier-based best-first parser on the Wall Street Journal corpus of the Penn Treebank (Marcus et al. , 1993) using the standard split: sections 2-21 were used for training, section 22 was used for development and tuning of parameters and features, and section 23 was used for testing. ::: P06-2089_95:169
	
M33	As such, it follows a bottom-up strategy, or bottom-up-trees, as defined in Buchholz and Marsi (2006), in contrast to the shift-reduce dependency parsing algorithm described by Nivre (2003), which is a bottom-up/topdown hybrid, or bottom-up-spans. ::: D07-1111_23:130
	
	We take as our starting point a re-implementation of McDonalds state-of-the-art dependency parser (McDonald et al. , 2005a). ::: N06-1021_54:189
	
	Eisner (1996) algorithm with non-projective rewriting and second order features. ::: D11-1116_118:208
	
M01	Syntactic parsing using dependency structures has become a standard technique in natural language processing with many different parsing models, in particular data-driven models that can be trained on syntactically annotated corpora (Yamada and Matsumoto, 2003; Nivre et al., 2004; McDonald et al., 2005a; Attardi, 2006; Titov and Henderson, 2007). ::: P09-1040_5:154
	
M23	The first is simplicity, given that it is based on a single transition system and makes a single pass over the input, whereas the pseudo-projective parsing technique involves preprocessing of training data and post-processing of parser output (Nivre and Nilsson, 2005). ::: P10-1151_195:201
	
	A parsing algorithm for building the dependency analyses (Eisner 1996; Sekine,Uchimoto,and Isahara 2000) 2. ::: J08-3003_116:443
	
M23	For comparison, we also trained two parsers using Sp, one that is strictly projective and one that uses the pseudo-projective parsing technique to recover non-projective dependencies in a post-processing step (Nivre and Nilsson, 2005). ::: P09-1040_120:154
	
M01	Natural language parsing with data-driven dependency-based frameworks has received an increasing amount of attention in recent years (McDonald et al., 2005; Buchholz and Marsi, 2006; Nivre et al., 2006). ::: C08-1095_5:163
	
	To overcome this, McDonald and Pereira (2006) use a 15We leave labeled parsing experiments to future work. ::: P08-1068_156:218
	
E03	F 1 =2 precision recall/(precision + recall) the figure, we find that F 1 score decreases when dependency length increases as (McDonald and Nivre, 2007) found. ::: I08-1012_11:180
	
P29,M23	Although the parser only derives projective graphs, the fact that graphs are labeled allows non-projective dependencies to be captured using the pseudo projective approach of Nivre and Nilsson (2005). ::: W06-2933_16:84
	
	These settings match the evaluation setting in previous work such as (McDonald et al., 2005a; Koo et al., 2008). ::: D09-1058_145:216
	
	We use the popular online learning algorithm of structured perceptron with parameter averaging (Collins, 2002). ::: D09-1127_91:211
	
	To train models, we used projectivized versions of the training dependency trees.2 1We are grateful to the providers of the treebanks that constituted the data for the shared task (Hajic et al. , 2004; Aduriz et al. , 2003; Mart et al. , 2007; Chen et al. , 2003; Bohmova et al. , 2003; Marcus et al. , 1993; Johansson and Nugues, 2007; Prokopidis et al. , 2005; Csendes et al. , 2005; Montemagni et al. , 2003; Oflazer et al. , 2003). ::: D07-1101_72:204
	
	Parsing algorithms which search the entire space (Eisner, 1996; McDonald, 2006) are restricted in the features they use to score a relation. ::: D07-1124_12:98
	
S01	For our experiments, we use MSTParser 0.4.3b 4 with 1-best projective decoding, using the algorithm of Eisner (1996), and second order features. ::: C10-2013_83:211
	
M	NP-L = non-projective list-based; P-L = projective list-based; PP-L = pseudo-projective list-based; P-E = projective arc-eager stack-based; PP-E = pseudo-projective arc-eager stack-based; P-S = projective arc-standard stack-based; PP-S = pseudo-projective arc-standard stack-based; McD = McDonald, Lerman and Pereira (2006); Niv = Nivre et al. ::: J08-4003_452:595
	
	Parsing technologies have improved considerably in the past few years, and high-performance syntactic parsers are no longer limited to PCFG-based frameworks (Charniak, 2000; Klein and Manning, 2003; Charniak and Johnson, 2005; Petrov and Klein, 2007), but also include dependency parsers (McDonald and Pereira, 2006; Nivre and Nilsson, 2005; Sagae and Tsujii, 2007) and deep parsers (Kaplan et al., 2004; Clark and Curran, 2004; Miyao and Tsujii, 2008). ::: P08-1006_7:181
	
	Global-optimization parsing methods are another common approach (Eisner, 1996; McDonald et al., 2005). ::: P10-2035_16:126
	
M33	A key difference with previous work on shift reduce dependency (Nivre et al., 2006) and CFG (Sagae and Lavie, 2006b) parsing is that, for CCG, there are many more shift actions a shift action for each word-lexical category pair. ::: P11-1069_66:201
	
M26	Eisner (1996) made the observation that if the head of each chart item is on the left or right periphery, then it is possible to parse in O(n3). ::: P05-1012_55:209
	
M01,M27	Eisner (1996) introduced a data-driven dependency parser and compared several probability models on (English) Penn Treebank data. ::: W06-2920_44:416
	
M24	Thus, transition-based parsers normally run in linear or quadratic time, using greedy deterministic search or fixed-width beam search (Nivre et al., 2004; Attardi, 2006; Johansson and Nugues, 2007; Titov and Henderson, 2007), and graph-based models support exact inference in at most cubic time, which is efficient enough to make global discriminative training practically feasible (McDonald et al., 2005a; McDonald et al., 2005b). ::: P09-1040_7:154
	
	This technique is similar to the parser voting methods used by Sagae and Lavie (2006). ::: D07-1013_195:290
	
F04,M37	Although (McDonald et al., 2005) used the prefix of each word form instead of word form itself as features, character-level features here for Chinese is essentially different from that. ::: P09-1007_110:206
	
	Graph transformations for recovering nonprojective structures (Nivre and Nilsson, 2005). ::: W06-2933_9:84
	
M10	Nivre and Nilsson (2005) presented a parsing model that allows for the introduction of non-projective edges into dependency trees through learned edge transformations within their memory-based parser. ::: H05-1066_30:223
	
M16	In this paper, we only used bigram-subtrees and the limited form of trigram-subtrees, though in theory we can use k-gram-subtrees, which are limited in the same way as our trigram subtrees, in (k-1)th-order MST parsing models mentioned in McDonald and Pereira (2006) or use grandparent type trigram-subtrees in parsing models of Carreras (2007). ::: D09-1060_111:253
	
	Our out-of-domain data is the Wall Street Journal (WSJ) portion of the Penn Treebank (Marcus et al. , 1993) which consists of about 40,000 sentences (one million words) annotated with syntactic information. ::: P06-1043_73:213
	
E02,V01,M37	Despite its simplicity, the partial correspondence approach proves very effective and leads to parsers that achieve unlabeled attachment scores that are only 5% behind the average UAS for Dutch in the CoNLL-X Shared Task (Buchholz and Marsi, 2006). ::: W09-1104_17:228
	
	In particular, one would assume that the score of a complete spanning tree Y for a given sentence, whether probabilistically motivated or not, can be decomposed as a sum of local scores for each link (a word pair) (Eisner, 1996; Eisner and Satta, 1999; McDonald et al., 2005a). ::: P08-1061_56:148
	
	Sagae and Lavie (2006a) and Zeman and abokrtsk (2005) have observed that reversing the direction of stepwise parsers can be beneficial in parser combinations. ::: D07-1111_65:130
	
	There are two main dependency parsing approaches (Nivre and McDonald, 2008). ::: P11-2121_8:136
	
	See (Charniak, 2000) for details. ::: P06-2089_153:169
	
	This becomes parsing failures in practice (Nivre and Scholz, 2004), leaving more than one fragments on stack. ::: D09-1127_78:211
	
P29	Like context-free grammars, projective dependency trees are not sufficient to represent all the linguistic phenomena observed in natural languages, but they have the advantage of being efficiently parsable: their parsing problem can be solved in cubic time with chart parsing techniques (Eisner, 1996; Gomez-Rodrguez et al., 2008), while in the case of general non-projective dependency forests, it is only tractable under strong independence assumptions (McDonald et al., 2005b; McDonald and Satta, 2007). ::: P10-1151_42:201
	
	Notably, using a simple arc-factored parser at level 1, we obtain an exact O(n2) stacked parser that outperforms earlier approximate methods (McDonald and Pereira, 2006). ::: D08-1017_213:217
	
M02	In particular, Nivre and Scholz (2004) and Attardi (2006) have developed deterministic dependency parsers with linear complexity, suitable for processing large amounts of text, as required, for example, in information retrieval applications. ::: N07-1049_11:180
	
F05	Before parsing, POS tags are assigned to the input sentence using our reimplementation of the POStagger from Collins (2002). ::: D08-1059_118:188
	
	Either they employ a non-standard parsing algorithm that can combine non-adjacent substructures (McDonald et al., 2005b; Attardi, 2006; Nivre, 2007), or they try to recover non351 ROOT0 A1 a7 a4 a63 DET hearing2 a7 a4 a63 SBJ is3 a7 a4 a63 ROOT scheduled4 a7 a4 a63 VG on5 a7 a4 a63 NMOD the6 a7 a4 a63 DET issue7 a7 a4 a63 PC today8 a7 a4 a63 ADV .9 a63 a7 a4P Figure 1: Dependency tree for an English sentence (non-projective). ::: P09-1040_18:154
	
	Oracles for practical parsers can be obtained by training classifiers on treebank data (Nivre et al., 2004). ::: P10-1151_101:201
	
	Comparison to Related Work Several ensemble models have been proposed for dependency parsing (Sagae and Lavie, 2006; Hall et al., 2007; Nivre and McDonald, 2008; Attardi and DellOrletta, 2009; Surdeanu and Manning, 2010). ::: P11-2125_83:98
	
	McDonald and Pereira (2006) define a second-order dependency parsing model in which interactions between adjacent siblings are allowed, and Carreras (2007) defines a second-order model that allows grandparent and sibling interactions. ::: D07-1015_178:443
	
	Given the need for high-quality dependency parses in applications such as statistical machine translation (Xu et al., 2009), natural language generation (Wan et al., 2009), and text summarization evaluation (Owczarzak, 2009), there is a corresponding need for high-quality dependency annotation, for the training and evaluation of dependency parsers (Buchholz and Marsi, 2006). ::: P10-1075_5:213
	
	We instead make use of a simple two-stage approach for extending the SS-SCM approach to the second-order parsing model of (Carreras, 2007). ::: D09-1058_101:216
	
M03	Things become worse still in a parser like the one described in Charniak (2000) because it conditions on (and hence splits the dynamic programming states according to) features of the grandparent node in addition to the parent, thus multiplying the number of possible dynamic programming states even more. ::: P05-1022_56:180
	
M01	Models for data-driven dependency parsing can be roughly divided into two paradigms: Graph-based and transition-based models (McDonald and Nivre, 2007). ::: W09-1104_113:228
	
M02	Our probability model uses the parsing order proposed in (Nivre et al. , 2004), but instead of performing deterministic parsing as in (Nivre et al. , 2004), this ordering is used to define a generative history based model, by adding word prediction to the Shift parser action. ::: D07-1099_16:90
	
	For more information on the data sets, see Buchholz and Marsi (2006). ::: P08-1108_96:172
	
M01	Practically all data-driven models that have been proposed for dependency parsing in recent years can be described as either graph-based or transition based (McDonald and Nivre, 2007). ::: P08-1108_11:172
	
M03	Given the lattice and Gs,s, lattice parsing is a straightforward generalization of the standard arc-factored dynamic programming algorithm from Eisner (1996). ::: D11-1044_125:210
	
	The second order algorithm of Carreras (2007) uses in addition to McDonald and Pereira (2006) the child of the dependent occurring in the sentence between the head and the dependent, and the an edge to a grandchild. ::: C10-1011_37:243
	
M37	Both English and Chinese sentences are tagged by the implementations of the POS tagger of Collins (2002), which trained on WSJ and CTB 5.0 respectively. ::: P10-1002_150:187
	
M16,M09	ULISSE was tested against the output of two really different datadriven parsers: the firstorder Maximum Spanning Tree (MST) parser (McDonald et al., 2006) and the DeSR parser (Attardi, 2006) using Support Vector Machine as learning algorithm. ::: W11-0314_61:167
	
	Carreras (2007) employs his own extension of Eisners algorithm for the case of projective trees and second-order models that include head grandparent relations. ::: D07-1096_280:410
	
	Most previous dependency parsing models have focused on projective trees, including the work of Eisner (1996), Collins et al. ::: H05-1066_24:223
	
M37	Unsupervised dependency parsing has seen rapid progress recently, with error reductions on English (Marcus et al., 1993) of about 15% in six years (Klein and Manning, 2004; Spitkovsky et al., 2010), and better and better results for other languages (Gillenwater et al., 2010; Naseem et al., 2010), but results are still far from what can be achieved with small seeds, language-specific rules (Druck et al., 2009) or using cross-language adaptation (Smith and Eisner, 2009; Spreyer et al., 2010). ::: W11-1109_12:202
	
	In turn, those features were inspired by successful previous work in firstorder dependency parsing (McDonald et al. , 2005). ::: D07-1101_65:204
	
	Then, we substitute Step 3 as a supervised learning such as MIRA with a second-order parsing model (McDonald et al., 2005a), which incorporates q1 as a real-values features. ::: D09-1058_105:216
	
	The best phrase-structure parsing models represent generatively the joint probability P(x,y) of sentence x having the structure y (Collins, 1999; Charniak, 2000). ::: P05-1012_5:209
	
M02,M16	One of the surprising things discovered by this evaluation is that the best results are achieved by methods which are quite different from state-of-the-art models for constituent parsing, e.g. the deterministic parsing method of (Nivre et al. , 2006) and the minimum spanning tree parser of (McDonald et al. , 2006). ::: W07-2218_8:279
	
V01	In this vein, the CoNLL 2008 shared task sets the challenge of learning jointly both syntactic dependencies (extracted from the Penn Treebank (Marcus et al., 1993) ) and semantic dependencies (extracted both from PropBank (Palmer et al., 2005) c2008. ::: W08-2122_6:117
	
M02	Deterministic dependency parsers which run in linear time have also been developed (Nivre & Scholz, 2004; Attardi, 2006). ::: W07-2217_14:259
	
M01,S01,S02	We conducted experiments with two data-driven parsers, MaltParser (Nivre et al., 2007b) and MSTParser (McDonald et al., 2006). ::: W10-1403_18:262
	
	These results are similar in spirit to (Nivre and McDonald, 2008), but with the following novel contributions: a stacking interpretation, a richer feature set that includes non-local features (shown here to improve performance), and a variety of stacking architectures. ::: D08-1017_56:217
	
	Note that Models 1 and 2 have the same complexity as Carreras (2007), but strictly greater expressiveness: for each sibling or grandchild part used in the Carreras (2007) factorization, Model 1 defines an enclosing grand-sibling, while Model 2 defines an enclosing tri-sibling or grand-sibling. ::: P10-1001_162:237
	
	However, current statistical dependency parsers provide worse results if the dependency length becomes longer (McDonald and Nivre, 2007). ::: I08-1012_7:180
	
V01	Table 1 gives additional statistics for treebanks from the CoNLL-X shared task (Buchholz and Marsi, 2006). ::: D11-1114_63:247
	
	In dependency reparsing we focus on unlabeled dependencies, as described by Eisner (1996). ::: N06-2033_12:93
	
	Our labeled data comes from the Penn Treebank (Marcus et al. , 1993) and consists of about 40,000 sentences from Wall Street Journal (WSJ) articles 153 annotated with syntactic information. ::: N06-1020_68:208
	
M24	Comparing with the baselines, we observe that our full model outperforms that of McDonald and Pereira (2006), and is in line with the most accurate dependency parsers (Nivre and McDonald, 2008; Martins et al., 2008), obtained by combining transition-based and graph-based parsers.14 Notice that our model, compared with these hybrid parsers, has the advantage of not requiring an ensemble configuration (eliminating, for example, the need to tune two parsers). ::: P09-1039_191:228
	
	R O O T ::: C10-2015_26:184
	
	We evaluated the ISBN parser on all the languages considered in the shared task (Hajic et al. , 2004; Aduriz et al. , 2003; Mart et al. , 2007; Chen et al. , 2003; Bohmova et al. , 2003; Marcus et al. , 1993; Johansson and Nugues, 2007; Prokopidis et al. , 2005; Csendes et al. , 2005; Montemagni et al. , 2003; Oflazer et al. , 2003). ::: D07-1099_62:90
	
	Freund and Schapire (1999) originally proposed the averaged parameter method; it was shown to give substantial improvements in accuracy for tagging tasks in Collins (2002). ::: P04-1015_76:297
	
M06	There are some algorithms 1149 Figure 1: This is an example of dependency tree to determine these relations of each word to another words, for instance, the modified CKY algorithm (Eisner, 1996) is used to define these relations for a given sentence. ::: D07-1126_25:136
	
	Collins (2002b) gives convergence proofs for the methods; Collins (2002a) directly compares the boosting and perceptron approaches on a named entity task; and Collins and Duffy (2001, 2002) use a reranking approach with kernels, which allow representations of parse trees or labeled sequences in very-high-dimensional spaces. ::: J05-1003_572:603
	
M16,M03	First, we implement a chart-based dynamic programming parser for the 2nd-ordered MST model, and develop a training procedure based on the perceptron algorithm with averaged parameters (Collins, 2002). ::: P10-1002_169:187
	
M23	For handling non-projective relations, Nivre and Nilsson (2005) suggested applying a preprocessing step to a dependency parser, which consists in lifting non-projective arcs to their head repeatedly, until the tree becomes pseudo-projective. ::: W06-2922_42:94
	
E02	However, since most previous studies instead use the mean attachment score per word (Eisner, 1996; Collins et al. , 1999), we will give this measure as well. ::: W04-2407_112:153
	
M06,M33	By contrast, the nave CKY algorithm for this model is O(n5) which can be improved to O(n3) (Eisner, 1996).6 The higher complexity of our algorithm is due to two factors: first, we have to maintain both h and h in one state, because the current shift-reduce model can not draw features across different states (unlike CKY); and more importantly, we group states by step in order to achieve increment ality and linear runtime with beam search that is not (easily) possible with CKY or MST. ::: P10-1110_132:231
	
	Again, since neither MIRA nor BPM outperforms the other on all measures, we conclude that the results constitute a valiation of the results reported in (McDonald et al. , 2005a). ::: N06-1021_151:189
	
	For English, we used the Penn Treebank version 3.0 (Marcus et al. , 1993) and extracted dependency relations by applying the head-finding rules of (Yamada and Matsumoto, 2003). ::: N06-1021_36:189
	
	Then we will define the generative parsing model, based on the algorithm of (Nivre et al. , 2004), and propose an ISBN for this model. ::: W07-2218_43:279
	
V01	Then, 4.2 will describe the application of three types of tree transformations, while subsection 4.3 will examine the application of propagating syntactic features through a first-stage dependency tree, a process that can also be seen as an application of stacked learning, as tested in (Nivre and McDonald, 2008; Martins et al., 2008) 4.1 Feature engineering The original CoNLL-X format uses 10 different columns (see Figure 12), grouping the full set of morpho syntactic features in a single column. ::: W10-1404_74:207
	
	As for overall performance, both the exact and relaxed full model outperform the arcfactored model and the second order model of McDonald and Pereira (2006) with statistical significance (p < 0.01) according to Dan Bikels randomized method (http://www.cis.upenn.edu/dbikel/software.html). ::: P09-1039_207:228
	
V01,E02	We report experiments on twelve languages from the CoNLL-X shared task (Buchholz and Marsi, 2006).5 All experiments are evaluated using the labeled attachment score (LAS), using the default settings.6 Statistical significance is measured using Dan Bikels randomized parsing evaluation comparator with 10,000 iterations.7 The additional features used in the level 1 parser are enumerated in Table 1 and their various subsets are depicted in Table 2. ::: D08-1017_154:217
	
M06,M16	Dependency Parsing Algorithms For simplicity of implementation, we use a standard CKY parser in the experiments, although Eisners algorithm (Eisner, 1996) and the Spanning Tree algorithm (McDonald et al., 2005b) are also applicable. ::: P08-1061_126:148
	
M33	For this we use the parsing strategy for projective dependency parsing introduced in (Nivre et al. , 2004), which is similar to a standard shift-reduce algorithm for context-free grammars (Aho et al. , 1986). ::: W07-2218_97:279
	
M16	These structures are equivalent to non-projective dependency parses (McDonald et al. , 2005b), and more generally could be relevant to any task that involves learning a mapping from a graph to an underlying spanning tree. ::: D07-1015_11:443
	
	In graph-based parsing, dependency trees are scored by factoring the tree into its arcs, and parsing is performed by searching for the highest scoring tree (Eisner, 1996; McDonald et al., 2005b). ::: D08-1017_72:217
	
M33,P32	A probabilistic shift reduce LR-like model, such as the one used in our parser, is different in many ways from a lexicalized PCFG-like model (using markov a grammar), such as those used in the Collins (1999) and Charniak (2000) parsers. ::: P06-2089_140:169
	
	We will extend these experiments with the use of different parser features (Nivre and McDonald, 2008; Martins et al., 2008). ::: W10-1404_140:207
	
	In addition, they used the parsing model by Carreras (2007) that applied second-order features on both sibling and grandparent interactions. ::: D09-1060_238:253
	
E02	UAS means unlabeled attachment score (Buchholz and Marsi, 2006). ::: C08-1132_42:161
	
V01	Recently, dependency-based parsing has been applied to 13 different languages in the shared task of the 2006 Conference on Computational Natural Language Learning (CoNLL) (Buchholz and Marsi 2006). ::: J08-3003_18:443
	
	Consequently, recent work in dependency parsing has been restricted to applications of second order parsers, the most powerful of which (Carreras, 2007) requires O(n4) time and O(n3) space, while being limited to second-order parts. ::: P10-1001_13:237
	
M12	McDonald and Pereira (2006) and McDonald and Satta (2007) describe complexity results for nonprojective parsing, showing that parsing for a variety of models is NP-hard. ::: D10-1125_34:289
	
	The second baseline is represented by the Sentence Length (SL), starting from the assumption, demonstrated by McDonald and Nivre (2007), that long sentences are harder to analyse using statistical dependency parsers than short ones. ::: W11-0314_80:167
	
M12	As a general result, our experiments confirm previous studies on non-projective dependency parsing (Nivre and Nilsson, 2005; Hall and Novk, 2005; 512 McDonald and Pereira, 2006): The phenomenon of non-projectivity cannot be ignored without also ignoring a significant portion of real-world data (around 15a37for DDT, and 23a37for PDT). ::: P06-2066_127:252
	
	Carreras (2007) extends the first-order model to incorporate a sum over scores for pairs of adjacent arcs in the tree, yielding a second-order model. ::: D07-1096_274:410
	
R01,F05	The graph-based parser Following MSTParser (McDonald et al., 2005; McDonald and Pereira, 2006), we define the graph Variables: agenda the beam for state items item partial parse tree output a set of output items index,prev word indexes Input: x POS-tagged input sentence. ::: D08-1059_30:188
	
	Nivre and Nilsson (2005) uses tree rewriting which is the most common technique. ::: W09-1210_50:135
	
M26,M03	With a projectivity constraint and arc factorization, the parsing problem can be solved in cubic time by dynamic programming (Eisner, 1996), and with a weaker tree constraint (permitting nonprojective parses) and arc factorization, a quadratic-time algorithm exists (Chu and Liu, 1965; Edmonds, 1967), as shown by McDonald et al. ::: D08-1017_84:217
	
	Unlike the original definition in (Nivre et al. , 2004) the Right-Arcr decision does not shift wj to the stack. ::: W07-2218_112:279
	
V01	NLP community has recently seen a surge of interest in dependency parsing, with several CoNLL shared tasks focusing on it (Buchholz and Marsi, 2006; Nivre et al., 2007). ::: D11-1116_17:208
	
R01	As a result, graph-based parsers (including MSTParser) often limit the scope of their features to a small number of adjacent arcs (usually two) and/or resort to approximate inference (McDonald and Pereira, 2006). ::: C10-1094_57:185
	
M02	A bottom-up algorithm proposed by (Nivre and Scholz, 2004) is use for a deterministic dependency structure analysis. ::: I05-3003_25:271
	
M01,S02	The experimental parsing results presented in this paper have been obtained using MaltParser, a freely available system for data-driven dependency parsing with state-of-the-art accuracy for most languages in previous evaluations (Buchholz and Marsi, 2006; Nivre et al., 2007a; Nivre et al., 2007b). ::: C08-1081_22:137
	
	We write act(i) to denote the feature representation extracted for action act at location i. The model is trained using a variant of the structured perceptron (Collins, 2002), similar to the algorithm of (Shen et al., 2007; Shen and Joshi, 2008). ::: N10-1115_57:202
	
	A is a replication of (Nivre and McDonald, 2008), except for the modifications described in footnote 4. ::: D08-1017_144:217
	
P32,R01,R02,M24	The Berkeley parser (Petrov et al., 2006) is a latent-variable PCFG parser, MSTParser (McDonald et al., 2006) is a graph-based dependency parser, and MaltParser (Nivre et al., 2006) is a transition-based dependency parser. ::: C10-2013_51:211
	
R02,R01	The parsers used in the experiments are MaltParser (Nivre et al. , 2004) and MSTParser (McDonald et al. , 2005). ::: P07-1122_96:184
	
M01,M12	There has been extensive work on data-driven dependency parsingfor both projective parsing(Eisner, 1996; Paskin, 2001; Yamadaand Matsumoto, 2003; Nivre and Scholz, 2004; McDonaldet al., 2005a) and non-projective parsing systems(Nivre and Nilsson,2005;Halland Novak, 2005;McDonald et al., 2005b). ::: D08-1017_47:217
	
M03	Instead of performing exact inference by dynamic programming, we incorporated the linear model and feature templates from McDonald and Pereira (2006) into our beam-search framework, while adding new global features. ::: D08-1059_163:188
	
	The three baselines are the second order model of McDonald and Pereira (2006) and the hybrid models of Nivre and McDonald (2008) and Martins et al. ::: P09-1039_201:228
	
V01	Although the contributions of this paper are mainly theoretical, we also present an empirical evaluation of the 2planar parser, showing that it outperforms the projective parser on four data sets from the CoNLL-X shared task (Buchholz and Marsi, 2006). ::: P10-1151_17:201
	
	It is easy to see that the main difference between the PA algorithms and the Perceptron algorithm (PC) (Collins, 2002) as well as the MIRA algorithm (McDonald et al. , 2005a) is in line 9. ::: D07-1126_58:136
	
M09,M37	Kudo and Matsumoto (2002) report a two week learning time on a Japanese corpus of about 8000 sentences with SVM. ::: W06-2922_9:94
	
	McDonald and Pereira (2006) developed a technique to rearrange edges in the tree in a postprocessing step after the projective parsing has taken place. ::: W09-1210_54:135
	
	We employ the second-order projective graphbased parsing model of Carreras (2007), which is an extension of the projective parsing algorithm of Eisner (1996). ::: C10-2015_50:184
	
	A major benefit of this choice is computational efficiency: an exhaustive search over all projective structures can be done in cubic, greedy parsing in linear time (Eisner, 1996; Nivre, 2003). ::: E09-1055_11:234
	
	All of the convergence and generalization results in Collins (2002) depend on notions of separability rather than the size of GEN. Two questions come to mind. ::: P04-1015_69:297
	
M33,M16	McDonald and Nivre (2007) describe how the systems behavioral differences are due to the different parsing algorithms implemented by the Shift Reduce and the MST parsing models. ::: W11-0314_71:167
	
V01	We participated in the multilingual track of the CoNLL 2007 shared task (Nivre et al. , 2007), and evaluated the system on data sets of 10 languages (Hajic et al. , 2004; Aduriz et al. , 2003; Mart et al. , 2007; Chen et al. , 2003; Bohmova et al. , 2003; Marcus et al. , 1993; Johansson and Nugues, 2007; Prokopidis et al. , 2005; Csendes et al. , 2005; Montemagni et al. , 2003; Oflazer et al. , 2003). ::: D07-1100_12:206
	
	The input for both systems is projectivized using the head+path schema (Nivre and Nilsson, 2005). ::: W09-1104_142:228
	
V01,R01,R02,E03	For direct comparison with the approach by Nivre and McDonald (2008), we present the results on the CoNLL-X corpora (Table 3): MST and MSTMalt are the results achieved by the MST parser and the MST parser using hints from Maltparser, Malt and 0.4 0.5 0.6 0.7 0.8 0.9 1 0 5 10 15 20 25 30 35 F-Measure Dependency Length Left-to-Right_DeSR Revision_DeSR Figure 1: English. ::: N09-2066_32:73
	
M16,M37	Looking rst at learning times, it is obvious that learning time depends primarily on the number of training instances, which is why we can observe a difference of several orders of magnitude in learning time between the biggest training set (Czech) and the smallest training set (Slovene) 14 This is shown by Nivre and Scholz (2004) in comparison to the iterative, arc-standard algorithm of Yamada and Matsumoto (2003) and by McDonald and Nivre (2007) in comparison to the spanning tree algorithm of McDonald, Lerman, and Pereira (2006). ::: J08-4003_523:595
	
	We created a dependency training corpus based on the Penn Treebank (Marcus et al. , 1993), or more specifically on the HPSG Treebank generated from the Penn Treebank (see section 2.2). ::: P07-1079_54:260
	
	Sagae and Lavie (2006) demonstrated that a simple combination scheme of the outputs of different parsers can obtain substantially improved accuracies. ::: W10-2927_13:238
	
M33,M16	The table also reports the scores obtained on the same data set by by the shift reduce parsers of Nivre and Scholzs (2004) and Yamada andMatsumoto(2003), andMcDonaldandPereiras second-order maximum spanning tree parser (McDonald & Pereira, 2006). ::: N07-1049_151:180
	
R01,R02	As expected, Malt and MST have very similar accuracy for short sentences but Malt degrades more rapidly with increasing sentence length because of error propagation (McDonald and Nivre, 2007). ::: P08-1108_117:172
	
	Although supervised learning methods bring stateof-the-art outcome for dependency parser inferring (McDonald et al., 2005; Hall et al., 2007), a large enough data set is often required for specific parsing accuracy according to this type of methods. ::: P09-1007_7:206
	
M37	The approach was extended to labeled dependency parsing by Nivre, Hall, and Nilsson (2004) (for Swedish) and Nivre and Scholz (2004) (for English), using a different parsing algorithm rst presented in Nivre (2003). ::: J08-4003_23:595
	
M23	Bengoetxea and Gojenola (2010) discuss non-projective dependencies in Basque and show that the pseudo-projective transformation of (Nivre and Nilsson, 2005) improves accuracy for dependency parsing of Basque. ::: W10-1401_208:252
	
	The best result on this dataset to date (92.7% UAS) is that of Sagae and Lavie (Sagae & Lavie, 2006) who use a parser which combines the predictions of several pre-existing parsers, including McDonalds and Nivres parsers. ::: W07-2217_217:259
	
	As a final note, following Collins (2002), we used the averaged parameters from the training algorithm in decoding test examples in our experiments. ::: P04-1015_73:297
	
	To overcome this, Eisner (1996a) proposed a back-off strategy which reduces the conditioning of a model. ::: D11-1137_66:241
	
M16	The dependency probability can then be defined as: C(i,j) = exp(w f(i,j,+))summation text r exp(w f(i,j,r)) = exp( summation text k wk fk(i,j,+))summation text r exp( summation text k wk fk(i,j,r)) (5) 2.2 Features for Classification The feature templates for the classifier are similar to those of 1st-ordered MST model (McDonald et al., 2005a). ::: P10-1002_50:187
	
V01,R02,F01	For comparison, the table also includes results for the two best performing systems in the original CoNLL-X shared task, Malt-06 (Nivre et al., 2006) and MST-06 (McDonald et al., 2006), as well as the integrated system MSTMalt, which is a graph-based parser guided by the predictions of a transition-based parser and currently has the best reported results on the CoNLL-X data sets (Nivre and McDonald, 2008). ::: P09-1040_125:154
	
	All our experimental settings match previous work (Yamada and Matsumoto, 2003; McDonald et al., 2005; Koo et al., 2008). ::: P11-2125_59:98
	
	Many of these are even in standardized formats (Buchholz and Marsi, 2006; Nivre et al., 2007). ::: D11-1006_190:303
	
M20	Moreover, the deterministic dependency parser of Yamada and Matsumoto (2003), when trained on the Penn Treebank, gives a dependency accuracy that is almost as good as that of Collins (1997) and Charniak (2000). ::: C04-1010_12:293
	
M26	We can reduce the time complexity to O(n3q3) by strictly adopting the DP structures in the parsing algorithm of Eisner (1996). ::: D11-1109_125:271
	
V01,M37	The Danish and Dutch datasets were prepared for the CoNLL 2006 shared task (Buchholz and Marsi, 2006); Arabic and Czech are from the 2007 shared task. ::: D07-1014_103:189
	
	In supervised dependency parsing, second order edge features provide improvements (McDonald and Pereira, 2006; Riedel and Clarke, 2006); moreover, the feature-based approach is not limited to dependency parsing. ::: D07-1070_270:458
	
	To combine the outputs of each parser we used the method of Sagae and Lavie (2006). ::: D07-1096_372:410
	
	Tournament All 91.96 57.44 SR algorithm (Sassano, 2004) All 91.48 55.67 CC algorithm (Kudo and Matsumoto, 2002) All 91.47 55.65 Combination CC and Relative preference Kudos (2005) 91.66 56.30 Relative preference (Kudo and Matsumoto, 2005) Kudos (2005) 91.37 56.00 CC algorithm (Kudo and Matsumoto, 2002) Kudos (2005) 91.23 55.59 Table 4: Dependency and sentence accuracy [%] using 24,263 sentences as training data with all features: comparison with Kudo(2005)s experiments. ::: C08-1046_201:234
	
M16,M24,R02	The two dominating approaches have been graph-based parsing, e.g. MST-parsing (McDonald et al., 2005b) and transition-based parsing, e.g. the MaltParser (Nivre et al., 2006a). ::: P10-3010_5:143
	
	Dependency parsing can be used to provide a bare bones syntactic structure that approximates semantics,and it hast he additional advantage of admitting fast parsing algorithms (Eisner, 1996; McDonald et al. , 2005b) with a negligible grammar constant in many cases. ::: D07-1014_9:189
	
	Factorizations like that of Carreras (2007) obtain grandchild parts by augmenting spans with the indices of modifiers, leading to limitations on 4The reason for the restriction is that in Model 2, grand siblings can only be derived via Figure 7(b), which does not recursively copy the grandparent index for reuse in smaller g-spans as Model 1 does in Figure 6(b). ::: P10-1001_128:237
	
	Though syntactic parsers for English are reported to have accuracies over 90% on the Wall Street Journal (WSJ) section of the Penn Treebank (PTB) (McDonald et al., 2005; Sagae and Lavie, 2006; Huang, 2008; Carreras et al., 2008), broad-coverage parsing is still far from being a solved problem. ::: C10-1094_3:185
	
	Ensemble systems: The error analysis presented in this paper could be used as inspiration for more refined weighting schemes for ensemble systems of the kind proposed by Sagae and Lavie (2006), making the weights depend on a range of linguistic and graph-based factors. ::: D07-1013_201:290
	
	The second extension is to apply the approach to second-order parsing models, more specifically the model of (Carreras, 2007), using a two-stage semi-supervised learning approach. ::: D09-1058_20:216
	
	For English, we use the automatically-assigned POS tags produced by an implementation of the POS tagger of Collins (2002). ::: P10-1002_125:187
	
	English we used the Wall Street Journal section of the Penn Treebank (Marcus et al. , 1993). ::: D07-1096_116:410
	
M12	Finally, Table 2 compares loopy BP to a previously proposed hill-climbing method for approximate inference in non-projective parsing McDonald and Pereira (2006). ::: D08-1016_333:363
	
	Tractability is usually ensured by strong factorization assumptions, like the one underlying the arc-factored model (Eisner, 1996; McDonald et al., 2005), which forbids any feature that depends on two or more arcs. ::: P09-1039_81:228
	
M01,P32	Some of the more popular and more accurate of these approaches to data-driven parsing (Charniak, 2000; Collins, 1997; Klein and Manning, 2002) have been based on generative models that are closely related to probabilistic context free grammars. ::: P06-2089_9:169
	
	It is worth noting that the parsing units in this treebank are sometimes smaller than conventional sentence units, which partly explains the low average number of tokens per sentence (Buchholz and Marsi, 2006). ::: D07-1096_110:410
	
M01,M12	Therehas been extensive work on data-driven dependency parsingfor both projective parsing(Eisner, 1996; Paskin,2001; Yamadaand Matsumoto, 2003; Nivre and Scholz,2004; McDonaldet al., 2005a)and non-projective parsing systems(Nivre andNilsson,2005;HallandNovak, 2005;McDonald et al., 2005b). ::: P09-1039_51:228
	
M24	This is done by performing higher order parsing, which is shown to improve parsing accuracy but also increase parsing complexity (Carreras, 2007; Koo and Collins, 2010).2 Transition based parsing is attractive because it can use parse information without increasing complexity (Nivre, 2006). ::: P11-2121_65:136
	
M05,M22	Subsequent research began to focus more on conditional models of parse structure given the input sentence, which allowed discriminative training techniques such as maximum conditional likelihood (i.e. maximum entropy ) to be applied (Ratnaparkhi, 1999; Charniak, 2000). ::: N07-3002_10:74
	
E02,M37	Indirect support for this assumption can be gained from previous experiments with Swedish data, where almost the same accuracy (85% unlabeled attachment score) has been achieved with a treebank which is much smaller but which contains proper dependency annotation (Nivre et al. , 2004). ::: C04-1010_126:293
	
V01,E02,S01,M37	We report experiments on seven languages, six (Danish, Dutch, Portuguese, Slovene, Swedish and Turkish) from the CoNLL-X shared task (Buchholz and Marsi, 2006), and one (English) from the CoNLL-2008 shared task (Surdeanu et al., 2008).8 All experiments are evaluated using the unlabeled attachment score (UAS), using the default settings.9 We used the same arc-factored featuresasMcDonaldetal.(2005)(include dint he MSTParser toolkit10); for the higher-order models described in 3.33.5, we employed simple higher order features that look at the word, part-of-speech tag, and (if available) morphological information of the words being correlated through the indicator variables. ::: P09-1039_171:228
	
M24,M33	Transition based parsers model the sequence of decisions of a shift-reduce parser, given previous decisions and current state, and parsing is performed by greedily choosing the highest scoring transition out of each successive parsing state or by searching for the best sequence of transitions (Ratnaparkhi et al., 1994; Yamada and Matsumoto, 2003; Nivre et al., 2004; Sagae and Lavie, 2005; Hall et al., 2006). ::: D08-1017_73:217
	
	In order to get a better understanding of these matters, we replicate parts of the error analysis presented by McDonald and Nivre (2007), where parsing errors are related to different structural properties of sentences and their dependency graphs. ::: P08-1108_114:172
	
	Much of this work has been fueled by the availability of large corpora annotated with syntactic structures, especially the Penn Treebank (Marcus et al. , 1993). ::: N06-2033_5:93
	
V01,M37	Dependency treebanks are becoming available in many languages, and several approaches to dependency parsing on multiple languages have been evaluated in the CoNLL 2006 and 2007 shared tasks (Buchholz & Marsi, 2006; Nivre et al. , 2007). ::: W07-2217_11:259
	
	This corresponds to the Baseline condition in Nivre and Nilsson (2005). ::: J08-4003_395:595
	
	McDonald and Pereira (2006) define this as a second-order Markov assumption. ::: W07-2216_228:259
	
	Our results yield relative error reductions of roughly 27% (English) and 20% (Czech) over McDonald and Pereira (2006)s second-order supervised dependency parsers, and roughly 9% (English) and 7% (Czech) over the previous best results provided by Koo et. ::: D09-1058_204:216
	
	We use a global linear model to score candidate items, trained discriminatively with the averaged perceptron (Collins, 2002). ::: P11-1069_91:201
	
M24,V01	R O O T I a t e t h e f i s h w i t h a f o r k . Figure 1: Example for dependency structure 2.1 Parsing approach For dependency parsing, there are two main types of parsing models (Nivre and McDonald, 2008): graph-based model and transition-based model, which achieved state-of-the-art accuracy for a wide range of languages as shown in recent CoNLL shared tasks (Buchholz et al., 2006; Nivre et al., 2007). ::: D09-1060_40:253
	
	The algorithms of Kudo and Matsumoto (2002), Yamada and Matsumoto (2003), and Nivre (2003, 2006b) all belong to this family. ::: J08-4003_35:595
	
M01,M16,M24	The two main approaches to data-driven dependency parsing are transition based dependency parsing (Nivre, 2003; Yamada and Matsumoto, 2003; Titov and Henderson, 2007) and maximum spanning tree based dependency parsing (Eisner, 1996; Eisner, 2000; McDonald and Pereira, 2006). ::: C10-2129_24:191
	
S02,M05	As shown by McDonald and Nivre (2007), the Single Malt parser tends to suffer from two problems: error propagation due to the deterministic parsing strategy, typically affecting long dependencies more than short ones, and low precision on dependencies originating in the artificial root node due to fragmented parses.9 The question is which of these problems is alleviated by the multiple views given by the component parsers in the Blended system. ::: D07-1097_108:119
	
M23	We will focus on tree transformations that combine preprocessing with post-processing, and where the parser is treated as a black box, such as the pseudo-projective parsing technique proposed by Nivre and Nilsson (2005) and the tree transformations investigated in Nilsson et al. ::: P07-1122_13:184
	
	Right-to-left parsing has been used as part of ensemble-based parsers (Sagae & Lavie, 2006; Hall et al., 2007). ::: N09-2066_6:73
	
V01,M37	We test our parsing models on the CONLL-2007 (Hajic et al. , 2004; Aduriz et al. , 2003; Mart et al. , 2007; Chen et al. , 2003; Bohmova et al. , 2003; Marcus et al. , 1993; Johansson and Nugues, 2007; Prokopidis et al. , 2005; Csendes et al. , 2005; Montemagni et al. , 2003; Oflazer et al. , 2003) data set on various languages including Arabic, Basque, Catalan, Chinese, English, Italian, Hungarian, and Turkish. ::: D07-1126_82:136
	
V01	Dependency Parsing, the task of inferring a dependency structure over an input sentence, has gained a lot of research attention in the last couple of years, due in part to to the two CoNLL shared tasks (Nivre et al., 2007; Buchholz and Marsi, 2006) in which various dependency parsing algorithms were compared on various data sets. ::: W10-2927_8:238
	
	In fact, our approach can also be applied to other parsers, such as (Yamada and Matsumoto, 2003)s parser, (McDonald et al., 2006)s parser, and so on. ::: I08-1012_67:180
	
	We use the discriminative perceptron learning algorithm (Collins, 2002; McDonald et al., 2005) to train the values of vectorw. ::: D08-1059_34:188
	
M16,M09,M03	A look at the performance sheet in the contest shows that two systems with quite different approaches (one using deterministic parsing with SVM and the other using MIRA with nondeterministic and dynamic programming based MST approach ) performed with good results (McDonald et al. , 2006; Nivre et al. , 2006). ::: D07-1124_9:98
	
M33	We present a statistical parser that is based on a shift-reduce algorithm, like the parsers of Sagae and Lavie (2005) and Nivre and Scholz (2004), but performs a best-first search instead of pursuing a single analysis path in deterministic fashion. ::: P06-2089_15:169
	
E02	Although the best published results for the Collins parser is 80% UAS (Collins, 1999), this parser reaches 82% when trained on the entire training data set, and an adapted version of Charniaks parser (Charniak, 2000) performs at 84% (Jan Hajic, pers. ::: P05-1013_130:139
	
D02	Corresponding manipulations in the form of tree transformations for dependency-based parsers have recently gained more interest (Nivre and Nilsson, 2005; Hall and Novak, 2005; McDonald and Pereira, 2006; Nilsson et al. , 2006) but are still less studied, partly because constituency-based parsing has dominated the field for a long time, and partly because dependency structures have less structure to manipulate than constituent structures. ::: P07-1122_8:184
	
	The results on Chinese are obtained on two different data sets, Chinese Treebank 4.0 and Chinese Treebank 5.0 as noted.3 Table 1 shows that the results I am able to achieve on English are competitive with the state of the art, but are still behind the best results of (McDonald and Pereira, 2006). ::: N07-3002_55:74
	
	An advantage of graph-based methods is that tractable inference enables the use of standard structured learning techniques that globally set parameters to maximize parsing performance on the training set (McDonald et al., 2005a). ::: P08-1108_35:172
	
R01	In our experiments, we implemented our systems on the MSTParser6 and extended with the parent-child-grandchild structures (McDonald and Pereira, 2006; Carreras, 2007). ::: C10-2015_142:184
	
F04	The first stage is based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages. ::: W06-2932_3:130
	
M05, E02	Deterministic parsing algorithms for building dependency graphs (Kudo and Matsumoto 2002; Nivre 2003; Yamada and Matsumoto 2003) Table3 Unlabeled attachment scores for different choices for morphological features. ::: J08-3003_214:443
	
R05,V01,M37	The Turkish Treebank has recently been parsed by 17 research groups in the CoNLLX shared task on multilingual dependency parsing (Buchholz and Marsi 2006),where it was seen as the most difcult language by the organizers and most of the groups. ::: J08-3003_410:443
	
M23,M12	In order to make the comparison more fair, we therefore also evaluate pseudo-projective versions of the latter algorithms, making use of graph transformations in pre-and post-processing to recover nonprojective dependency arcs, following Nivre and Nilsson (2005). ::: J08-4003_390:595
	
P04	Consider, for instance, algorithms for converting the phrase-structure trees in the Penn Treebank (Marcus et al., 1993) into dependency structures. ::: D11-1036_29:232
	
F04	In the feature selection, we follow a bit more McDonald and Pereira (2006) since we have in addition the lemmas, morphologic features and the distance between the word forms. ::: W09-1210_74:135
	
M12	Such models are commonly referred to as edge-factored since their parameters factor relative to individual edges of the graph (Paskin, 2001; McDonald et al., 2005a).Edge-factored model shave many computational benefits,most notably that inference for nonprojective dependency graphs can be achieved in polynomial time(McDonaldet al., 2005b).Theprimary problem in treating each dependency as independent is that it is not a realistic assumption. ::: D08-1017_20:217
	
	Interestingly, though the two systems have similar accuracies overall, there is a clear distinction between the kinds of errors each system makes, which we argue is consistent with observations by McDonald and Nivre (2007). ::: C10-1094_24:185
	
S01,S02,E03	Again, we find the clearest patterns in the graphs for precision, where Malt has very low precision near the root but improves with increasing depth, while MST shows the opposite trend (McDonald and Nivre, 2007). ::: P08-1108_130:172
	
F05	The baseline features capture information about the lexical items and their part of speech (POS) tags (as defined in (McDonald et al., 2005)). ::: P11-2125_25:98
	
R01,F02	McDonald and Nivre (2007) showed that the MSTParser and MaltParser produce different errors. ::: D08-1059_98:188
	
M37	The rst strictly incremental parser of this kind was described in Nivre (2003) and used for classier-based parsing of Swedish by Nivre, Hall, and Nilsson (2004) and English by Nivre and Scholz (2004). ::: J08-4003_566:595
	
	For the second-order parsing experiments, we used the Carreras (2007) parser. ::: P08-1068_153:218
	
	We can then state the following theorem (see (Collins, 2002) for a proof): Theorem 1 For any training sequence (xi;yi) that is separable with margin, for any value of T, then for the perceptron algorithm in figure 1 Ne R 2 2 where R is a constant such that 8i;8z 2 GEN(xi) jj (xi;yi) (xi;z)jj R. This theorem implies that if there is a parameter vector U which makes zero errors on the training set, then after a finite number of iterations the training algorithm will converge to parameter values with zero training error. ::: P04-1015_65:297
	
	Unfortunately, it is not possible to directly compare the parsers accuracy with most popular constituent parsers such as the Charniak (2000) and Berkeley (Petrov et al., 2006; Petrov and Klein, 2007) parsers9 both because they do not produce functional tags for subjects, direct objects, etc., which are required for the final script of the constituent-to-dependency conversion routine, and because they determine part-of-speech tags in conjunction with the parsing. ::: D11-1116_133:208
	
M33,P32,M06	As a concrete example, Figure 4 simulates an edge-factored model (Eisner, 1996; McDonald et al., 2005a) using shift-reduce with dynamic programming, which is similar to bilexical PCFG parsing using CKY (Eisner and Satta, 1999). ::: P10-1110_124:231
	
	The parsing algorithms used in Carreras (2007) independently find the left and right dependents of a word and then combine them later in a bottomup style based on Eisner (1996). ::: C10-2015_51:184
	
	As learning algorithm, we use Perceptron tailored for structured scenarios, proposed by Collins (2002). ::: W06-2925_33:112
	
R01	Table 1 shows the feature templates from the MSTParser (McDonald and Pereira, 2006), which are defined in terms of the context of a word, its parent and its sibling. ::: D08-1059_50:188
	
M37	Classier-based dependency parsing was pioneered by Kudo and Matsumoto (2002) for unlabeled dependency parsing of Japanese with head-nal dependencies only. ::: J08-4003_21:595
	
	Ensemble-based methods have attracted a lot of attention in dependency parsing recently (Sagae and Lavie, 2006; Hall et al., 2007; Nivre and McDonald, 2008; Martins et al., 2008; Fishel and Nivre, 2009; Surdeanu and Manning, 2010). ::: C10-1120_12:233
	
	To learn arc scores, these models use large-margin structured learning algorithms (McDonald et al. , 2005a), which optimize the parameters of the model to maximize the score margin between the correct dependency graph and all incorrect dependency graphs for every sentence in a training set. ::: D07-1013_43:290
	
	Conversion to Dependencies 3.2.1 Syntactic Dependencies There exists no large-scale dependency treebank for English, and we thus had to construct a dependency-annotated corpus automatically from the Penn Treebank (Marcus et al., 1993). ::: W08-2121_150:401
	
M03	Follow the edge based factorization method (Eisner, 1996), we factorize the score of a dependency tree s(x,y) into its dependency edges, and design a dynamic programming algorithm to search for the candidate parse with maximum score. ::: P10-1002_42:187
	
	Previous work (McDonald and Pereira, 2006; Carreras, 2007) has shown that second-order parsing models, which include information from sibling or grandparent relationships between dependencies, can give significant improvements in accuracy over first-order parsing models. ::: D09-1058_96:216
	
M26	This algorithm has a runtime of O(n3) and has been employed successfully in both generative and discriminative parsing models (Eisner, 1996; McDonald et al. , 2005). ::: H05-1066_104:223
	
M05	Since deterministic dependency parsing has previously been shown to be competitive in terms of parsing accuracy (Yamada and Matsumoto, 2003; Nivre et al. , 2004), we believe that this is a promising approach for situations that require parsing to be robust, efficient and (almost) incremental. ::: W04-0308_142:145
	
M01,M34,M35	The most popular strategy for capturing nonprojective structures in data-driven dependency parsing is to apply some kind of post-processing to the output of a strictly projective dependency parser, as in pseudo-projective parsing (Nivre and Nilsson, 2005), corrective modeling (Hall and Novak, 2005), or approximate non-projective parsing (McDonald and Pereira, 2006). ::: N07-1050_8:146
	
M05	Deterministic dependency parsing (Nivre et al. , 2004; Yamada and Matsumoto, 2003) can apply global constraints by conditioning attachment decisions on the intermediate parse built. ::: W06-1616_58:238
	
	We will briefly review the perceptron algorithm, and its convergence properties see Collins (2002) for a full description. ::: P04-1015_55:297
	
R01,V01	We used the MSTParser (McDonald et al., 2006), which achieved top results in the CoNLL 2006 (CoNLL-X) shared task, as a base dependency parser. ::: I08-2097_66:160
	
	The table also confirms the commonly known fact (Yamada and Matsumoto, 2003; McDonald et al., 2005) that constituency parsers are more accurate at producing dependencies than dependency parsers (at least when the dependencies were produced by a deterministic transformation of a constituency treebank, as is the case here). ::: D10-1069_70:206
	
M23	This is in contrast to other non-projective methods, such as that of Nivre and Nilsson (2005), who implement non-projectivity in a pseudo-projective parser with edge transformations. ::: H05-1066_115:223
	
F02,F05	Our baseline features (baseline) are very similar to those described in (McDonald et al., 2005a; Koo et al., 2008): these features track word and POS bigrams, contextual features surrounding dependencies, distance features, and so on. ::: D09-1058_132:216
	
M23	Although the parser only derives projective graphs, the fact that these graphs are labeled allows non-projective dependencies to be captured using the pseudo-projective approach of Nivre and Nilsson (2005) (section 3.4). ::: C08-1081_69:137
	
V01	Both models have been used to achieve state-of-the-art accuracy for a wide range of languages, as shown in the CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007), but McDonald and Nivre (2007) showed that a detailed error analysis reveals important differences in the distribution of errors associated with the two models. ::: P08-1108_23:172
	
	The model is enhanced for non-projective languages by Nivre and Nilsson (2005). ::: C08-1046_17:234
	
	The score of a tree for second order parsing is now s(x,y) = summation display (i,k,j)y s(i,k,j) where k and j are adjacent, same-side children of i in the tree y. The second-order model allows us to condition onthe mostrecent parsing decision, thatis, the last dependent picked up by a particular word, which is analogous to the the Markov conditioning of in the Charniak parser (Charniak, 2000). ::: E06-1011_48:205
	
	We adapted this system to first perform unlabeled parsing, then label the arcs using a log-linear classifier with access to the full unlabeled parse (McDonald et al., 2005a; McDonald et al., 2005b; McDonald and Pereira, 2006). ::: D08-1017_150:217
	
	These are passed into a fast algorithm for maximum spanning tree (Tarjan, 1977) or maximum projective spanning tree (Eisner, 1996). ::: D08-1016_243:363
	
	In our experiments we use the 1-best MIRA algorithm (McDonald and Pereira, 2006)1 as a 1We used a slightly modified version of 1-best MIRA, whose difference can be found in the third line in Eq. ::: D09-1058_107:216
	
	An online learning algorithm considers a single training instance for each update to the weight vector w. We use the common method of setting the final weight vector as the average of the weight vectors after each iteration (Collins, 2002), which has been shown to alleviate overfit ting. ::: E06-1011_129:205
	
	Figure 1 gives an example dependency graph for the sentence Mr. Tomash will remain as a director emeritus, which has been extracted from the Penn Treebank (Marcus et al. , 1993). ::: W07-2216_7:259
	
	Most of the graph-based parsers were trained using an online inference-based method such as passive aggressive learning (Nguyen et al. , 2007; Schiehlen and Spranger, 2007), averaged perceptron (Carreras, 2007), or MIRA (Shimizu and Nakagawa, 2007), while some systems instead used methods based on maximum conditional likelihood (Nakagawa, 2007; Hall et al. , 2007b). ::: D07-1096_285:410
	
M26	Givenann-wordinput sentence, theparser begins by scoring each of the O(n2) possible edges, and then seeks the highest-scoring legal dependency tree formed by any n1 of these edges, using an O(n3) dynamic programming algorithm (Eisner, 1996) for projective trees. ::: D07-1070_28:458
	
M26,M13	The projectivity constraint also leads to favourable parsing complexities: chart-based parsing of projective dependency grammars can be done in cubic time (Eisner, 1996); hard-wiring projectivity into a deterministic dependency parser leads to linear-time parsing in the worst case (Nivre, 2003). ::: P06-2066_42:252
	
V01,M37	Weal so report an evaluation on all thirteen languages of the CoNLL-X shared task(Buchholz&Marsi, 2006), for comparison with the results by Nivre and McDonald (2008). ::: N09-2066_26:73
	
V01	We have used the 10 smallest data sets from CoNNL-X (Buchholz and Marsi, 2006) in our experiments. ::: P10-3010_66:143
	
	Syntactic representation The dependency treebank we use is a conversion of the English WSJ treebank (Marcus et al., 1993) to dependency structure using the procedure described in (Johansson and Nugues, 2007). ::: W10-2927_109:238
	
	Local models without global constraints are therefore mislead into deadend interpretations from which they cannot recover (McDonald and Nivre, 2007). ::: D10-1069_112:206
	
M03	For PTREE (projective), it is the inside-outside version of a dynamic programming algorithm (Eisner, 1996). ::: D08-1016_213:363
	
R01,R02	A more recent approach (Nivre and McDonald, 2008) combined MSTParser and MaltParser by using the output of one parser for features in the other. ::: D08-1059_177:188
	
P29	Table 2 shows the results for English projective dependency trees extracted from the Penn Treebank (Marcus et al. , 1993) using the rules of Yamada and Matsumoto (2003). ::: H05-1066_208:223
	
R01	We test on two stateof-the art parsers: MST We modified the publicly-available MST parser (McDonald et al., 2005)6 to employ our filters before carrying out feature extraction. ::: C10-1007_205:238
	
V01,M37	(Kromann, 2003), the Alpino Treebank of Dutch (van der Beek et al. , 2002), the TIGER Treebank of German (Brants et al. , 2002), the Floresta Sintactica of Portuguese (Afonso et al. , 2002), and the Slovene Dependency Treebank (Dzeroski et al. , 2006).4 The data sets used are the training sets from the CoNLLX Shared Task on multilingual dependency parsing (Buchholz and Marsi, 2006), with 20% of the data reserved for testing using a pseudo-random split. ::: N07-1050_89:146
	
	This includes work that integrates second and even third order features (McDonald et al., 2006; Carreras, 2007; Koo and Collins, 2010). ::: D11-1116_182:208
	
M24	Both the graph-based (McDonald et al., 2005a; McDonald and Pereira, 2006; Carreras et al., 2006) and the transition-based (Yamada and Matsumoto, 2003; Nivre et al., 2006) parsing algorithms are related to our word-pair classification model. ::: P10-1002_91:187
	
	This type of domain adaptation is reminiscent of self-training (McClosky et al., 2006a; Huang and Harper, 2009) and co-training (Blum and Mitchell, 1998; Sagae and Lavie, 2006), except that the goal here is not to further improve the performance of the very best model. ::: D10-1069_26:206
	
M16	The observed time complexity of our DP parser is in fact linear compared to the super linear complexity of Charniak, MST (McDonald et al., 2005b), and Berkeley parsers. ::: P10-1110_197:231
	
V01	We applied the above two marginal-based training algorithms to six languages with varying degrees of non-projectivity, using datasets obtained from the CoNLL-X shared task (Buchholz and Marsi, 2006). ::: D07-1015_23:443
	
	Averaging has been shown to help reduce overfit ting (McDonald et al. , 2005a; Collins, 2002). ::: D07-1126_57:136
	
M01,M24	Combinations of graph-based and transition-based models for data-driven dependency parsing have previously been explored by Sagae and Lavie (2006), who report improvements of up to 1.7 percentage points over the best single parser when combining three transition-based models and one graph-based model for unlabeled dependency parsing, evaluated on data from the Penn Treebank. ::: P08-1108_151:172
	
E02	English UAS Complete Y&M2003 90.3 38.4 CO2006 90.8 37.6 Hall2006 89.4 36.4 Wang2007 89.2 34.4 Z&C2008 92.1 45.4 KOO08-dep1c 92.23 KOO08-dep2c 93.16 Carreras2008 93.5 Ord1 90.95 37.45 Ord1s 91.76 40.68 Ord1c 91.88 40.71 Ord1i 91.68 41.43 Ord1sc 92.20 42.98 Ord1sci 92.60 44.28 Ord2 91.71 42.88 Ord2s 92.51 46.19 Ord2c 92.40 44.08 Ord2i 92.12 44.37 Ord2sc 92.70 46.56 Ord2sci 93.16 47.15 Table 2: Dependency parsing results for English, for our parsers and previous work To demonstrate that our approach and other work are complementary, we thus implemented a system using all the techniques we had at hand that used subtreeand cluster-based features and applied the integrating method of Nivre and McDonald (2008). ::: D09-1060_162:253
	
	Parsing method This chapter presents a basic parsing algorithm proposed by (Nivre and Scholz, 2004). ::: I05-3003_41:271
	
	This feature template is almost the same as the one used by McDonald and Pereira (2006). ::: D07-1100_56:206
	
	See for example (Lombardi, 1996; Eisner, 1996), who also discuss Early-style parsers for projective dependency grammars. ::: P98-1106_93:180
	
	The fact that our model defines a probability model over parse trees, unlike the previous state-ofthe-art methods (Nivre et al. , 2006; McDonald et al. , 2006), makes it easier to use this model in applications which require probability estimates, e.g. in language processing pipelines. ::: W07-2218_38:279
	
M23,M01	Pseudo-projective parsing was proposed by Nivre and Nilsson (2005) as a way of dealing with nonprojective structures in a projective data-driven parser. ::: C08-1081_81:137
	
M24,M16	The two main approaches to dependency parsing are transition based dependency parsing (Nivre, 2003; Yamada and Matsumoto., 2003; Titov and Henderson, 2007) and maximum spanning tree based dependency parsing (Eisner, 1996; Eisner, 2000; McDonald and Pereira, 2006). ::: C10-1011_28:243
	
	That algorithm, in turn, is similar to the dependency parsing algorithm of Nivre and Scholz (2004), but it builds a constituent tree and a dependency tree simultaneously. ::: P06-2089_21:169
	
M22	In this work we use the averaged perceptron algorithm (Collins, 2002) since it is an online algorithm much simpler and orders of magnitude faster than Boosting and MaxEnt methods. ::: P08-1067_50:181
	
M37	Recent work has successfully developed dependency parsing models for many languages using supervised learning algorithms (Buchholz and Marsi, 2006; Nivre et al., 2007). ::: D09-1058_8:216
	
	This is the best automatically learned part-of-speech tagging result known to us, representing an error reduction of 4.4% on the model presented in Collins (2002), using the same data splits, and a larger error reduction of 12.1% from the more similar best previous loglinear model in Toutanova and Manning (2000). ::: N03-1033_24:202
	
	As a result, the dependency parsing problem is written: G = argmax G=(V,A) summation display (i,j,l)A s(i,j,l) This problem is equivalent to finding the highest scoring directed spanning tree in the complete graph over the input sentence, which can be solved in O(n2) time (McDonald et al., 2005b). ::: P08-1108_33:172
	
	In general, dependency parsers (McDonald et al., 2006; Nivre et al., 2007), seem to suffer more from this domain change than constituency parsers (Charniak and Johnson, 2005; Petrov et al., 2006). ::: D10-1069_19:206
	
M24	We will also explore ways of combining graph-based and transition-based parsers along the lines of Nivre and McDonald (2008). ::: W09-1104_226:228
	
M01,M12	Therehas been extensive work on data-driven dependency parsingfor both projective parsing(Eisner, 1996;Paskin,2001;Yamadaand Matsumoto, 2003; Nivre and Scholz,2004; McDonaldet al., 2005a)and non-projective parsing systems(Nivre andNilsson,2005;HallandNovak,2005;McDonald et al., 2005b). ::: P09-1039_67:228
	
R01,R02	We present a comparison of three statistical parsing architectures that output typed dependencies for French: one constituency-based architecture featuring the Berkeley parser (Petrov et al., 2006), and two dependency-based systems using radically different parsing methods, MSTParser (McDonald et al., 2006) and MaltParser (Nivre et al., 2006). ::: C10-2013_17:211
	
V01	The system with online learning and Nivres parsing algorithm was trained on the data released by CoNLL Shared Task Organizers for all the ten languages (Hajic et al. , 2004; Aduriz et al. , 2003; Mart et al. , 2007; Chen et al. , 2003; Bohmova et al. , 2003; Marcus et al. , 1993; Johansson and Nugues, 2007; Prokopidis et al. , 2005; Csendes et al. , 2005; Montemagni et al. , 2003; Oflazer et al. , 2003). ::: D07-1124_76:98
	
P04	This includes work on phrase structure parsing (Collins, 1997; Charniak, 2000; Petrov et al., 2006), dependency parsing (McDonald et al., 2005; Nivre et al., 2006) as well as a number of other formalisms (Clark and Curran, 2004; Wang and Harper, 2004; Shen and Joshi, 2008). ::: D11-1006_8:303
	
M37	Except these three languages, we use software of projectivization/deprojectivization provided by Nivre and Nilsson (2005) for other languages. ::: D07-1098_97:168
	
	However, evaluations on the widely used WSJ corpus of the Penn Treebank (Marcus et al. , 1993) show that the accuracy of these parsers still lags behind the state-of-theart. ::: P06-2089_12:169
	
	We will evaluate the parser on both the full PTB (Marcus et al. 1993) and on a sense annotated subset of the Brown Corpus portion of PTB, in order to investigate the upper bound performance of the models given gold-standard sense information, as in Agirre et al. ::: P11-2123_13:109
	
	We evaluate the accuracy of HPSG parsing with dependencyconstraintsontheHPSGTreebank(Miyao et al. , 2003), which is extracted from the Wall Street Journal portion of the Penn Treebank (Marcus et al. , 1993)1. ::: P07-1079_80:260
	
R01,R02	Compared to graph-based dependency parsing, it typically offers linear time complexity and the comparative freedom to define non-local features, as exemplified by the comparison between MaltParser and MSTParser (Nivre et al., 2006b; McDonald et al., 2005; McDonald and Nivre, 2007). ::: P11-2033_8:103
	
	Averaging has been shown to help reduce overfit ting (Collins, 2002). ::: P05-1012_75:209
	
	Nivre and Scholz (2004) proposed a variant of the model of Yamada and Matsumoto that reduces the complexity from the worst case quadratic to linear. ::: N07-1049_41:180
	
	The recent advances in parsing have achieved parsers with 3 ()On time complexity without the grammar constant (McDonald et al. , 2005). ::: P05-1067_199:217
	
	On the other hand, the best available parsers trained on the Penn Treebank, those of Collins (1997) and Charniak (2000), use statistical models for disambiguation that make crucial use of dependency relations. ::: C04-1010_11:293
	
	Algorithm 2: Average multiclass perceptron input :S= (xi,yi)N;0k =vector0, kY for t = 1 to T do choose j Et ={rY :xj,trxj,tyj} if|Et|> 0 then t+1r = tr xj|Et|, rEt t+1yj = tyj +xj output: k = 1T summationtextt tk, kY 3.4 Higher-order feature spaces Yamada and Matsumoto (2003) and McDonald and Pereira (2006) have shown that higher-order feature representations and modeling can improve parsing accuracy, although at significant computational costs. ::: W07-2217_86:259
	
	If the trees are constrained to be projective, EM is easily applied using the inside-outside variant of the parsing algorithm described by Eisner (1996) to compute the marginal probability. ::: D07-1014_134:189
	
R01,F02	This is a pattern that has been observed in previous evaluations of the parsers and can be explained by the global learning and inference strategy of MSTParser and the richer feature space of MaltParser (McDonald and Nivre, 2007). ::: C10-1094_178:185
	
	To train the model, we use the averaged perceptron algorithm (Collins, 2002). ::: P10-1110_56:231
	
M37	The parsing methodology investigated here has previously been applied to Swedish, where promising results were obtained with a relatively small treebank (approximately 5000 sentences for training), resulting in an attachment score of 84.7% and a labeled accuracy of 80.6% (Nivre et al. , 2004).1 However, since there are no comparable results available for Swedish, it is difficult to assess the significance of these findings, which is one of the reasons why we want to apply the method to a benchmark corpus such as the the Penn Treebank, even though the annotation in this corpus is not ideal for labeled dependency parsing. ::: C04-1010_24:293
	
M16	MST uses Chu-LiuEdmonds (Chu and Liu, 1965; Edmonds, 1967) Maximum Spanning Tree algorithm for nonprojective parsing and Eisner's algorithm for projective parsing (Eisner, 1996). ::: W10-1403_132:262
	
V01	The data are from 2006/7 CoNLL shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007a), where punctuation was identified by the organizers, who also furnished disjoint train/test splits. ::: W11-0303_172:220
	
M26	Combinatorial algorithms (Chu and Liu, 1965; Edmonds, 1967) can solve this problem in cubic time.4 If the dependency parse trees are restricted to be projective, cubic-time algorithms are available via dynamic programming (Eisner, 1996). ::: P09-1039_85:228
	
M01,M23	There has been extensive work on data-driven dependency parsingfor both projective parsing(Eisner, 1996; Paskin, 2001; Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; McDonaldet al., 2005a) and non-projective parsing systems (Nivre and Nilsson,2005;Hall and Novak, 2005;McDonald et al., 2005b). ::: D08-1017_29:217
	
	For example, Sagae and Lavie (2006) displayed that combining the predictions of both parsing models can lead to significantly improved accuracies. ::: D07-1013_25:290
	
	For the baseline systems, we used the firstand second-order (parent-sibling) features that were used in McDonald and Pereira (2006) and other second-order features (parent-child-grandchild) that were used in Carreras (2007). ::: C10-2015_143:184
	
M33	Classifier-based dependency parsers (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004) learn from an annotated corpus how to select an appropriate sequence of Shift/Reduce actions to construct the dependency tree for a sentence. ::: D07-1119_8:133
	
M01,P29,M37	In the experiments below, we employ a data-driven deterministic dependency parser producing labeled projective dependency graphs,3 previously tested on Swedish (Nivre et al. , 2004) and English (Nivre and Scholz, 2004). ::: P05-1013_80:139
	
M24	Nivre and McDonald (2008) instead use hints from one parse as features in a second parse, exploiting the complementary properties of graph-based parsers (Eisner, 1996; McDonald et al., 2005) and transition-based dependency parsers (Yamada & Matsumoto, 2003; Nivre & Scholz, 2004). ::: N09-2066_7:73
	
M24	To that end, there are two dominant approaches: graph-based methods, characterized by arc features in an exhaustive search, and transition-based methods, characterized by operational features in a greedy search (McDonald and Nivre, 2007). ::: C10-1007_28:238
	
	Following last years shared task practice (Buchholz and Marsi, 2006), we use the following definition of projectivity: An arc (i, j) is projective iff all nodes occurring between i and j are dominated by i (where dominates is the transitive closure of the arc relation). ::: D07-1096_176:410
	
	With a linear scoring function, the parser solves: parse(s) = argmaxts summation display [h,m]t w f(h,m,s) The weights w are typically learned using an online method, such as an averaged perceptron (Collins, 2002) or MIRA (Crammer and Singer, 2003). ::: C10-1007_34:238
	
	All three of the models are based on versions of the Carreras (2007) parser, so modifying these methods to work with our new third-order parsing algorithms would be an interesting topic for future research. ::: P10-1001_217:237
	
	The feature function is a second-order edge factored representation (McDonald and Pereira, 2006; Carreras, 2007). ::: W08-2123_25:149
	
	This is part of the explanation of why (Charniak, 2000) finds that early generation of head tags as in (Collins, 1999) is so beneficial. ::: P03-1054_198:233
	
M03	Unlike the deterministic parsers above, this parser uses a dynamic programming algorithm (Eisner, 1996) to determine the best tree, so there is no difference between presenting the input from left-to-right or right-to-left. ::: N06-2033_65:93
	
	For parsing features, we follow standard practice for graph-based dependency parsing (McDonald, 2006; Carreras, 2007; Koo and Collins, 2010). ::: D11-1109_80:271
	
	Note that the approximations to maximizing over spanning trees with second-order features, proposed by McDonald and Pereira (2006), do not permit estimating the clusters as part of the same process as weight estimation (at least not without modification). ::: D07-1014_171:189
	
M33	Shift-reduce parsers have become popular for dependency parsing, building on the initial work of Yamada and Matsumoto (2003) and Nivre and Scholz (2004). ::: P11-1069_11:201
	
	This algorithm can thus be viewed as a large-margin version of the perceptron algorithm for structured outputs Collins (2002). ::: E06-1011_136:205
	
	To regularize the model we take as the final model the average of all weight vectors posited during training (Collins, 2002). ::: W07-2217_83:259
	
	To match previous work (McDonald and Pereira, 2006; Koo et al., 2008), we split the data into a training set (sections 2-21), a development set (Section 22), and a test set (section 23). ::: C10-2015_133:184
	
	Length of dependency links: McDonald and Nivre (2007) report that statistical parsers have a drop in 116 accuracy when analysing long distance dependencies. ::: W11-0314_45:167
	
	The experimental results in (McDonald and Nivre, 2007) show a negative impact on the parsing accuracy from too long dependency relation. ::: P09-1007_170:206
	
	Similar observations regarding the effect of model order have also been made by Carreras (2007). ::: P08-1068_125:218
	
M16	This contrasts with the online learning algorithms used in previous work with spanning-tree models (McDonald et al. , 2005b). ::: D07-1015_22:443
	
V01	The reasons for this interest are manifold; the availability of shared task data from various CoNLL conferences (among others (Buchholz and Marsi, 2006; Hajic et al., 2009)), comprising collections of languages based on a single representation format, has certainly been instrumental. ::: C10-2129_9:191
	
	After all iterations, the algorithm computes the average of v , which reduces the effect of overfit ting (Collins, 2002). ::: C10-1011_83:243
	
	For experiment on English, we used the English Penn Treebank (PTB) (Marcus et al., 1993) and the constituency structures were converted to dependency trees using the same rules as (Yamada and Matsumoto, 2003). ::: P08-1061_117:148
	
	The second extension is to apply the approach to second order parsing models, such as those described in (Carreras, 2007), using a twostage semi-supervised learning approach. ::: D09-1058_5:216
	
	The following treebanks were used for training the parser: (Aduriz et al. , 2003; Bhmov et al. , 2003; Chen et al. , 2003; Haji et al. , 2004; Marcus et al. , 1993; Mart et al. , 2002; Montemagni et al. 2003; Oflazer et al. , 2003; Prokopidis et al. , 2005; Csendes et al. , 2005). ::: D07-1119_130:133
	
	For example, the parser of McDonald and Pereira (2006) defines parts for sibling interactions, such as the trio plays, Elianti, and . in Figure 1. ::: P08-1068_32:218
	
	The data sets derived from the original treebanks (section 3) were in the same column-based format as for the 2006 shared task (Buchholz and Marsi, 2006). ::: D07-1096_21:410
	
M33,M24	Most subsequent works on shift-reduce or transition-based dependency parsing followed arc-eager (Nivre and Scholz, 2004; Zhang and Clark, 2008), which now becomes the dominant style. ::: D09-1127_68:211
	
	Thus, the Penn Treebank of American English (Marcus et al. , 1993) has been used to train and evaluate the best available parsers of unrestricted English text (Collins, 1999; Charniak, 2000). ::: W04-2407_86:153
	
	For languages with treebanks, supervised models give the state-of-the-art performance in dependency parsing (McDonald and Pereira, 2006; Nivre et al., 2006; Koo and Collins, 2010; Martins et al., 2010) and constituent parsing (Collins, 2003; Charniak and Johnson, 2005; Petrov et al., 2006). ::: D11-1110_6:175
	
V01,M37	Several approaches to dependency parsing on multiple languages have been evaluated in the CoNLL-X Shared Task (Buchholz & Marsi, 2006). ::: N07-1049_43:180
	
	In many dependency parsing models such as (Eisner, 1996) and (MacDonald et al. , 2005), the score of a dependency tree is the sum of the scores of the dependency links, which are computed independently of other links. ::: W05-1516_151:171
	
	The base parsing models are either independently trained (Sagae and Lavie, 2006; Hall et al., 2007; Attardi and DellOrletta, 2009; Surdeanu and Manning, 2010), or their training is integrated, e.g. using stacking (Nivre and McDonald, 2008; Attardi and DellOrletta, 2009; Surdeanu and Manning, 2010). ::: P11-2125_87:98
	
M03	Parsers that attempt to disambiguate the input completely full parsing typically first employ some kind of dynamic programming algorithm to derive a packed parse forest and then applies a probabilistic top-down model in order to select the most probable analysis (Collins, 1997; Charniak, 2000). ::: W04-0308_10:145
	
M16	More importantly, when this classifier is integrated into a 2nd-ordered maximum spanning tree (MST) dependency parser (McDonald and Pereira, 2006) in a weighted average manner, significant improvement is obtained over the MST baselines. ::: P10-1002_27:187
	
	For the non-directional parser, we projectivize the training set prior to training using the procedure described in (Carreras, 2007). ::: N10-1115_145:202
	
M37	We show that stacking methods outperform the approximate second-order parser of McDonald and Pereira (2006) on twelve languages and can be used within that approximation to achieve even better results. ::: D08-1017_55:217
	
M09	These SVM settings are the same as previous research (Kudo and Matsumoto, 2002; Sassano, 2004). ::: C08-1046_136:234
	
	To overcome a minor shortcoming of the parsing algorithm of (Nivre et al. , 2004) we introduce a simple language independent post-processing step. ::: D07-1099_55:90
	
	Several works have tested the effect of using a two-stage parser (Nivre and McDonald, 2008; Martins et al., 2008), where the second parser takes advantage of features obtained by the first one. ::: W10-1404_21:207
	
V01,E02	Best: For the CoNLL-X languages only, the best UAS for any parser in the original shared task (Buchholz and Marsi, 2006) or in any column of Martins et al. ::: D10-1125_203:289
	
M23	Examples of this include McDonald and Pereiras (2006) rewriting of projective trees produced by the Eisner (1996) algorithm, and Nivre and Nilssons (2005) pseudo projective approach that creates projective trees with specially marked arcs that are later transformed into non-projective dependencies. ::: D11-1116_170:208
	
R02,M23	As our baseline, we take the strictly projective arc-eager transition system proposed by Nivre (2003), as implemented in the freely available MaltParser system (Nivre et al., 2006a), with and without the pseudo-projective parsing technique for recovering non-projective dependencies (Nivre and Nilsson, 2005). ::: P10-1151_186:201
	
P30	However, we have found this not to be a problem when measuring multi planarity in natural language treebanks, since the effective problem size can be reduced by noting that each connected component of the crossings graph can be treated separately, and that nodes that are not part of a cycle need not be considered.5 Given that non-projective sentences in natural language tend to have a small proportion of non-projective links (Nivre and Nilsson, 2005), the connected components of their crossings graphs are very small, and k-colourings for them can quickly be found by brute-force search. ::: P10-1151_76:201
	
	At this point, we have many different parsing models that reach and even surpass 90% dependency or constituency accuracy on this test set (McDonald et al., 2006; Nivre et al., 2007; Charniak and Johnson, 2005; Petrov et al., 2006; Carreras et al., 2008; Koo and Collins, 2010). ::: D10-1069_10:206
	
M16	The optimal parse can be found using a spanning tree algorithm (Eisner, 1996; McDonald et al. , 2005). ::: W07-2220_21:50
	
	Carreras (2007) presents a second-order parser that can score both sibling and grandchild parts, with complexities of O(n4) time and O(n3) space. ::: P10-1001_160:237
	
	N&N2005: The pseudo-projective parser of Nivre and Nilsson (2005). ::: H05-1066_179:223
	
V01,M37	Our experiments are based on five data sets from the CoNLL-Xshared task: Arabic, Czech, Danish, Slovene, and Turkish (Buchholz and Marsi, 2006). ::: P09-1040_102:154
	
R02	Columns 24 enumerate LAS for Malt, MST2O and Malt + MST2O as in Nivre and McDonald (2008). ::: D08-1017_188:217
	
	The features are designed over edges of dependency trees and the weights are given by model parameters (McDonald and Pereira, 2006; Carreras, 2007). ::: C10-2015_66:184
	
R02,M16	We evaluate these methods on the Prague Dependency Treebank using online large-margin learning techniques (Crammer et al. , 2003; McDonald et al. , 2005) and show that MST parsing increases efficiency and accuracy for languages with non-projective dependencies. ::: H05-1066_5:223
	
M12	Exact inference for parsing models that allow non-projective trees is NP hard, except under very restricted independence assumptions (Neuhaus and Broker, 1997; McDonald and Pereira, 2006; McDonald and Satta, 2007). ::: P09-1040_12:154
	
V01,M12	We bootstrapped non-projective parsers for languages assembled for the CoNLL dependency parsing competitions (Buchholz and Marsi, 2006). ::: D07-1070_168:458
	
	In the second category are those that employ exhaustive inference algorithms, usually by making strong independence assumptions, as is the case for edge-factored models (Paskin, 2001; McDonald et al. , 2005a; McDonald et al. , 2005b). ::: W07-2216_44:259
	
M12,M01	However, recent results in non-projective dependency parsing, especially using data-driven methods, indicate that most non-projective structures required for the analysis of natural language are very nearly projective, differing only minimally from the best projective approximation (Nivre and Nilsson, 2005; Hall and Novk, 2005; McDonald and Pereira, 2006). ::: P06-2066_11:252
	
	The features used to score, while based on the previous work in dependency parsing (McDonald et al. , 2005), introduce some novel concepts such as better codification of context and surface distances, and runtime information from dependencies previously parsed. ::: W06-2925_9:112
	
	Beam Search Several researchers dealt with the early-commitment and error propagation of deterministic parsers by extending the greedy decisions with various flavors of beam-search (Sagae and Lavie, 2006a; Zhang and Clark, 2008; Titov and Henderson, 2007). ::: N10-1115_178:202
	
	In our dependency parsing experiments we used unlabeled dependencies extracted from the Penn 130 Treebank using the same head-table as Yamada and Matsumoto (2003), using sections 02-21 as training data and section 23 as test data, following (McDonald et al. , 2005; Nivre & Scholz, 2004; Yamada & Matsumoto, 2003). ::: N06-2033_53:93
	
	Mc06: The best UAS reported by McDonald and Pereira (2006). ::: D10-1125_202:289
	
	Specifically, we view stacked learning as a way of approximating non-local features in a linear model, rather than making empirically dubious independence (McDonald et al., 2005b) or structural assumptions (e.g., projectivity, Eisner, 1996), using search approximations (Sagae and Lavie, 2005; Hall et al., 2006; McDonald and Pereira, 2006), solving a (generally NP-hard) integer linear program (Riedel and Clarke, 2006), or adding latent variables (Titov and Henderson, 2007). ::: D08-1017_15:217
	
	The model in Charniak (2000) is quite different, however. ::: J05-1003_505:603
	
	Furthermore, it made the system homogeneous in terms of learning algorithms since that is what is used to train our unlabeled parser (McDonald and Pereira, 2006). ::: W06-2932_45:130
	
V01,M37,P24	We use all 23 train/test splits from the 2006/7 CoNLL shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007),6 which cover 19 different languages.7 We splice out all punctuation labeled in the data, as is standard practice (Paskin, 2001; Klein and Manning, 2004), introducing new arcs from grandmothers to grand-daughters where necessary, both in trainand test-sets. ::: D11-1117_67:207
	
M20	This type of classier has been used successfully in deterministic parsing by Kudo and Matsumoto (2002),Yamada and Matsumoto (2003), and Sagae and Lavie (2005),among others. ::: J08-3003_237:443
	
	For the baseline system (1), we used the system of McDonald and Pereira (2006) on a MacPro 2.8 Ghz as well for our implementation (2). ::: W09-1210_102:135
	
	We adopted the second order MST parsing algorithm as outlined by Eisner (1996). ::: W09-1210_19:135
	
	This model is related to the averaged perceptron algorithm of Collins (2002). ::: H05-1066_143:223
	
	For example, Nivre and McDonald (2008) present the combination of two state of the art dependency parsers feeding each another, showing that there is a significant improvement over the simple parsers. ::: W10-1404_43:207
	
M16	Overall the accuracy of the DeSR parser with semantic information is slightly inferior to that of the second-order MST parser (McDonald & Pereira, 2006) (91.5% UAS). ::: W07-2217_216:259
	
	Over the past decade, there has been tremendous progress on learning parsing models from treebank data (Magerman, 1995; Collins, 1999; Charniak, 1997; Ratnaparkhi, 1999; Charniak, 2000; Wang et al. , 2005; McDonald et al. , 2005). ::: N07-3002_7:74
	
M23,M37	The first thing to note is that pseudo-projective parsing gives a significant improvement for PDT, as previously reported by Nivre and Nilsson (2005), but also for Alpino, where the improvement is even larger, presumably because of the higher proportion of non-projective dependencies in the Dutch treebank. ::: P07-1122_108:184
	
	To illustrate how this framework allows for improvements in the accuracy of dependency parsing to be used directly to improve the accuracy of HPSG parsing, we showed that by combining the results of different dependency parsers using the search-based parsing ensemble approach of (Sagae and Lavie, 2006), we obtain improved HPSG parsing accuracy as a result of the improved dependency accuracy. ::: P07-1079_153:260
	
	These are due to the limitations of the parsing algorithm of (McDonald and Pereira, 2006), which does not allow the features defined on those types of trigram-subtrees. ::: D09-1060_70:253
	
F01	Features Used for Selecting Reduce The features used in (Nivre and Scholz, 2004) to define a state transition are basically obtained from the two target words wi and wj, and their related words. ::: P10-2035_39:126
	
	In 2stage, we can provide features specific to each stage which cant be done in a single stage approach (McDonald et al., 2006). ::: W10-1403_214:262
	
	In fact, the largest source of English dependency trees is automatically generated from the Penn Treebank (Marcus et al. , 1993) and is by convention exclusively projective. ::: H05-1066_14:223
	
P29,M12	Some authors propose to solve it by techniques for recovering non-projectivity from the output of a projective parser in a post-processing step (Hall and Novk, 2005; Nivre and Nilsson, 2005), others extend projective parsers by heuristics that allow at least certain non-projective constructions to be parsed (Attardi, 2006; Nivre, 2007). ::: E09-1055_15:234
	
M01	In the case of dependency parsers it is also possible to use grammars (Eisner and Satta, 1999), but many algorithms use a data-driven approach instead, making individual decisions about which dependencies to create by using probabilistic models (Eisner, 1996) or classifiers (Yamada and Matsumoto, 2003). ::: E09-1034_54:178
	
P29	The majority of graph-based parsers in the shared task were based on what McDonald and Pereira (2006) call the first-order model, where the score of each arc is independent of every other arc, but there were also attempts at exploring higher-order models, either with exact inference limited to projective dependency graphs (Carreras, 2007), or with approximate inference (Nakagawa, 2007). ::: W07-2220_24:50
	
	This paper describes a simple yet novel method for constructing sets of 50-best parses based on a coarse-to-fine generative parser (Charniak, 2000). ::: P05-1022_4:180
	
	Averaging parameters is a way to reduce overfit ting for perceptron training (Collins, 2002), and is applied to all our experiments. ::: D08-1059_36:188
	
M16	By and large, transition-based models use a greedy inference strategy, whereas graph-based models used different Maximum Spanning Tree (MST) algorithms: Carreras (2007) MSTC, Eisner (2000) MSTE, or Chu-Liu/Edmonds (McDonald et al., 2005; Chu and Liu, 1965; Edmonds, 1967) MSTCL/E. More interestingly, most of the best systems used some strategy to mitigate parsing errors. ::: W08-2121_296:401
	
R01	While we see improvements over the single-parser baseline 4We made other modifications to MSTParser, implementing many of the successes described by (McDonald et al., 2006). ::: D08-1017_160:217
	
	Supervised structured large margin training approaches have been applied to parsing and produce promising results (Taskar et al., 2004; McDonald et al., 2005a; Wang et al., 2006). ::: P08-1061_59:148
	
M37	This allows us to ef ciently use ILP for dependency parsing and add constraints which provide a signi cant improvement over the current stateof-the-art parser (McDonald et al. , 2005b) on the Dutch Alpino corpus (see bl row in Table 1). ::: W06-1616_234:238
	
	CoNLL-X shared task (Buchholz and Marsi, 2006). ::: D07-1099_11:90
	
	Our features are in many ways similar to those of Charniak (2000). ::: J05-1003_504:603
	
	There has been much recent work on dependency parsing using graph-based, transition-based, and hybrid methods; see Nivre and McDonald (2008) for an overview. ::: P09-1039_78:228
	
	This is supported by the fact that the accuracy of short dependencies is in general greater than that of long dependencies as reported in McDonald and Nivre (2007) for graph-based models. ::: C10-2015_13:184
	
	These include the perceptron (Collins, 2002) and its large-margin variants (Crammer and Singer, 2003; McDonald et al. , 2005a). ::: W07-2216_137:259
	
	Indeed, as for the voted perceptron of Collins (2002), we can get performance gains by reducing the support threshold for features to be included in the model. ::: N03-1033_22:202
	
	On the WSJ corpus, this parser achieves the same performance as that of McDonald and Pereira (2006). ::: P10-1002_170:187
	
P32	NO-RERANK Charniak (2000)s parser, based on a lexicalized PCFG model of phrase structure trees.3 The probabilities of CFG rules are parameterized on carefully hand-tuned extensive information such as lexical heads and symbols of ancestor/sibling nodes. ::: P08-1006_35:181
	
M33,S02,S01	We show how Nivres (2009) swap-based reordering technique for non-projective shift-reduce-style parsing can be integrated into the non-directional easy-first framework of Goldberg and Elhadad (2010) to support non-projectivity, and we report the results of our parsing experiments on the standard test section of the PTB, providing comparisons with several freely available parsers, including Goldberg and Elhadads (2010) implementation, MALTPARSER (Nivre et al., 2006), MSTPARSER (McDonald et al., 2005; McDonald and Pereira, 2006), the Charniak (2000) parser, and the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007). ::: D11-1116_12:208
	
	Recently dependency parsing has received renewed interest, both in the parsing literature (Buchholz and Marsi, 2006) and in applications like translation (Quirk et al. , 2005) and information extraction (Culotta and Sorensen, 2004). ::: D07-1014_8:189
	
M05	This is true of the widely used link grammar parser for English (Sleator and Temperley, 1993), which uses a dependency grammar of sorts, the probabilistic dependency parser of Eisner (1996), and more recently proposed deterministic dependency parsers (Yamada and Matsumoto, 2003; Nivre et al. , 2004). ::: P05-1013_10:139
	
R01	Parsers For graph-based parsers, we used the projective first-order (MST1) and second order (MST2) variants of the freely available MST parser4 (McDonald et al., 2005; McDonald and Pereira, 2006). ::: W10-2927_116:238
	
M37	We parse the English sentences with the Charniak Parser (Charniak and Johnson, 2005), and tag the Chinese sentences with a POS tagger implemented faithfully according to (Collins, 2002) and trained on the Penn Chinese Treebank 5.0 (Xue et al., 2005). ::: D11-1110_104:175
	
	This approach is one of those described in Eisner (1996). ::: W06-2920_233:416
	
F05	For this paper, we used POS tags that were provided either by the Treebank itself (gold standard tags) or by the perceptron POS tagger3 presented in Collins (2002). ::: P04-1015_174:297
	
	The best projective parse tree is obtained using the Eisner algorithm (Eisner, 1996) with the scores, and the best non-projective one is obtained using the ChuLiu-Edmonds (CLE) algorithm (McDonald et al. , 2005b). ::: D07-1100_36:206

	The second-order model of McDonald and Pereira (2006) considers h, m, ch. ::: D07-1101_40:204

S02,S01	We know that Maltparser is good at short distance labeling and MST is good at long distance labeling (McDonald and Nivre, 2007). ::: W10-1403_230:262

M24,S02,S01	The terms graph-based and transition-based were used by McDonald and Nivre (2007) to describe the difference between MSTParser (McDonald and Pereira, 2006), which is a graph-based parser with an exhaustive search decoder, and MaltParser (Nivre et al., 2006), which is a transition-based parser with a greedy search decoder. ::: D08-1059_9:188

E02	The evaluation metric traditionally associated with dependency parsing is based on scoring labeled or unlabeled attachment decisions, whereby each correctly identified pair of head-dependent words is counted towards the success of the parser (Buchholz and Marsi, 2006). ::: D11-1036_7:232

	With both these classifiers we use the same top-1 approach as with the CW-classifers and also averaging which has been shown to alleviate overfit ting (Collins, 2002). ::: P10-3010_76:143

M05	Additionally, following Sagae and Lavie (2006), we extend the basic deterministic LR algorithm with a bestfirst search, which results in a parsing strategy similar to generalized LR parsing (Tomita, 1987; 1990), except that we do not perform Tomitas stack-merging operations. ::: D07-1111_27:130

V01	The treebank data in our experiments are from the CoNLL shared-tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007). ::: D11-1006_49:303

	The parser of McDonald and Pereira (2006) had been applied to English, Czech and Danish, and the parser of Nivre et al. ::: D07-1096_56:410

	Now it is easy to see why the original decision Right-Arcr (Nivre et al. , 2004) had to be decomposed into two distinct decisions: the decision to construct a labeled arc and the decision to shift the word. ::: W07-2218_169:279

	Carreras (2007) and Titov and Henderson (2007) obtained the second and third positions, respectively. ::: W10-1404_146:207

	The algorithms are commonly used in other online-learning dependency parsing, such as in (McDonald et al. , 2005a). ::: D07-1126_36:136

	Table 5: Comparison against the state-of-the-art full up to 40 (McDonald and Pereira, 2006)a 0.825 (Wang et al., 2007) 0.866 (Chen et al., 2008) 0.852 0.884 Ours 0.861 0.889 aThis results was reported in (Wang et al., 2007). ::: P09-1007_169:206

	An alternative method, used by Charniak in the adaptation of his parser for Czech 6 and used by Nivre and Nilsson (2005), alters the dependency links by raising the governor to a higher node in the tree whenever 5 Bilexical dependencies are components of both the Collins and Charniak parsers and effectively model the types of syntactic subordination that we wish to extract in a dependency tree. ::: W05-1505_66:197

	This paper explores an alternative approach to parsing, based on the perceptron training algorithm introduced in Collins (2002). ::: P04-1015_16:297

M37	The shared tasks of multi-lingual dependency parsing took place at CoNLL-2006 (Buchholz and Marsi, 2006) and CoNLL-2007 (Nivre et al., 2007). ::: C08-1046_6:234

	It is straight-forward to extend the algorithms of Eisner (1996) and Paskin (2001) to the labeled case adding only a factor of O(|L|n2). ::: W07-2216_132:259

M12	McDonald and Pereira (2006), it was shown that non-projective dependency parsing with horizontal Markovization is FNP-hard. ::: W07-2216_220:259

	Dependency grammar has proven to be a very useful syntactic formalism, due in no small part to the development of efficient parsing algorithms (Eisner, 2000; McDonald et al., 2005b; McDonald and Pereira, 2006; Carreras, 2007), which can be leveraged for a wide variety of learning methods, such as feature-rich discriminative models (Lafferty et al., 2001; Collins, 2002; Taskar et al., 2003). ::: P10-1001_5:237

S01	For baseline systems, we used the firstand second-order basic features, which were the same as the features used by McDonald and Pereira (2006), and we used the default settings of MSTParser throughout the paper: iters=10; training-k=1; decode-type=proj. ::: D09-1060_132:253

	Exact nonprojective inference and estimation become intractable if we break edge factoring (McDonald and Pereira, 2006). ::: D07-1014_51:189

	We projectivize training data by a minimal transformation, lifting non-projective arcs one step at a time, and extending the arc label of lifted arcs using the encoding scheme called HEAD by Nivre and Nilsson (2005), which means that a lifted arc is assigned the label rh, where r is the original label and h is the label of the original head in the nonprojective dependency graph. ::: W06-2933_32:84

	Ablation studies In order to better understand the contributions of the various feature types, we ran additional ablation experiments; the results are listed in Table 3, in addition to the scores of Model 0 and the emulated Carreras (2007) parser (see Section 4.3). ::: P10-1001_220:237

M16	McDonald and Pereira (2006) expanded their first-order spanning tree model to be second-order by factoring the score of the tree into the sum of adjacent edge pair scores. ::: C08-1132_143:161

	A dependency parser is trained on a corpus annotated with lexical dependencies, which are easier to produce by annotators without deep linguistic knowledge and are becoming available in many languages (Buchholz & Marsi, 2006). ::: N07-1049_31:180

	The second model is similar to that of McDonald and Pereira (2006): a factor consists of a main labeled dependency and the head child closest to the modifier (ch). ::: D07-1101_80:204

	This behavior is somewhat similar to parser stacking (Nivre and McDonald, 2008; Martins et al., 2008), in which a first-order parser derives some of its input features from the full 1-best output of another parser. ::: D08-1016_26:363

	We use feature sets that are very similar to those described in Carreras (2007). ::: D10-1125_174:289

M16	In this short paper, we extend the baseline feature templates with the following: Distance between S0 and N0 Direction and distance between a pair of head and modifier have been used in the standard feature templates for maximum spanning tree parsing (McDonald et al., 2005). ::: P11-2033_39:103

	From the fact that neither MIRA nor BPM clearly outperforms the other, we conclude that we have successfully replicated the results reported in (McDonald et al. , 2005a) for English. ::: N06-1021_147:189

M37	The definition of 2(x, h, m, c) is: dir cpos(xh) cpos(xm) cpos(xc) dir cpos(xh) cpos(xc) dir cpos(xm) cpos(xc) dir form(xh) form(xc) dir form(xm) form(xc) dir cpos(xh) form(xc) dir cpos(xm) form(xc) dir form(xh) cpos(xc) dir form(xm) cpos(xc) 3 Experiments and Results We report experiments with higher-order models for the ten languages in the multilingual track of the CoNLL-2007 shared task (Nivre et al. , 2007).1 In all experiments, we trained our models using the averaged perceptron (Freund and Schapire, 1999), following the extension of Collins (2002) for structured prediction problems. ::: D07-1101_71:204

	(McDonald et al., 2005a), x is used to denote the sentence to be parsed, and xi to denote the i-th word in the sentence. ::: P10-1002_33:187

M26,M06	Eisner (1996) proposed a CKY-like O(n3) algorithm. ::: C08-1046_9:234

	The performance of MIRA based parsing achieves the state-ofthe-art performance in English data (McDonald et al. , 2005a; McDonald et al. , 2006). ::: D07-1126_14:136

S01	This paper makes two contributions: 1) We combine together multiple word representations based on semantic and syntactic clusters in order to improve discriminative dependency parsing in the MSTParser framework (McDonald et al., 2005), and 2) We provide an ensemble method for combining diverse clustering algorithms that is the discriminative parsing analog to the generative product of experts model for parsing described in (Petrov, 2010). ::: P11-2125_11:98

S01	One of the MSTPARSER models used the Chu-Liu-Edmonds maximum spanning tree approach, and the other used the Eisner (1996) algorithm with second order features and a nonprojective rewriting post-processing step. ::: D11-1116_132:208

M26,M07	Eisner (1996) proposes an O(n3) decoding algorithm for dependency parsing. ::: D11-1109_63:271

	The method gives very similar accuracy to the model of Charniak (2000), which also uses a rich set of initial features in addition to Charniaks (1997) original model. ::: J05-1003_439:603

	Features For simplicity, in current work, we only used two sets of features word-pair and tag-pair indicator features, which are a subset of features used by other researchers on dependency parsing (McDonald et al., 2005a; Wang et al., 2007). ::: P08-1061_123:148

	McDonald and Pereira (2006) adopted an approximation based on O(n3) projective parsing followed by rearrangement to permit crossing arcs, achieving higher performance. ::: D08-1017_87:217

	This corresponds to the Head condition in Nivre and Nilsson (2005). ::: J08-4003_398:595

	Discriminative classifiers for mapping histories to parser actions (Kudo and Matsumoto, 2002; Yamada and Matsumoto, 2003). ::: C08-1081_60:137

M26,M07	Using this representation, the parsing algorithm of Eisner (1996) is sufficient for searching over all projective trees in O(n3) time. ::: H05-1066_3:223

S02,S01	To study the influence of parsing methodology, we will compare two different parsers: MaltParser (Nivre et al. , 2004) and MSTParser (McDonald et al. , 2005). ::: P07-1122_16:184

M09,S02,V01,M10	The best results have been achieved using Support-Vector Machines placing the MaltParser very high in both the CoNNL shared tasks on dependency parsing in 2006 and 2007 (Buchholz and Marsi, 2006; Nivre et al., 2007) and it has been shown that SVMs are better for the task than Memory-based learning (Hall et al., 2006). ::: P10-3010_44:143

M05	At a recent evaluation of data-driven systems for dependency parsing with data from 13 different languages (Buchholz and Marsi 2006), the deterministic classier-based parser of Nivre et al. ::: J08-4003_24:595

	However, in this work, we use forests from a Treebank parser (Charniak, 2000) whose grammar is often flat in many productions. ::: P08-1067_39:181

	In the grandparent models in our experiments, we use a similar definition with feature vectors L(x,i,k,lk1,lk) and R(x,i,k,rk1,rk), where k is the parent for word i under y|i. We train the model using the averaged perceptron for structured problems (Collins, 2002). ::: D10-1125_160:289

	Wetrained the models onprojectivized graphs following Nivre and Nilsson (2005) method. ::: W06-2930_53:214

V01	For a more complete definition, see the CoNLLX shared task description paper (Buchholz and Marsi, 2006). ::: D07-1111_15:130

M20,M05	Deterministic methods for dependency parsing have now been applied to a variety of languages, including Japanese (Kudo and Matsumoto, 2000), English (Yamada and Matsumoto, 2003), Turkish (Oflazer, 2003), and Swedish (Nivre et al. , 2004). ::: C04-1010_8:293

M07,M26	Figure 7 illustrates the cubic parsing actions of the Eisners parsing algorithm (Eisner, 1996) in the right direction, where s, r, and t refer to the start and end indices of the chart items. ::: C10-2015_56:184

V01	In order to get a first estimate of the empirical accuracy that can be obtained with transition-based 2-planar parsing, we have evaluated the parser on four data sets from the CoNLL-X shared task (Buchholz and Marsi, 2006): Czech, Danish, German and Portuguese. ::: P10-1151_185:201

M27	They use a variant of Eisners generative model C (Eisner, 1996b; Eisner, 1996a) for reranking and extend it to capture higher-order information than Eisners second-order generative model. ::: D11-1137_13:241

M22,	Many models have been successfully applied to sequence labeling problems, such as maximum entropy (Ratnaparkhi, 1996), conditional random fields (CRF) (Lafferty et al., 2001) and perceptron (Collins, 2002). ::: D11-1109_46:271

S02,M24,S01	In this paper, we use the following baseline parsers: MaltParser (Nivre et al., 2007) for transition-based parsing; MSTParser (McDonald et al., 2005) (with sibling 2-edge factors) and BohnetParser (Bohnet, 2010) (with general 2-edge factors) for graph-based parsing; and Berkeley Parser (Petrov et al., 2006) for constituency-based parsing. ::: D11-1113_67:193

	The two algorithms we employed in our dependency parsing model are the Eisner parsing (Eisner, 1996) and Chu-Lius algorithm (Chu and Liu, 1965). ::: D07-1126_35:136

S01	While the MSTParser uses exact-inference (Eisner, 1996), we apply beam-search to decoding. ::: D08-1059_37:188

M05	Sagae and Lavies parsing algorithm is similar to the one used by Nivre and Scholz (2004) for deterministic dependency parsing (using kNN). ::: P06-2089_83:169

M07	The (Eisner, 1996) algorithm is typically used for projective parsing. ::: P11-2125_22:98

M17	Like a variation of Eisners generative model C (Eisner, 1996b; Eisner, 1996a), 1In Figure 1, according to custom of dependency tree description, the direction of hyperedge is written as from head to tail nodes. ::: D11-1137_52:241

	Finally, we note that 50-best parsing is only a fac1Charniak in (Charniak, 2000) cites an accuracy of 89.5%. ::: P05-1022_104:180

	Using the full set of features described in (McDonald et al., 2005a; Wang et al., 2007) and comparing the corresponding dependency parsing 537 English PTB-10 Training(l/ul) 3026/1016 Dev 163 Test 270 PTB-15 Training 7303/2370 Dev 421 Test 603 PTB-20 Training 12519/4003 Dev 725 Test 1034 Chinese CTB4-10 Training(l/ul) 642/347 Dev 61 Test 40 CTB4-15 Training 1262/727 Dev 112 Test 83 CTB4-20 Training 2038/1150 Dev 163 Test 118 CTB4-40 Training 4400/2452 Dev 274 Test 240 CTB4 Training 5314/2977 Dev 300 Test 289 Table 1: Size of Experimental Data (# of sentences) results with previous work remains a direction for future work. ::: P08-1061_125:148

V01	Graph-based parsing models (McDonald and Pereira, 2006; Carreras, 2007) have achieved state-of-the-art accuracy for a wide range of languages as shown in recent CoNLL shared tasks (Buchholz et al., 2006; Nivre et al., 2007). ::: C10-2015_8:184

	Recent work by Nivre and Nilsson introduces a technique where the projectivization transformation is encoded in the non-terminals of constituents during parsing (Nivre and Nilsson, 2005). ::: W05-1505_133:197

	The two dependency parsers use radically different parsing approaches but have achieved very similar performance for a wide range of languages (McDonald and Nivre, 2007). ::: C10-2013_57:211

M05,M02	However, instead of performing deterministic parsing as in (Nivre et al. , 2004), we use this ordering to define a generative history-based model, by integrating word prediction operations into the set of parser actions. ::: W07-2218_26:279

	Our experiments involve data from two treebanks: the Wall Street Journal Penn treebank (Marcus et al., 1993) and the Chinese treebank (Xue et al., 2004). ::: N09-1009_165:240

	We evaluate its performance on the standard Penn English Treebank (PTB) dependency parsing task, i.e., train on sections 02-21 and test on section 23 with automatically assigned POS tags (at 97.2% accuracy) using a tagger similar to Collins (2002), and using the headrules of Yamada and Matsumoto (2003) for conversion into dependency trees. ::: D09-1127_146:211

	These independence assumptions are unwarranted, as it has already been established that modeling non-local information such as arity and nearby parsing decisions improves the accuracy of dependency models (Klein and Manning, 2002; McDonald and Pereira, 2006). ::: W07-2216_187:259

	We also demonstrate that our approach and other improvement techniques (Koo et al., 2008; Nivre and McDonald, 2008) are complementary and that we can achieve very high accuracies when we combine our method with other improvement techniques. ::: D09-1060_30:253

S01	Rows MSTParser 1/2 show the first-order (using feature templates 1 5 from Table 1) (McDonald et al., 2005) and second order (using all feature templates from Table 1) (McDonald and Pereira, 2006) MSTParsers, as reported by the corresponding papers. ::: D08-1059_130:188

P25,M23,P38	Since the parsing algorithm only produces projective dependency graphs, we may use pseudo-projective parsing to recover non-projective dependencies, i.e., projectivize training data and encode information about these transformations in extended arc labels to support deprojectivization of the parser output (Nivre and Nilsson, 2005). ::: D07-1097_41:119

M37	In the multilingual parsing track, participants train dependency parsers using treebanks provided for ten languages: Arabic (Hajic et al. , 2004), Basque (Aduriz et al. 2003), Catalan (Mart et al. , 2007), Chinese (Chen et al. , 2003), Czech (Bhmova et al. , 2003), English (Marcus et al. , 1993; Johansson and Nugues, 2007), Greek (Prokopidis et al. , 2005), Hungarian (Czendes et al. , 2005), Italian (Montemagni et al. , 2003), and Turkish (Oflazer et al. , 2003). ::: D07-1111_17:130

M24	First, the graph-based models have better precision than the transition-based models when predicting long arcs, which is compatible with the results of McDonald and Nivre (2007). ::: P08-1108_124:172

	But the denominator Zi is a normalizing constant that sums over all parses; it is found by a dependency-parsing variant of the inside algorithm, following (Eisner, 1996). ::: D07-1070_159:458

	If the parse has to be projective, Eisners bottom-up-span algorithm (Eisner, 1996) can be used for the search. ::: W06-2920_237:416

M26,M07	Eisner (1996) gave a generative model with a cubic parsing algorithm based on an edge factorization of trees. ::: P05-1012_22:209

	Another use of bottom-up is due to Eisner (1996), who introduced the notion of a span. ::: W06-2920_157:416

	The boosting approach to ranking has been applied to named entity segmentation (Collins 2002a) and natural language generation (Walker, Rambow, and Rogati 2001). ::: J05-1003_579:603

M33	In the first category are those methods that employ approximate inference, typically through the use of linear time shift-reduce parsing algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Nivre and Nilsson, 2005). ::: W07-2216_43:259

	The constraints are chosen based on the two criteria: (1) adding them to the base constraints (those added in advance) would result in an extremely large program, and (2) it must be efcient to detect whether the constraint is violated in y. No Cycles (T2) For every possible cycle c for the sentence x we have a constraint which forbids the case where all edges in c are active simultaneously: summation display (i,j)c di,j |c|1 Comma Coordination (C3) For each symmetric conjunction token i which forms a symmetric coordination and each set of tokens A in x to the left of i with no comma between each pair of successive tokens we add: summation display aA di,a |A|1 which forbids con gurations where i has the argument tokens A. Compatible Coordination Arguments (C4) For each conjunction token i and each set of tokens A in x with incompatible POS tags, we add a constraint to forbid con gurations where i has the argument tokens A. summation display aA di,a |A|1 Selective Projective Parsing (P1) For each pair of triplets (i,j,l1) and (m,n,l2) we add the constraint: ei,j,l1 + em,n,l2 1 if l1 or l2 is in P. 3.2 Training For training we use single-best MIRA (McDonald et al. , 2005a). ::: W06-1616_117:238

	It uses online large margin learning as the learning algorithm (McDonald et al., 2005). ::: W10-1403_133:262

	The second order algorithm of Carreras (2007) uses in addition to McDonald and Pereira (2006) the child of the dependent occurring in the sentence between the head and the dependent as well as the edge from the dependents to a grandchild. ::: C10-2129_33:191

S01,S02	Indeed, the highest scoring parsers trained using the MSTPARSER (McDonald and Pereira, 2006) and MALTPARSER (Nivre et al., 2006) parsing suites achieved only 78.8 and 81.1 labeled attachment F1, respectively. ::: D11-1116_23:208

	Their model 1 is defined by an enclosing grand sibling for each sibling or grandchild part used in Carreras (2007). ::: D11-1137_219:241

	Dependency Features, fdep(x,t,h,m) Unigram Features: whth dir, wmtm dir Bigram Features: whth wmtm dir dist In Between Features: th tb tm dir dist Surrounding Features: th1 th th+1 tm1 tm tm+1 dir dist Sibling Features, fsib(x,t,h,s,m) wh th ws ts wm tm dir Grandparent Features, fgrd(x,t,g,h,m) wg tg wh th wm tm dir gdir Grand-sibling Features, fgsib(x,t,g,h,s,m) wg tg wh th ws ts wm tm dir gdir 2This second-order model incorporates grandparent features composed of all grandchildren rather than just outermost ones, and outperforms the one of Carreras (2007) according to the results in Koo and Collins (2010). ::: D11-1109_82:271

	Performance of Alternative Models 157 5 Related Work Previous parsing models (e.g. , Collins, 1997; Charniak, 2000) maximize the joint probability P(S, T) of a sentence S and its parse tree T. We maximize the conditional probability P(T | S). ::: W05-1516_140:171

	Another unexploited connection is that probabilistic approaches pay closer attention to the individual errors made by each component of a parse, whereas the training error minimized in the large margin approach the structured margin loss (McDonald et al. , 2005) is a coarse measure that only assesses the total error of an entire parse rather than focusing on the error of any particular component. ::: N07-3002_14:74

	The model parameters are trained using a discriminative learning algorithm, e.g. averaged perceptron (Collins, 2002) or MIRA (Crammer and Singer, 2003). ::: P11-2125_23:98

	This averaging effect has been shown to reduce overfit ting and produce much more stable results (Collins, 2002). ::: P08-1067_60:181

S01,S02	For the parser stacking, we follow the approach of Nivre and McDonald (2008), using MaltParser as a guide for the MST parser with the hash kernel, i.e., providing the arcs and labels assigned by MaltParser as features. ::: C10-2129_156:191

M37,V01	There are now several approaches for multilingual dependency parsing, as demonstrated in the CoNLL 2006 shared task (Buchholz and Marsi, 2006). ::: D07-1111_8:130

M22	Whereas Ratnaparkhi (1996) used feature support cutoffs and early stopping to stop overfit ting of the model, and Collins (2002) contends that including low support features harms a maximum entropy model, our results show that low support features are useful in a regularized maximum entropy model. ::: N03-1033_188:202

	Graph-based dependency parsing finds the highest-scoring tree according to a scoring function that decomposes under an exhaustive search (McDonald et al., 2005). ::: C10-1007_30:238

	Journal Penn TreeBank (Marcus et al., 1993). ::: P11-1067_141:326

M23,P38	Most algorithms in this tradition are restricted to projective dependency graphs, but it is 549 Computational Linguistics Volume 34, Number 4 possible to recover non-projective dependencies using pseudo-projective parsing (Nivre and Nilsson 2005). ::: J08-4003_568:595

S01,S02,V01	Representative of each method, MSTParser and MaltParser gave comparable accuracies in the CoNLL-X shared task (Buchholz and Marsi, 2006). ::: D08-1059_15:188

S01,M24,M09	For the MST-parsing MIRA (McDonald et al., 2005a; McDonald and Pereira, 2006) and for transition-based parsing Support-Vector Machines (Hall et al., 2006; Nivre et al., 2006b). ::: P10-3010_7:143

	In this paper, we work with both first-order and second-order models, we train the models using MIRA, and we use the (Eisner, 1996) algorithm for inference. ::: P11-2125_24:98

	The edge template was developed on development data from the English Penn-III treebank (Marcus et al., 1993). ::: W11-1109_85:202

V01	The CoNLL-X shared task (Buchholz and Marsi, 2006) made a wide selection of standardized treebanks for different languages available for the research community and allowed for easy comparison between various statistical methods on a standardized benchmark. ::: W07-2218_7:279

M24	It is worth noting that both these systems combine transition based base parsers with a graph-based method for parser combination, as first described by Sagae and Lavie (2006). ::: D07-1096_241:410

M24,S01,S02	McDonald and Nivre (2007) analyze the difference between graph-based and transition-based parsers (specifically the MALT and MST parsers) by comparing the different kinds of errors made by both parsers. ::: W10-2927_20:238

M16	Czech dependency structures may contain nonprojective edges, so we employ a maximum directed spanning tree algorithm (Chu and Liu, 1965; Edmonds, 1967; McDonald et al., 2005b) as our firstorder parser for Czech. ::: P08-1068_152:218

M05	Assuming that the calls o(c) and t(c) can both be performed in constant time, the worst-case time complexity of a deterministic parser based on a transition system S is given by an upper bound on the length of transition sequences in S. When building practical parsing systems, the oracle can be approximated by a classifier trained on treebank data, a technique that has been used successfully in a number of systems (Yamada and Matsumoto, 2003; Nivre et al., 2004; Attardi, 2006). ::: P09-1040_49:154

M24,M01	Graph-based (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras et al., 2006) and transition-based (Yamada and Matsumoto, 2003; Nivre et al., 2006) parsing algorithms offer two different approaches to data-driven dependency parsing. ::: D08-1059_7:188

	The PredEdge features are exactly the six features used by Nivre and McDonald (2008) in their MSTMalt parser; therefore, feature set A is a replication of this parser except for modifications noted in footnote 4. ::: D08-1017_155:217

	First, in supervised models, a head outward process is modeled (Eisner, 1996; Collins, 1999). ::: P04-1061_71:208

	The learning methods using in discriminative parsing are Perceptron (Collins, 2002) and online large-margin learning (MIRA) (Crammer and Singer, 2003). ::: D07-1126_8:136

	Differences with our approach are that we use a beam, rather than best-first, search; we use a global model rather than local models chained together; and finally, our results surpass the best published results on the CCG parsing task, whereas Sagae and Lavie (2006a) matched the best PTB results only by using a parser combination. ::: P11-1069_179:201

	Such models are commonly referred to as edge-factored since their parameters factor relative to individual edges of the graph (Paskin, 2001; McDonald et al., 2005a).Edge-factored model shave many computational benefits,most notably that inference for nonprojective dependency graphs can be achieved in polynomial time(McDonaldet al.,2005b).Theprimary problemin treating each dependency as independent is that it is not a realistic assumption. ::: D08-1017_38:217

	For dependency parsing, McDonald and Pereira (2006) proposed a method which can incorporate some types of global features, and Riedel and Clarke (2006) studied a method using integer linear programming which can incorporate global linguistic constraints. ::: D07-1100_9:206

	Although the accuracy of our method did not reach that of (McDonald and Pereira, 2006), the scores were competitive even though our method is deterministic. ::: P10-2035_110:126

S01,S02	McDonald and Nivre (2007) compared the accuracy of MSTParser and MaltParser along a number of structural and linguistic dimensions. ::: C10-1094_67:185

S01,S02,M23	Table 7 gives the results for both development (cross-validation for SDT, PADT, and Alpino; 974 development set for PDT) and final test, compared to the two top performing systems in the shared task, MSTParser with approximate second-order non-projective parsing (McDonald et al. , 2006) and MaltParser with pseudo-projective parsing (but no coordination or verb group transformations) (Nivre et al. , 2006). ::: P07-1122_178:184

M23,M07	It is well known that projective dependency parsing using edge based factorization can be handled with the Eisner algorithm (Eisner, 1996). ::: H05-1066_103:223

	However, they make different types of errors, which can be seen as a reflection of their theoretical differences (McDonald and Nivre, 2007). ::: D08-1059_16:188

M23	Pseudo-projective parsing for recovering nonprojective structures (Nivre and Nilsson, 2005). ::: C08-1081_61:137

	We use the MIRA 217 online learner to set the weights (Crammer and Singer, 2003; McDonald et al. , 2005a) since we found it trained quickly and provide good performance. ::: W06-2932_44:130

	Nivre and McDonald (2008) were first to introduce stacking in the context of dependency parsing. ::: C10-1120_13:233

	Ax = {(i,j,l) | i,j Vx and l L} Let D(Gx) represent the subgraphs of graph Gx that are valid dependency graphs for the sentence x. Since Gx contains all possible labeled arcs, the set D(Gx) must necessarily contain all valid dependency graphs for x. Assume that there exists a dependency arc scoring function, s : V V L R. Furthermore, define the score of a graph as the sum of its arc scores, s(G = (V,A)) = summation display (i,j,l)A s(i,j,l) The score of a dependency arc, s(i,j,l) represents the likelihood of creating a dependency from word wi to word wj with the label l. If the arc score function is known a priori, then the parsing problem can be stated as, 123 G = argmax GD(Gx) s(G) = argmax GD(Gx) summation display (i,j,l)A s(i,j,l) This problem is equivalent to finding the highest scoring directed spanning tree in the graph Gx originatingoutoftherootnode0, which can be solved for both the labeled and unlabeled case in O(n2) time (McDonald et al. , 2005b). ::: D07-1013_40:290

	Statistical parsers reap dramatic gains from punctuation (Engel et al., 2002; Roark, 2001; Charniak, 2000; Johnson, 1998; Collins, 1997, inter alia). ::: W11-0303_186:220

	We used the method proposed by (Carreras, 2007) for our second-order parsing model. ::: D09-1058_146:216

	Another advantage of generative models is that they do not suffer from the label bias problems (Bottou, 1991), which is a potential problem for conditional or deterministic history-based models, such as (Nivre et al. , 2004). ::: W07-2218_41:279

	At any rate, regularized conditional loglinear models have not previously been applied to the problem of producing a high quality part-of-speech tagger: Ratnaparkhi (1996), Toutanova and Manning (2000), and Collins (2002) all present unregularized models. ::: N03-1033_178:202

	This extends the experiments of Nivre and McDonald (2008), replicated in our feature subset A. Table 4 enumerates the results. ::: D08-1017_174:217

	This approach has been further developed in particular by Ryan McDonald and his colleagues (McDonald, Crammer, and Pereira 2005; McDonald et al. 2005; McDonald and Pereira 2006) and is now known as spanning tree parsing, because the problem of nding the most probable tree under this type of model is equivalent to nding an optimum spanning tree in a dense graph containing all possible dependency arcs. ::: J08-4003_560:595

	Compared with the parsing algorithms of Carreras (2007), Algorithm 2 uses history information by adding line 8, 10, and 11. ::: C10-2015_107:184

	The majority of previous work on dependency parsing has focused on local (i.e. , classification of individual edges) discriminative training methods (Yamada and Matsumoto, 2003; Nivre et al. , 2004; Y. Cheng, 2005). ::: D07-1015_174:443

	However, global constraints cannot be incorporated into the CLE algorithm (McDonald et al. , 2005b). ::: W06-1616_81:238

	This limited look-ahead window leads to error propagation and worse performance on root and long distant dependencies relative to graphbased parsers (McDonald and Nivre, 2007). ::: N10-1115_14:202

	Our bottom-up deterministic analyzer adopt Nivres algorithm (Nivre and Scholz, 2004). ::: I05-3003_2:271

	In the first-order parsing models, the parts are individual head-modifier arcs in the dependency tree (McDonald et al., 2005). ::: P11-2125_16:98

M33,M09,M10	Y&M2003 is the SVM-shift reduce parsing model of Yamada and Matsumoto (2003), N&S2004 is the memory-based learner of Nivre and Scholz (2004) and MIRA is the the system we have described. ::: P05-1012_141:209

M12	For non-projective languages the algorithm is NP-hard and McDonald and Pereira (2006) introduce an approximate algorithm to handle such cases. ::: W07-2217_23:259

E02	In particular, metrics like attachment score for dependency parsers (Buchholz and Marsi, 2006) and Parseval for constituency parsers (Black et al., 1991) suffer from being an average over a highly skewed distribution of different grammatical constructions. ::: C10-1094_4:185

	As for the second-order features, we again base our features with those of McDonald and Pereira (2006), who reported successful experiments with second-order models. ::: D07-1101_68:204

M15,M03	This feature set forces us to adopt the expensive search procedure by Carreras (2007), which extends Eisners span-based dynamic programming algorithm (1996) to allow second-order feature dependencies. ::: W08-2123_27:149

S01,S02	Nivre and McDonald (2008) first showed how the MSTParser (McDonald et al., 2005) and the MaltParser (Nivre et al., 2007) could be improved by stacking each parser on the predictions of the other. ::: C10-1120_73:233

	It was already known that the two systems make different errors through the work of Sagae and Lavie (2006). ::: D07-1013_182:290

	We provide additional evidence that the parser ensemble approach proposed by Sagae and Lavie (2006a) can be used to improve parsing accuracy, even when only a single parsing algorithm is used, as long as variation can be obtained, for example, by using different learning techniques or changing parsing direction from forward to backward (of course, even greater gains may be achieved when different algorithms are used, although this is not pursued here); and, finally, 4. ::: D07-1111_12:130

	The feature sets we used are similar to other feature sets in the literature (McDonald et al., 2005a; Carreras, 2007), so we will not attempt to give a exhaustive description of the features in this section. ::: P08-1068_54:218

	The term no-3rd indicates a parser that was trained and tested with the thirdorder feature mappings fgsib and ftsib deactivated, though lower-order features were retained; note that Model 2, no-3rd is not identical to the Carreras (2007) parser as it defines grandchild parts for the pair of grandchildren. ::: P10-1001_226:237

M24	As with the graph-based parser, we use the discriminative perceptron (Collins, 2002) to train the transition-based model (see Figure 5). ::: D08-1059_94:188

	For neighbouring parse decisions, we extend the work of McDonald and Pereira (2006) and show that modeling vertical neighbour hoods makes parsing intractable in addition to modeling horizontal neighbour hoods. ::: W07-2216_39:259

	An existing method to combine multiple parsing algorithms is the ensemble approach (Sagae and Lavie, 2006a), which was reported to be useful in improving dependency parsing (Hall et al., 2007). ::: D08-1059_176:188

	Finally, in contrast to the results reported by Nivre and Nilsson (2005), simply projectivizing the training data (without using an inverse transformation) is not beneficial at all, except possibly for Alpino. ::: P07-1122_113:184

	Determining constraints with dependency parser combination Parser combination has been shown to be a powerful way to obtain very high accuracy in dependency parsing (Sagae and Lavie, 2006). ::: P07-1079_116:260

	Buchholz and Marsi (2006) report that [f]or most parsers, their ranking differs at most a few places from their overall ranking. ::: D07-1096_359:410

	We also implemented an averaged perceptron system (Collins, 2002) (another online learning algorithm) for comparison. ::: P05-1012_142:209

E02	Labeled attachment score (LAS): The proportion of words that are assigned the correct head and dependency type (or no head if the word is a root) (Nivre et al. , 2004). ::: C04-1010_104:293

M02	Nivres parser has been tested for Swedish (Nivre et al. , 2004), English (Nivre and Scholz, 2004), Czech (Nivre and Nilsson, 2005), Bulgarian (Marinov and Nivre, 2005) and Chinese Cheng et al. ::: W06-2920_46:416

	The first-stage 50-best parser The first stage of our parser is the lexicalized probabilistic context-free parser described in (Charniak, 2000) and (Charniak and Johnson, 2005). ::: N06-1020_51:208

	We implemented the tournament model, the CC algorithm (Kudo and Matsumoto, 2002), SR algorithm (Sassano, 2004) and CLE algorithm (McDonald et al., 2005) with SVM classifiers. ::: C08-1046_119:234

M16	The search for the best parse can then be formalized as the search for the maximum spanning tree (MST) (McDonald et al. , 2005b). ::: W06-2920_236:416

	For dependency parsing domain, McDonald et al (2005a) modified the MIRA learning algorithm (McDonald et al. , 2005a) for structured domains in which the optimization problem can be solved by using Hidreths algorithm (Censor and Zenios, 1997), which is faster than the quadratic programming technique. ::: D07-1126_43:136

	For English, we used the Penn Treebank (Marcus et al., 1993) in our experiments and the tool Penn2Malt3 to convert the data into dependency structures using a standard set of head rules (Yamada and Matsumoto, 2003a). ::: C10-2015_132:184

	At test time, each input sentence is parsed using each of the three LR models, and the three resulting dependency structures are combined according to the maximum-spanning-tree parser combination scheme6 (Sagae and Lavie, 2006a) where each dependency proposed by each of the models has the same weight (it is possible that one of the more sophisticated weighting schemes proposed by Sagae and Lavie may be more effective, but these were not attempted). ::: D07-1111_69:130

	The state-ofthe-art accuracy of Chinese POS tagging is about 93.5%, which is much lower than that of English (about 97% (Collins, 2002)). ::: D11-1109_16:271

	Research on dependency parsing is mainly based on machine learning methods, which can be called history-based (Yamada and Matsumoto, 2003; Nivre et al. , 2006) and discriminative learning methods (McDonald et al. , 2005a; Corston-Oliver et al. , 2006). ::: D07-1126_7:136

S01	However, unrestricted global inference for graph-based dependency parsing is NP-hard, and graph-based parsers like MSTParser therefore limit the scope of their features to a small number of adjacent arcs (usually two) and/or resort to approximate inference (McDonald and Pereira, 2006). ::: C10-2013_82:211

M24	An alternative to this approach is to use transition based parsing (Yamada and Matsumoto, 2003; Nivre and Nilsson, 2005; Attardi, 2006; Nivre, 2009; Gomez-Rodrguez and Nivre, 2010), where there is an incremental processing of a string with a model that scores transitions between parser states, conditioned on the parse history. ::: D11-1114_8:247

M09	Support vector machines for mapping histories to parser actions (Kudo and Matsumoto, 2002). ::: W06-2933_8:84

	Previous studies (McDonald and Pereira, 2006; Yamada and Matsumoto, 2003; Zhang and Clark, 2008) show that the accuracies of complete trees are about 40% for English and about 35% for Chinese, while the accuracies of relations between two words are much higher: about 90% for English and about 85% for Chinese. ::: D09-1060_21:253

	Nivre and Scholz (2004) uses this term with reference to Yamada and Matsumoto (2003), whose parser has to find all children of a token before it can attach that token to its head. ::: W06-2920_155:416

	Table 2 shows the dependency accuracy, root accuracy and complete match scores for our best parser (Model 2 with label set B) in comparison with Collins (1997) (Model 3), Charniak (2000), and Yamada and Matsumoto (2003).5 It is clear that, with respect to unlabeled accuracy, our parser does not quite reach state-of-the-art performance, even if we limit the competition to deterministic methods such as that of Yamada and Matsumoto (2003). ::: C04-1010_120:293

E02	More precisely, parsing accuracy is measured by the attachment score, which is a standard measure used in studies of dependency parsing (Eisner, 1996; Collins et al. , 1999). ::: W04-2407_109:153

	Stage 1: Unlabeled Parsing The first stage of our system creates an unlabeled parse y for an input sentence x. This system is primarily based on the parsing models described by McDonald and Pereira (2006). ::: W06-2932_20:130

	This has been found to work well in previous work on dependency parser combination (Zeman and Zabokrtsky, 2005; Sagae and Lavie, 2006). ::: P07-1079_120:260

	The structured large margin approach, on the other hand, uses a global scoring function by minimizing a training loss the structured margin loss (McDonald et al. , 2005) which is directly coordinated with the global tree. ::: N07-3002_30:74

M37	Though some of the parsing algorithms are language independent and show state-of-the-art performance on multilingual dependency Treebanks (Nivre et al., 2007; Buchholz and Marsi, 2006), they are often too slow for online purpose. ::: C08-2034_9:141

	The edge model was developed on development data from the English Penn-III treebank (Marcus et al., 1993), and we evaluate on Sect. ::: W11-1109_164:202

	Note that the best-performing stacked configuration for each and every language outperforms MST2O, corroborating results reported by Nivre and McDonald (2008). ::: D08-1017_175:217

	The usage of reductions list is identical to Eisner (1996a) and readers may refer to it for further details. ::: D11-1137_68:241

V01	German was included in the CoNLL shared tasks in 2006 (Multilingual Dependency Parsing, (Buchholz and Marsi, 2006)) and in 2009 (Syntactic and Semantic Dependencies in Multiple Languages, (Hajic et al., 2009)) with data based on the TIGER 1123 corpus (Brants et al., 2002) in both cases. ::: C10-2129_38:191

	Note that the (incorrect) alignment between heeft and You will not be pursued because it would lead to heeft being a dependent of itself and thus violating the wellformed3I.e., single headedness and acyclicity; we do not require the trees to be projective, but instead train pseudo-projective models (Nivre and Nilsson, 2005) on the projected data (cf. ::: W09-1104_90:228

M09,M10	To assign probabilities to these actions, previous work has proposed memory-based classifiers (Nivre et al., 2004), SVMs (Nivre et al., 2006b), and Incremental Sigmoid Belief Networks (ISBN) (Titov and Henderson, 2007b). ::: P11-2003_11:127

S01	We choose the 2nd-ordered MST model (McDonald and Pereira, 2006) as the baseline. ::: P10-1002_88:187

	Models similar to model 2 have been found to work well for datasets with a rich annotation of dependency types, such as the Swedish dependency treebank derived from Einarsson (1976), where the extra part-of-speech features are largely redundant (Nivre et al. , 2004). ::: C04-1010_79:293

	To learn these structures we used online large-margin learning (McDonald et al. , 2005) that empirically provides state-of-the-art performance for Czech. ::: H05-1066_214:223

M24,M03,S01	We include in the table results from the pure transition-based parser of Zhang and Clark (2008) (row Z&C08 transition), the dynamic-programming arc-standard parser of Huang and Sagae (2010) (row H&S10), and graphbased models including MSTParser (McDonald and Pereira, 2006), the baseline feature parser of Koo et al. ::: P11-2033_78:103

	However, this algorithm cannot be generalized to the second-order setting McDonald and Pereira (2006) proved that this problem is NPhard, and described an approximate greedy search algorithm. ::: W08-2123_34:149

	Another example is the second-order approximate spanning tree parser developed by McDonald and Pereira (2006). ::: P07-1122_38:184

	Subsequently, researchers have begun to look at both porting these parsers to new domains (Gildea, 2001; McClosky et al., 2006; Petrov et al., 2010) and constructing parsers for new languages (Collins et al., 1999; Buchholz and Marsi, 2006; Nivre et al., 2007). ::: D11-1006_10:303

	In computational linguistics, dependency based syntactic representations have in recent years been used primarily in data-driven models, which learn to produce dependency structures for sentences solely from an annotated corpus, as in the work of Eisner (1996), Yamada and Matsumoto (2003), Nivre, Hall, and Nilsson (2004), and McDonald, Crammer, and Pereira (2005), among others. ::: J08-4003_47:595

	Currently, the work on conditional parsing models appears to have culminated in large margin training approaches (Taskar et al. , 2004; McDonald et al. , 2005), which demonstrates the state of the art performance in English dependency parsing. ::: N07-3002_11:74

	It is also worth noting that the parsing units in this treebank are in many cases larger than conventional sentences, which partly explains the high average number of tokens per sentence (Buchholz and Marsi, 2006). ::: D07-1096_96:410

	The fact that our model defines a probability model over parse trees, unlike the previous stateof-the-art methods (Nivre et al. , 2006; McDonald et al. , 2006), makes it easier to use this model in applications which require probability estimates, such as in language processing pipelines or for language modeling. ::: D07-1099_88:90

M13,M27	Related Work Data-driven dependency parsing using supervised machine learning was pioneered by Eisner (1996), who showed how traditional chart parsing techniques could be adapted for dependency parsing to give efcient parsing with exact inference over a probabilistic model where the score of a dependency tree is the sum of the scores of individual arcs. ::: J08-4003_559:595

	It should be noted that unlike Model 1, Model 2 produces grand-sibling parts only for the outermost pair of grandchildren,4 similar to the behavior of the Carreras (2007) parser. ::: P10-1001_119:237

	This is a very simple approach, but provided significant performance improvements comparing with the stateof-the-art supervised dependency parsers such as (McDonald and Pereira, 2006). ::: D09-1058_13:216

	In fact, the resemblance is more than passing, as Model 2 can emulate the Carreras (2007) algorithm by demoting each third-order part into a second-order part: SCOREGS(x,g,h,m,s) = SCOREG(x,g,h,m) SCORETS(x,h,m,s,t) = SCORES(x,h,m,s) where SCOREG, SCORES, SCOREGS and SCORETS are the scoring functions for grandchildren, siblings, grand-siblings and tri-siblings, respectively. ::: P10-1001_120:237

	Using stacking with rich features, we obtain results competitive with Nivre and McDonald (2008) while preserving the fast quadratic parsing time of arcfactored spanning tree algorithms. ::: D08-1017_57:217

P38	McDonald and Pereira (2006) consider a method for recovering non-projective attachments from a graph representation of a sentence, in which an optimal projective parse tree has been identified. ::: D11-1113_26:193

	System LAS Nivre et al. 2007b (MaltParser, combined) 76.94% Carreras, 2007 75.75% Titov and Henderson, 2007 75.49% C o N L L 07 Hall et al., 2007 (MaltParser (single parser) + pseudo projective transformation) 74.99% BDT I MaltParser (single parser) 74.52% BDT I size 74.83% BDT II MaltParser (single parser) Baseline 77.08% Table 1. ::: W10-1404_152:207

V01	An important part of this research effort are the CoNLL 2006 and 2007 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007), which allowed for a comparison of many algorithms and approaches for this task on many languages. ::: N10-1115_10:202

	But it keeps all the parsing histories of a head, which is different from only keeping adjacent two modifiers in (McDonald and Pereira, 2006). ::: C08-1132_145:161

V01	We find that partial correspondence projection gives rise to parsers that outperform parsers trained on aggressively filtered data sets, and achieve unlabeled attachment scores that are only 5% behind the average UAS for Dutch in the CoNLL-X Shared Task on supervised parsing (Buchholz and Marsi, 2006). ::: W09-1104_5:228

	The lower-order feature mappings fdep, fsib, and fgch are based on feature sets from previous work (McDonald et al., 2005a; McDonald and Pereira, 2006; Carreras, 2007), to which we added lexicalized versions of several features. ::: P10-1001_174:237

S02,S01	The dependency parsers that we compare are the deterministic shift-reduce MaltParser (Nivre et al., 2007) and the second-order minimum spanning tree algorithm based MstParser (McDonald et al., 2006). ::: D10-1069_52:206

M34,M21	The approach of deterministic classier-based parsing was rst proposed for Japanese by Kudo and Matsumoto (2002) and for English by Yamada and Matsumoto (2003). ::: J08-4003_564:595

V01,M21	CoNLL-X shared task (Buchholz and Marsi, 2006)wasa large-scale evaluation of data-driven dependency parsers, with data from 13 different languages and 19 participating systems. ::: D07-1013_76:290

S01	They used a method of combining several parsers outputs in the framework of MST parsing (Sagae and Lavie, 2006). ::: I08-2097_28:160

	We combined this parsing algorithm with the passive-aggressive perceptron algorithm (Crammer et al., 2003; McDonald et al., 2005; Crammer et al., 2006). ::: C10-1011_19:243

	We compare the performance of our forest reranker against n-best reranking on the Penn English Treebank (Marcus et al., 1993). ::: P08-1067_123:181

	For example, it must not contain dependency chains such as en kom ik en . For a more formal definition see previous work (Nivre et al. , 2004). ::: W06-1616_37:238

	The second-order model of McDonald and Pereira (2006) incorporates sibling parts and also needs O(n3) parsing time. ::: D11-1109_66:271

	Notice that the strategy just described to handle sibling features is not fully compatible with the features proposed by Eisner (1996) for projective parsing, as the latter correlate only consecutive siblings and are also able to place special features on the first child of a given word. ::: P09-1039_142:228

S01	Preliminary experiments using a different dependency parser MSTParser (McDonald et al., 2005) resulted in similar empirical observations. ::: D11-1006_74:303

	Specifically, these approaches considered sibling relations of the modifier token (Eisner, 1996; McDonald and Pereira, 2006). ::: D07-1101_13:204

M01,M12	There has been extensive work on data-driven dependency parsing for both projective parsing (Eisner, 1996; Paskin, 2001; Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; McDonald et al. , 2005a) and non-projective parsing systems (Nivre and Nilsson, 2005; Hall and Novak, 2005; McDonald et al. , 2005b). ::: W07-2216_41:259

M24	Table 4 compares our baseline against the state-of-the-art graph-based (McDonald et al., 2005) and transition-based (Zhang and Clark, 2008) approaches, and confirms that our system performs at the same level with those stateof-the-art, and runs extremely fast in the deterministic mode (k=1), and still quite fast in the beamsearch mode (k=16). ::: D09-1127_148:211

	English main results In our English experiments, we tested eight different parsing configurations, representing all possible choices between baseline or cluster-based feature sets, first-order (Eisner, 2000) or second-order (Carreras, 2007) factorizations, and labeled or unlabeled parsing. ::: P08-1068_115:218

	Introduce through post-processing, e.g. through reattachment rules (Bick, 2006) or if the change increases overall parse tree probability (McDonald et al. , 2006). ::: W06-2920_262:416

M37	In 2006 the shared task was multilingual dependency parsing, where participants had to train a single parser on data from thirteen different languages, which enabled a comparison not only of parsing and learning methods, but also of the performance that can be achieved for different languages (Buchholz and Marsi, 2006). ::: D07-1096_8:410 

P38	Following (Nivre et al. , 2006), the encoding scheme called HEAD in (Nivre and Nilsson, 2005) was used to encode the original non-projective dependencies in the labels of the projectivized dependency tree. ::: D07-1099_14:90

	The second order parsing algorithm of McDonald and Pereira (2006) uses a separate algorithm for edge labeling. ::: C10-1011_35:243

	For projective parsing, it is significantly faster than exact dynamic programming, at the cost of small amounts of search error, We are interested in extending these ideas to phrase-structure and lattice parsing, and in trying other higher-order features, such as those used in parse reranking (Charniak and Johnson, 2005; Huang, 2008) and history-based parsing (Nivre and McDonald, 2008). ::: D08-1016_354:363

	Perceptron 82.9 88.0 30.3 (inc punc) MIRA 83.3 88.6 31.3 Bayes Point Machine 84.0 88.8 30.9 Table 3: Comparison to previous best published results reported in (McDonald et al. , 2005a). ::: N06-1021_165:189

	Refer to (McDonald et al. , 2005b) for a detailed treatment of both algorithms. ::: N06-1021_67:189

M01,M24	According to (McDonald and Nivre, 2007), all data-driven models for dependency parsing that have been proposed in recent years can be described as either graph-based or transition-based. ::: P09-1007_88:206

V01	We use the CoNLL-X data format for dependency trees (Buchholz and Marsi, 2006) to encode partial structures. ::: W09-1104_122:228

M24	Previous work showed that rich features over a wide range of decision history can lead to significant improvements in accuracy for transition-based models (Yamada and Matsumoto, 2003a; Nivre et al., 2004). ::: C10-2015_10:184

	More precisely, dependency arcs (or pairs of arcs) are first represented by a high dimensional feature vector f(i,j,l) Rk, where f is typically a binary feature vector over properties of the arc as well as the surrounding input (McDonald et al., 2005a; McDonald et al., 2006). ::: P08-1108_79:172

V01	We use five datasets from the CONLL-X Shared Task (Buchholz and Marsi, 2006).1 Lemmas and morphological features (FEATS) are ignored, since we only add POS and CPOS tags to unlabeled data. ::: C10-1120_139:233

	Parsing experiments 5.1 Data and setup We used the standard partitions of the Wall Street Journal Penn Treebank (Marcus et al. , 1993); i.e., sections 2-21 for training, section 22 for development and section 23 for evaluation. ::: W07-2217_170:259

	The reason may be that shorter dependencies are often modifier of nouns such as determiners or adjectives or pronouns modifying their direct neighbors, while longer dependencies typically represent modifiers of the root or the main verb in a sentence(McDonald and Nivre, 2007). ::: I08-1012_156:180

S01	Then, we present results using the graph-based parser MSTParser (McDonald and Pereira, 2006), again with default settings, to test the methods across parsers. ::: P10-1075_141:213

	The pchemtb-closed shared task (Marcus et al. , 1993; Johansson and Nugues, 2007; Kulick et al. , 2004) is used to illustrate our models. ::: D07-1126_116:136

M23	The resulting algorithm is projective, and nonprojectivity is handled by pseudo-projective transformations as described in (Nivre and Nilsson, 2005). ::: D07-1111_28:130

S01,S02	This was shown to be the case for both MaltParser (Nivre et al., 2007c) and MST (McDonald et al., 2005), two of the best performing parsers on the whole. ::: W10-1401_77:252

	Despite the realization that maximum margin training is closely related to maximum conditional likelihood for conditional models (McDonald et al. , 2005), a suf ciently uni ed view has not yet been achieved that permits the easy exchange of improvements between the probabilistic and nonprobabilistic approaches. ::: N07-3002_12:74

	In contrast to previous work where this was constrained to sibling relations of the dependent (McDonald and Pereira, 2006), here head-grandchild relations can be taken into account. ::: D07-1096_275:410

	But the lack of corpora has led to a situation where much of the current work on parsing is performed on a single domain using training data from that domain the Wall Street Journal (WSJ) section of the Penn Treebank (Marcus et al. , 1993). ::: P06-1043_14:213

	We employ, as a basis for our parser, the second order maximum spanning tree dependency parsing algorithm of Carreras (2007). ::: C10-1011_17:243

S01,S02,M13,M33	Finally, our work is similar to the comparison of the chart-based MSTParser (McDonald et al., 2005) and shift-reduce MaltParser (Nivre et al., 2006) for dependency parsing. ::: P11-1069_191:201

	This enables search by either minimum spanning tree (West, 2001) or by Eisners (1996) projective parser. ::: C10-1007_32:238

	Nivre and McDonald (2008) explore a parser stacking approach in which the output of one parser is fed as an input to a different kind of parser. ::: W10-2927_14:238

	Related work Our graph-based parser is derived from the work of McDonald and Pereira (2006). ::: D08-1059_162:188

P38	The verb krijg is incorrectly coordinated with the preposition om . work is ef cient and has also been extended to non-projective trees (McDonald et al. , 2005b). ::: W06-1616_47:238

	Bilexical statistics (Eisner 1996), as represented by the maximal context of the P L w and P R w parameters, serve as a proxy for such semantic preferences, where the actual modifier word (as opposed to, say, merely its part of speech) indicates the particular semantics of its head. ::: J04-4004_431:602

V01,M37	The CoNLL shared tasks on multilingual dependency parsing in 2006 and 2007 (Buchholz and Marsi, 2006; Nivre et al., 2007a) demonstrated that dependency parsing for MRLs is quite challenging. ::: W10-1401_72:252

	Kudo and Matsumoto (2002) applied the 361 cascaded chunking algorithm (hereafter CC algorithm) to Japanese dependency parsing. ::: C08-1046_28:234

	Freund and Schapire (1999) discuss how the theory for classification problems can be extended to deal with both of these questions; Collins (2002) describes how these results apply to NLP problems. ::: P04-1015_72:297

	We have successfully replicated the state-of-the-art results for dependency parsing (McDonald et al. , 2005a) for both Czech and English, using Bayes Point Machines. ::: N06-1021_183:189

M17	Given this assumption, the parsing problem reduces to find Y = arg max Y(X) score(Y|X) (1) = arg max Y(X) summation display (xixj)Y score(xi xj) where the score(xi xj) can depend on any measurable property of xi and xj within the sentence X. This formulation is sufficiently general to capture most dependency parsing models, including probabilistic dependency models (Eisner, 1996; Wang et al., 2005) as well as non-probabilistic models (McDonald et al., 2005a). ::: P08-1061_57:148

	Such models are commonly referred to as edge-factored since their parameters factor relative to individual edges of the graph (Paskin, 2001; McDonald et al. , 2005a). ::: W07-2216_26:259

	In the probabilistic LR model, probabilities are assigned to tree 696 Precision Recall F-score Time (min) Best-First Classifier-Based (this paper) 88.1 87.8 87.9 17 Deterministic (MaxEnt) (this paper) 85.4 84.8 85.1 < 1 Charniak & Johnson (2005) 91.3 90.6 91.0 Unk Bod (2003) 90.8 90.7 90.7 145* Charniak (2000) 89.5 89.6 89.5 23 Collins (1999) 88.3 88.1 88.2 39 Ratnaparkhi (1997) 87.5 86.3 86.9 Unk Tsuruoka & Tsujii (2005): deterministic 86.5 81.2 83.8 < 1* Tsuruoka & Tsujii (2005): search 86.8 85.0 85.9 2* Sagae & Lavie (2005) 86.0 86.1 86.0 11* Table 1: Summary of results on labeled precision and recall of constituents, and time required to parse the test set. ::: P06-2089_141:169

	Johansson and Nugues (2008) reported training times of 2.4 days for English with the high-order parsing algorithm of Carreras (2007). ::: C10-1011_46:243

	The results reported here for English and Czech are comparable to the previous best published numbers in (McDonald et al. , 2005a), as Table 3 shows. ::: N06-1021_141:189

M12	Exhaustive nonprojective dependency parsing with more powerful models is intractable (McDonald and Satta, 2007), and one has to resort to approximation algorithms (McDonald and Pereira, 2006). ::: E09-1055_19:234

M21	Recently, classifier-based dependency parsing (Nivre and Scholz, 2004; Yamada and Matsumoto, 2003) has showed that deterministic parsers are capable of high levels of accuracy, despite great simplicity. ::: P06-2089_10:169

	Sagae and Lavie (2006b) extend this model to alternate between left-to-right and right-to-left passes. ::: N10-1115_175:202

P38	However, just as it has been noted that most non-projective structures appearing in practice are only slightly nonprojective (Nivre and Nilsson, 2005), we characterise a sense in which the structures appearing in treebanks can be viewed as being only slightly ill-nested. ::: E09-1034_127:178

	Discriminative classiers to map histories to parser actions (Kudo and Matsumoto 2002; Yamada and Matsumoto 2003; Nivre,Hall,and Nilsson 2004) A system of this kind employs no grammar but relies completely on inductive learning from treebank data for the analysis of new sentences,and on deterministic parsing for disambiguation. ::: J08-3003_217:443

M37,V01,S01	In the multilingual track of the CoNLL 2007 shared task on dependency parsing, a single parser must be trained to handle data from ten different languages: Arabic (Hajic et al. , 2004), Basque (Aduriz et al. , 2003), Catalan, (Mart et al. , 2007), Chinese (Chen et al. , 2003), Czech (Bohmova et al. , 2003), English (Marcus et al. , 1993; Johansson and Nugues, 2007), Greek (Prokopidis et al. , 2005), Hungarian (Csendes et al. , 2005), Italian (Montemagni et al. , 2003), and Turkish (Oflazer et al. , 2003).1 Our contribution is a study in multilingual parser optimization using the freely available MaltParser system, which performs 1For more information about the task and the data sets, see Nivre et al. ::: D07-1097_8:119

	Even before the 2006 shared task, the parsers of Collins (1997) and Charniak (2000), originally developed for English, had been adapted for dependency parsing of Czech, and the parsing methodology proposed by Kudo and Matsumoto (2002) and Yamada and Matsumoto (2003) had been evaluated on both Japanese and English. ::: D07-1096_55:410

V01	In order to get at least a preliminary answer to this question, we extracted LCFRS productions from the data used in the 2006 CoNLL shared task on data-driven dependency parsing (Buchholz and Marsi, 2006), and evaluated how large a portion of these productions could be binarized using our algorithm. ::: E09-1055_220:234

	Parsing history has been used to improve parsing accuracy by many researchers (Yamada and Matsumoto, 2003; McDonald and Pereira, 2006). ::: C08-1132_141:161

M01	Several efficient, accurate and robust approaches to data-driven dependency parsing have been proposed recently (Nivre and Scholz, 2004; McDonald et al. , 2005; Buchholz and Marsi, 2006) for syntactic analysis of natural language using bilexical dependency relations (Eisner, 1996). ::: P07-1079_5:260

	To reduce the data sparseness problem, we use the back-off strategy proposed in (Eisner, 1996a). ::: D11-1137_154:241

	We argue that bootstrapping a parser is most promising when the model uses a rich set of redundant features, as in recent models for scoring dependency parses (McDonald et al. , 2005). ::: D07-1070_4:458

	Dependency parsers have been tested on parsing sentences in English (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; McDonald et al., 2005) as well as many other languages (Nivre et al., 2007a). ::: D11-1036_6:232

	Charniak (2000) shows the value his parser gains from parent annotation of nodes, suggesting that this information is at least partly complementary to information derivable from lexicalization, and Collins (1999) uses a range of linguistically motivated and carefully hand-engineered subcategorizations to break down wrong context-freedom assumptions of the naive Penn treebank covering PCFG, such as differentiating base NPs from noun phrases with phrasal modifiers, and distinguishing sentences with empty subjects from those where there is an overt subject NP. ::: P03-1054_18:233

	Third-order features of S0 and N0 Higher-order context features have been used by graph-based dependency parsers to improve accuracies (Carreras, 2007; Koo and Collins, 2010). ::: P11-2033_58:103

M23	The pseudo-projective transformation proposed by Nivre and Nilsson (2005) is such an approach, which is compatible with different parser engines. ::: P07-1122_42:184

	We experiment on two popular treebanks, the Wall Street Journal (WSJ) portion of the Penn English Treebank (Marcus et al., 1993), and the Penn Chinese Treebank (CTB) 5.0 (Xue et al., 2005). ::: P10-1002_123:187

P38,P29	Nivre and Nilsson (2005) observe that most non-projective dependency structures appearing in practice are close to being projective, since they contain only a small proportion of nonprojective arcs. ::: E09-1034_12:178

	Dependency trees are usually assumed to be projective (no crossing arcs), which means that if there is an arc a2a5a4a20a19a15a21a27a4a25a24a28a16, then a4a29a19 is an ancestor of all the words between a4a30a19 and a4a25a24 . Let a31a7a2a32a0a33a16 denote the set of all the directed, projective trees that span a0 . From an input sentence a0, one would like to be able to compute the best parse; that is, a projective tree, a18a35a34 a31a7a2a32a0a33a16, that obtains the highest score . In particular, I follow Eisner (1996) and McDonald et al. ::: N07-3002_18:74

	The best performing system (McDonald et al. 2006; note: this system is different to our baseline) achieves 79.2% labelled accuracy while our baseline system achieves 78.6% and our constrained version 79.8%. ::: W06-1616_176:238

	Table 6 contrasts our results with those from Collins (2002). ::: N03-1033_189:202

	Algorithm 2 Parsing algorithm 1: Initialization: V [s,s,dir,I/C] = 0.0s,dir 2: for k = 1 to n do 3: for s = 0 to nk do 4: t = s + k 5: % Create incomplete items 6: Lst=V [s,t,,I]= maxsr<tV I(r); 7: Rst=V [s,t,,I]= maxsr<tV I(r); 8: Calculate Ldfst and Rdfst; 9: % Update the scores of incomplete chart items 10: V [s,t,,I]=L+st=Lst + Ldfst 11: V [s,t,,I]=R+st=Rst + Rdfst 12: % Create complete items 13: V [s,t,,C]= maxsr<tV C(r); 14: V [s,t,,C]= maxs<rtV C(r); 15: end for 16: end for Algorithm 2 is the parsing algorithm with thehistory-based features, whereV [s,t,dir,I/C] refers to the score of chart item [s,t,dir,I/C], V I(r) is a function to search for the optimal sibling and grandchild nodes for the incomplete items (line 6 and 7) (Carreras, 2007) given the 130 splitting point r and return the score of the structure, and V C(r) is a function to search for the optimal grandchild node for the complete items (line 13and14). ::: C10-2015_106:184

	This is done by training a classifier to determine parser actions based on local features that represent the current state of the parser (Nivre and Scholz, 2004; Sagae and Lavie, 2005). ::: P07-1079_59:260

	Dependency parsing has been applied to a fairly broad range of languages, especiallyintheCoNLLsharedtasksin2006and2007 (Buchholz and Marsi, 2006; Nivre et al., 2007). ::: C10-2013_16:211

M26	While large factors are desirable for capturing sophisticated linguistic constraints, they come at the cost of time complexity: for the projective case, adaptations of Eisners algorithm (Eisner, 1996) are O(n3) for 1-edge factors (McDonald et al., 2005) or sibling 2-edge factors (McDonald and Pereira, 2006), and O(n4) for general 2-edge factors (Carreras, 2007) or 3-edge factors (Koo and Collins, 2010). ::: D11-1113_63:193

	In the second approach, combination of 3Downloadedfromhttp://source forge.net/projects/mstparser the three dependency parsers is done according to the maximum spanning tree combination scheme of Sagae and Lavie (2006), which results in high accuracy of surface dependencies. ::: P07-1079_126:260

M23	For handling non-projective relations, Nivre and Nilsson (2005) suggested applying a preprocessing step to a dependency parser, which consists in lifting non-projective arcs to their head repeatedly, until the tree becomes pseudo-projective. ::: D07-1119_24:133

	The averaged perceptron (Collins, 2002) is a variant which averages the w across all iterations; it has demonstrated good generalization especially with data that is not linearly separable, as in many natural language processing problems. ::: N06-1021_77:189

	Compared with the algorithm of Carreras (2007) that only considers the furthest children of s and t, Algorithm 2 considers all the children. ::: C10-2015_125:184

P38,M01	To avoid too small training sets, we pool together categories that have a frequency below a certain threshold t. 2.4 Pseudo-Projective Parsing Pseudo-projective parsing was proposed by Nivre and Nilsson (2005) as a way of dealing with non-projective structures in a projective data-driven parser. ::: W06-2933_31:84

M22	Decision Trees(Haruno et al. , 1998) and Maximum Entropy models(Ratnaparkhi, 1997; Uchimoto et al. , 1999; Charniak, 2000) have been applied to dependency or syntactic structure analysis. ::: W00-1303_17:181

	For example, it would be easy to enforce such constraints in the Eisner (1996) algorithm or using Integer Linear Programming approaches (Riedel and Clarke, 2006; Martins et al., 2009). ::: D10-1069_85:206

M33	The deterministic shift-reduce parsing algorithm of (Nivre & Scholz, 2004) was used to create two parsers2, one that processes the input sentence from left-to-right (LR), and one that goes from right-toleft (RL). ::: N06-2033_57:93

	Occasionally, in 59 sentences out of 2416 on section 23 of the Wall Street Journal Penn Treebank (Marcus et al. , 1993), the shift-reduce parser fails to attach a node to a head, producing a disconnected graph. ::: N07-1049_112:180

	This permits us to make exact comparisons with the parser of Yamada and Matsumoto (2003), but also the parsers of Collins (1997) and Charniak (2000), which are evaluated on the same data set in Yamada and Matsumoto (2003). ::: C04-1010_93:293

	While we expect a longer runtime than using the Chu-Liu-Edmonds as in previous work (McDonald et al. , 2005b), we are interested in how large the increase is. Table 2 shows the average solve time (ST) for sentences with respect to the number of tokens in each sentence for our system with constraints (cnstr) and the Chu-Liu-Edmonds (CLE) algorithm. ::: W06-1616_188:238

M23	The pseudo-projective approach (Nivre and Nilsson, 2005): Transform non-projective training trees to projective ones but encode the information necessary to make the inverse transformation in the DEPREL, so that this inverse transformation can also be carried out on the test trees (Nivre et al. , 2006). ::: W06-2920_263:416

	We used our implementation of the Collins (2002) tagger (with 97.3% accuracy on a standard Penn Treebank test) to perform POS-tagging. ::: P11-2033_68:103

	The comparison reported in this section is similar to the comparison between the chartbased MSTParser (McDonald et al., 2005) and shift reduce MaltParser (Nivre et al., 2006) for dependency parsing. ::: P11-1069_136:201

M37	In 2006, the shared task was multilingual dependency parsing, where participants had to train and test a parser on data from thirteen different languages (Buchholz and Marsi, 2006). ::: W07-2220_6:50

	Also, we chose to average each individual perceptron (Collins, 2002) prior to Bayesian averaging. ::: N06-1021_97:189

	As mentioned previously, the source data is drawn from a corpus of news, specifically the Wall Street Journal section of the Penn Treebank (Marcus et al. , 1993). ::: D07-1096_135:410

	Our main training set consists of Sections 02-21 of the Wall Street Journal portion of the Penn Treebank (Marcus et al., 1993), with Section 22 serving as development set for source domain comparisons. ::: D10-1069_35:206

	We generalize the standard deterministic stepwise framework to probabilistic parsing, with the use of a best-first search strategy similar to the one employed in constituent parsing by Ratnaparkhi (1997) and later by Sagae and Lavie (2006); 3. ::: D07-1111_11:130

S01,S02	In this work we use two well-known, publicly available dependency parsers, MSTParser (McDonald et al., 2005b),1 which implements ex1http://source forge.net/projects/mstparser 159 act first-order arc-factored nonprojective parsing (2.1.2) and approximate second-order nonprojective parsing, and MaltParser (Nivre et al., 2006), which is a state-of-the-art transition-based parser.2 We do not alter the training algorithms used in prior work for learning these two parsers from data. ::: D08-1017_106:217

	In the higher-order models, the parts consist of arcs together with some context, e.g. the parent or the sister arcs (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). ::: P11-2125_17:98

	To facilitate comparisons with previous work (McDonald et al., 2005b; McDonald and Pereira, 2006), we used the training/development/test partition defined in the corpus and we also used the automatically-assigned part of speech tags provided in the corpus.10 Czech word clusters were derived from the raw text section of the PDT 1.0, which contains about 39 million words of newswire text.11 We trained the parsers using the averaged perceptron (Freund and Schapire, 1999; Collins, 2002), which represents a balance between strong performance and fast training times. ::: P08-1068_96:218

	They will be based solely on different dependency tree configurations (see Figure 5), similarly to (Nivre and McDonald, 2008; Martins et al., 2008). ::: W10-1404_136:207

P38	Different ways have been proposed to deal with non-projectivity (Nivre and Nilsson, 2005; McDonald et al., 2005; McDonald and Pereira, 2006; Nivre, 2009). ::: W10-1401_207:252

	In addition, in English parsing we ignore the parent-predictions of punctuation tokens,13 and in Czech parsing we retain the punctuation tokens; this matches previous work (Yamada and Matsumoto, 2003; McDonald et al., 2005a; McDonald and Pereira, 2006). ::: P08-1068_114:218

S01,S02	The behavior of MSTParser and Malt Pars erin this respect is consistent with the results of McDonald and Nivre (2007). ::: C10-2013_193:211

	The labeled attachment scores for the ISBN with tuned features (TF) and local features (LF) and ISBN with tuned features and no edges connecting latent variable vectors (TF-NA) are presented in table 1, along with results for the MALT parser both with tuned and local feature, the MST parser (McDonald et al. , 2006), and the average score (Aver) across all systems in the CoNLL-X shared task. ::: W07-2218_213:279

M12	For higher-order non-projective parsing, however, computational complexity results (McDonald and Pereira, 2006; McDonald and Satta, 2007) indicate that exact solutions to the three inference problems of Section 2.2 will be intractable. ::: D07-1015_181:443

	This can be seen in state-of-the-art constituency-based parsers such as Collins (1999), Charniak (2000), and Petrovetal.(2006),and the effects of different transformations have been studied by Johnson (1998), KleinandManning(2003),andBikel(2004). ::: P07-1122_7:184

	We then discuss the application of pseudo-projective transformations (Nivre and Nilsson, 2005) and an additional arc-reversing transform to dependency DAGs. ::: C08-1095_38:163

	This approach corresponds to the training problem posed in (McDonald et al., 2005a) and has yielded the best published results for English dependency parsing. ::: P08-1061_65:148

	We trained on the standard Penn Treebank WSJ corpus (Marcus et al., 1993). ::: N09-1012_177:217

	Supervised learning algorithms still represent the state of the art approach for inferring dependency parsers from data (McDonald et al., 2005a; McDonald and Pereira, 2006; Wang et al., 2007). ::: P08-1061_6:148

	This feature was particularly helpful for nouns identifying their parent (McDonald et al. , 2005a). ::: D07-1126_78:136

	We follow the edge based factorization method of Eisner (Eisner, 1996) and define the score of a dependency tree as the sum of the score of all edges in the tree, s(x,y) = summation display (i,j)y s(i,j) = summation display (i,j)y w (i,j) (1) where (i,j) is a high-dimensional binary feature representation of the edge from xwi to xwj . For example in Figure 1, we can present an example (i,j) as follows; (i,j) = brace left Bigg 1if xwi =prime hitprimeandxwj =prime ballprime 0otherwise The basic question must be answered for models of this form: how to find the dependency tree y with the highest score for sentence x? ::: D07-1126_34:136

	A first-order (or edge-factored) parsing model (McDonald et al., 2005) contains only LINK factors, along with a global TREE or PTREE factor. ::: D08-1016_91:363

	Supervised dependency parsing achieves the stateof-the-art in recent years (McDonald et al., 2005a; McDonald and Pereira, 2006; Nivre et al., 2006). ::: P10-1002_6:187

	In the domain adaptation track, participants were provided with English training data from the Wall Street Journal portion of the Penn Treebank (Marcus et al. , 1993) converted to dependencies (Johansson and Nugues, 2007) to train parsers to be evaluated on material in the biological (development set) and chemical (test set) domains (Kulick et al. , 2004), and optionally on text from the CHILDES database (MacWhinney, 2000; Brown, 1973). ::: D07-1111_18:130

	For regularization purposes we adopt an average perceptron (Collins, 2002) which returns for each y, y = 1T summationtextTt=1 ty, the average of all weight vectors ty posited during training. ::: N07-1049_95:180

V01	One of the reasons are CoNLL shared tasks for syntactic dependency parsing in the years 2006, 2007 (Buchholz and Marsi, 2006; Nivre et al., 2007) and the CoNLL shared task for joint parsing of syntactic and semantic dependencies in the year 2008 and of cause this shared task in 2009, cf. ::: W09-1210_10:135

	Method # Step Time[s] # Example # Feature Tournament 26396 371 374579 56165 SR algorithm (Sassano, 2004) 15641 80 94669 37183 CC algorithm (Kudo and Matsumoto, 2002) 18922 99 112759 37183 Table 3: Parsing time and the size of the training examples. ::: C08-1046_197:234

	Although both parser models show a similar accuracy, McDonald and Nivre (2007) demonstrate that the two types of models exhibit different behaviors. ::: W11-0314_67:167

E02	Thus, table 4 shows labeled attachment scores, the main evaluation metric used in the shared task, in comparison to the two highest scoring systems from the original evaluation (McDonald et al. , 2006; Nivre et al. , 2006). ::: N07-1050_130:146

	Approximate algorithms have been employed to handle models that are not arc-factored (although features are still fairly local): McDonald and Pereira (2006) adopted an approximation based on O(n3) projective parsing followed by a hill climbing algorithm to rearrange arcs, and Smith and Eisner (2008) proposed an algorithm based on loopy belief propagation. ::: P09-1039_87:228

	These results show that the discriminative spanning tree parsing framework (McDonald et al. , 2005b; McDonald and Pereira, 2006) is easily adapted across all these languages. ::: W06-2932_71:130

S01,S02,M24	Supported by the Lynn and William Frankel Center for Computer Sciences, Ben Gurion University Current dependency parsers can be categorized into three families: local-and-greedy transition based parsers (e.g., MALTPARSER (Nivre et al., 2006)), globally optimized graph-based parsers (e.g., MSTPARSER (McDonald et al., 2005)), and hybrid systems (e.g., (Sagae and Lavie, 2006b; Nivre and McDonald, 2008)), which combine the output of various parsers into a new and improved parse, and which are orthogonal to our approach. ::: N10-1115_11:202

	A solution that leverages the complementary strengths of these two approaches described in detail by McDonald and Nivre (2007)was recently and successfully explored by Nivre and McDonald (2008). ::: D08-1017_12:217

V01	For comparison, we also include the results of the two top scoring systems in the CoNLL-X shared task, those of McDonald, Lerman, and Pereira (2006) and Nivre et al. ::: J08-4003_441:595

M03	Alternatively, discriminative parsing is tractable with exact and efficient search based on dynamic programming (DP) if all features are restricted to be local, that is, only looking at a local window within the factored search space (Taskar et al., 2004; McDonald et al., 2005). ::: P08-1067_16:181

	The second order model of Carreras (2007) incorporates both sibling and grandparent parts, and needs O(n4) parsing time. ::: D11-1109_67:271

	We follow the edge based factorization method of Eisner (1996) and define the score of a dependency tree as the sum of the score of all edges in the tree, s(x,y) = summation display (i,j)y s(i,j) = summation display (i,j)y w f(i,j) where f(i,j) is a high-dimensional binary feature representation of the edge from xi to xj. ::: P05-1012_42:209

	Sen T/S Lem CPoS PoS MSF Dep NPT NPS Danish 94 5.2 18.2 no 10 24 47 52 1.0 15.6 Dutch 195 13.3 14.6 yes 13 302 81 26 5.4 36.4 German 700 39.2 17.8 no 52 52 0 46 2.3 27.8 Portuguese 207 9.1 22.8 yes 15 21 146 55 1.3 18.9 Slovene 29 1.5 18.7 yes 11 28 51 25 1.9 22.2 Table 1: Data sets; Tok = number of tokens (*1000); Sen = number of sentences (*1000); T/S = tokens per sentence (mean); Lem = lemmatization present; CPoS = number of coarse-grained part-of-speech tags; PoS = number of (fine-grained) part-of-speech tags; MSF = number of morpho syntactic features (split into atoms); Dep = number of dependency types; NPT = proportion of non-projective dependencies/tokens (%); NPS = proportion of non-projective dependency graphs/sentences (%) The history-based classifier can be trained with any of the available supervised methods for function approximation, butin the experiments below we will rely on SVM, which has previously shown good performance for this kind of task (Kudo and Matsumoto, 2002; Yamada and Matsumoto, 2003). ::: N07-1050_80:146

M33	The deterministic shift/reduce classifier-based dependency parsing approach (Nivre and Scholz, 2004) has been shown to offer state-of-the-art accuracy (Nivre et al. , 2006) with high efficiency due to a greedy search strategy. ::: P07-1079_17:260

	To match previous work (McDonald et al., 2005; McDonald and Pereira, 2006; Koo et al., 2008), we split the data into a training set (sections 2-21), a development set (Section 22), and a test set (section 23). ::: D09-1060_116:253

	Blended parser is an ensemble system based on the methodology proposed by Sagae and Lavie (2006). ::: D07-1097_71:119

V01	Most recent English dependency parsers produce one of three sets of dependency types: unlabeled, some variant of the coarse labels used by the CoNLL dependency parsing shared-tasks (Buchholz and Marsi, 2006; Nivre et al., 2007) (e.g., ADV, NMOD, PMOD), or Stanfords dependency labels (de Marneffe and Manning, 2008). ::: D11-1116_31:208

	Nivre and Scholz (2004) proposed a variant of the model of Yamada and Matsumoto that reduces the complexity, from the worst case quadratic to linear. ::: W07-2217_43:259

M06,M03	If projectivity (no crossing branches) is desired, Eisners (1996) dynamic programming algorithm (similar to CYK) for dependency parsing can be used instead. ::: N06-2033_28:93

V01,M01	As evident from the CoNLL-X shared task on dependency parsing (Buchholz and Marsi, 2006), there are currently two dominant models for data-driven dependency parsing. ::: D07-1013_14:290

	We reimplemented Eisners decoder (Eisner, 1996), which searches among all projective parse trees, and the Chu-Liu-Edmonds decoder (Chu and Liu, 1965; Edmonds, 1967), which searches in the space of both projective and non-projective parses. ::: N06-1021_62:189

	It consists of the second order parsing algorithm of Carreras (2007), the non-projective approximation algorithm (McDonald and Pereira, 2006), the passive aggressive support vector machine, and a feature extraction component. ::: C10-1011_49:243

	The English sentences are then parsed by an implementation of 2nd-ordered MST model of McDonald and Pereira (2006), which is trained on dependency trees extracted from WSJ. ::: P10-1002_151:187

M23	We used the pseudo-projective transformation introduced in (Nivre and Nilsson, 2005) to cast non-projective parsing tasks as projective. ::: D07-1099_13:90

	The data partition and head rules were chosen to match previous work (Yamada and Matsumoto, 2003; McDonald et al., 2005a; McDonald and Pereira, 2006). ::: P08-1068_94:218

	We obtain k-best list sand forests generated from the baseline discriminative model which has the same feature set as provided in (McDonald et al., 2005), using the second order Eisner algorithms. ::: D11-1137_150:241

	All non-projective dependencies in the training and gold sets were projectivized prior to training and parsing using the algorithm of Nivre and Nilsson (2005). ::: D11-1036_178:232

E02	All systems are evaluated using unlabeled attachment score (UAS), which is the percentage of words (ignoring punctuation tokens) in a corpus that modify the correct head (Buchholz and Marsi, 2006). ::: D11-1006_75:303

M23	To simplify implementation, we instead opted for the pseudo-projective approach (Nivre and Nilsson, 2005), in which nonprojective links are lifted upwards in the tree to achieve projectivity, and special trace labels are used to enable recovery of the nonprojective links at parse time. ::: W08-2123_35:149

M16	That work extends the maximum spanning tree dependency parsing framework (McDonald et al. , 2005a; McDonald et al. , 2005b) to incorporate features over multiple edges in the dependency graph. ::: W06-2932_21:130

V01	Any details concerning the conversion from the original formats of the various treebanks to the CoNLLformat,a pure dependency based format,are found in documentation referred to in Buchholz and Marsi (2006). ::: P07-1122_79:184

	Similarly to Carreras (2007), we see that these features have a very large impact on parsing accuracy, but also that the parser pays dearly in terms of efficiency as the search complexity increases fromO(n3)toO(n4). ::: W08-2123_127:149

	An example of a stacked feature is a binary feature f2,a(x,g(x)) that fires if and only if the arc a was predicted by g, i.e., if aAg(x); such a feature was used by Nivre and McDonald (2008). ::: D08-1017_122:217

P38,S02	The corresponding unlabeled figures are 73.3 and 33.4.3 This confirms the results of previous studies showing that the pseudo-projective parsing technique used by MaltParser tends to give high precision given that non-projective dependencies are among the most difficult to parse correctly but rather low recall (McDonald and Nivre, 2007). ::: C08-1081_114:137

S02	MST McDonald and Pereira (2006)s dependency parser,1 based on the Eisner algorithm for projective dependency parsing (Eisner, 1996) with the second order factorization. ::: P08-1006_29:181

	Non-local information, such as arity (or valency) and neighbouring dependencies, can be crucial to obtaining high parsing accuracies (Klein and Manning, 2002; McDonald and Pereira, 2006). ::: W07-2216_29:259

M03,M26	Decoding can be carried out using Viterbistyle dynamic-programming algorithms, for example the O(n3) algorithm of Eisner (1996). ::: D07-1015_73:443

	The Perceptron style for natural language processing problems as initially proposed by (Collins, 2002) can provide state of the art results on various domains including text chunking, syntactic parsing, etc. The main drawback of the Perceptron style algorithm is that it does not have a mechanism for attaining the maximize margin of the training data. ::: D07-1126_39:136

M01	Data-driven dependency parsing has been shown to give accurate and efficient parsing for a wide range of languages, such as Japanese (Kudo and Matsumoto, 2002), English (Yamada and Matsumoto, 2003), Swedish (Nivre et al. , 2004), Chinese (Cheng et al. , 2004), and Czech (McDonald et al. , 2005). ::: N07-1050_6:146

M12	For non-projective parsing, we use the NonProjective Approximation Algorithm of McDonald and Pereira (2006). ::: C10-1011_203:243

	Note that it is straightforward to calculate these expected counts using a variant of the inside-outside algorithm (Baker, 1979) applied to the (Eisner, 1996) dependency-parsing data structures (Paskin, 2001) for projective dependency structures, or the matrix-tree theorem (Koo et al., 2007; Smith and Smith, 2007; McDonald and Satta, 2007) for nonprojective dependency structures. ::: D09-1058_62:216

	Let R(T,Tprime) be the Hamming distance between two dependency graphs for an input sentence x = x0x1 xn, R(T,Tprime) = n summation display (i,j)kET I((i,j)k,Tprime) This is a common definition of risk between two graphs as it corresponds directly to labeled dependency parsing accuracy (McDonald et al. , 2005a; Buchholz et al. , 2006). ::: W07-2216_147:259

	This type of model has been used by, among others, Eisner (1996), McDonald et al. ::: P08-1108_13:172

	Most of these features are inspired by previous work in dependency parsing (McDonald et al. , 2005; Collins, 1999). ::: W06-2925_53:112

	Stepwise approaches can use an explicit probability model over next steps, e.g. a generative one (Eisner, 1996; Dreyer et al. , 2006), or train a machine learner to predict those. ::: W06-2920_248:416

M06,M03	The work of Eisner (1996) showed that the argmax problem for digraphs could be solved in O(n3) using a bottom up dynamic programming algorithmsimilartoCKY. ::: W07-2216_127:259

	Nivre and Scholz (2004) developed a history-based learning model. ::: P05-1012_25:209

M24	Transition based parsers typically have a linear or quadratic complexity (Nivre et al., 2004; Attardi, 2006). ::: C10-1011_29:243

V01,M37	The data sets used are taken from the CoNLL-X shared task on multilingual dependency parsing (Buchholz and Marsi 2006). ::: J08-4003_384:595

	Unlabeled attachment score (UAS): The proportion of words that are assigned the correct head (or no head if the word is a root) (Eisner, 1996; Collins et al. , 1999). ::: C04-1010_102:293

S01,S02	In this paper, we employ the MSTParser (McDonald et al., 2006) and MaltParser (Nivre, 2003) for comparison. ::: C08-2034_118:141

V01,M01	Some support for this view can be found in the results from the CoNLL shared tasks on dependency parsing in 2006 and 2007, where a variety of data-driven methods for dependency parsing have been applied with encouraging results to languages of great typological diversity (Buchholz and Marsi, 2006; Nivre et al., 2007a). ::: C08-1081_6:137

	Best-performing constituency parsers like Charniak (2000) and Berkeley (Petrov and Klein, 2007) do outperform our parser, since they consider more information during parsing, but they are at least 5 times slower. ::: P10-1110_195:231

	Following standard practice for higher-order dependency parsing (McDonald and Pereira, 2006; Carreras, 2007), Models 1 and 2 evaluate not only the relevant third-order parts, but also the lower-order parts that are implicit in their third-order factorizations. ::: P10-1001_171:237

	It uses graph transformation to handle non-projective trees (Nivre and Nilsson, 2005). ::: W10-1403_131:262

	A typical approach in graph-based dependency parsing has been to assume a factorized model, where local features are used but a global function is optimized (McDonald et al., 2005b). ::: D08-1017_3:217

	Nivre and McDonald (2008) present an application of stacked learning to dependency parsing, in which a second predictor is trained to improve the performance of the first. ::: W10-1404_118:207

	The problem occurs most often on punctuations (66/84 on WSJ section 23), so it affects only marginally the accuracy scores (UAS, LAS) as computed in the CoNLL-X evaluation (Buchholz & Marsi, 2006). ::: N07-1049_114:180

	We follow McDonald and Nivre (2007) and characterize the errors of the two parsers by sentence and dependency length and dependency type. ::: P11-1069_137:201

V01,M37	We used data from the CoNLL-X shared task on multilingual dependency parsing (Buchholz and Marsi, 2006). ::: D07-1015_196:443

	This formalization generalizes standard projective parsing models based on the Eisner algorithm (Eisner, 1996) to yield efficient O(n2) exact parsing methods for nonprojective languages like Czech. ::: H05-1066_34:223

	We projectivize training data by a minimal transformation, lifting non-projective arcs one step at a time, and extending the arc label of lifted arcs using the encoding scheme called HEAD by Nivre and Nilsson (2005), which means that a lifted arc is assigned the label rh, where r is the original label and h is the label of the original head in the non-projective dependency graph. ::: C08-1081_82:137

S01,S02	The Setup We use two different parsers: (i) MaltParser (Nivre et al., 2007b) with the arc eager algorithm as optimized for English in (Nivre et al., 2010) and (ii) MSTParser with the second-order projective model of McDonald and Pereira (2006). ::: D11-1036_175:232

	In the following decade, great success in terms of parse disambiguation and even language modeling was achieved by various lexicalized PCFG models (Magerman, 1995; Charniak, 1997; Collins, 1999; Charniak, 2000; Charniak, 2001). ::: P03-1054_8:233

	We extracted tagged sentences from the parse trees.5 We split the data into training, development, and test sets as in (Collins, 2002). ::: N03-1033_93:202

	In another study, Blaheta and Charniak (2000) report an F-measure of 98.9% for the assignment of Penn Treebank grammatical role labels (our G set) to phrases that were correctly parsed by the parser described in Charniak (2000). ::: C04-1010_134:293

	The 50-best parser is a probabilistic parser that on its own produces high quality parses; the maximum probability parse trees (according to the parsers model) have an f-score of 0.897 on section 23 of the Penn Treebank (Charniak, 2000), which is still state-of-the-art. ::: P05-1022_8:180

S01,S02	As expected, we see that MST does better than Malt for all categories except nouns and pronouns (McDonald and Nivre, 2007). ::: P08-1108_135:172

	Given a feature representation for edges and a weight vector w, we seek the dependency tree or 92 h1 h1 h2 h2 s h1 h1 r r+1 h2 h2 t h1 h1 h2 h2 s h1 h1 h2 h2 t h1 h1 s h1 h1 t Figure 2: O(n3) algorithm of Eisner (1996), needs to keep 3 indices at any given stage. ::: P05-1012_51:209

M24	Nivre and McDonald (2008) and Zhang and Clark (2008) proposed stacking methods to combine graph-based parsers with transition-based parsers. ::: C10-2015_178:184

M05	Recently statistical dependency parsing techniques have been proposed which are deterministic and/or linear (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004). ::: W06-2922_6:94

	A simple example is shown in Figure 1, where the arc between a and hat indicates that hat is the head of a. Current statistical dependency parsers perform better if the dependency lengthes are shorter (McDonald and Nivre, 2007). ::: C08-1015_24:255

	As far as we know, this makes it different from all previous systems for dependency parsing applied to the Penn Treebank (Eisner, 1996; Yamada and Matsumoto, 2003), although there are systems that extract labeled grammatical relations based on shallow parsing, e.g. Buchholz (2002). ::: C04-1010_19:293

M23,P38	Because the frequency of non-projective dependencies in the Turkish Treebank is not high enough to learn such dependencies and mostly due to the unconnected punctuations with which we are dealing by adding an extra dependency label,we did not observe any improvement when applying the pseudo-projective processing of Nivre and Nilsson (2005),which is reported to improve accuracy for other languages. ::: J08-3003_244:443

P38	Since the number of non-projective dependencies is much smaller than the number of projective dependencies (Nivre and Nilsson, 2005), it is not efficient to perform non-projective parsing for all cases. ::: P11-2121_34:136

M12,V01	In order to compare the performance to the state of the art in dependency parsing, we have retrained the non-projective parser on the entire training data set for each language and evaluated it on the final test set from the CoNLL-X shared task (Buchholz and Marsi, 2006). ::: N07-1050_129:146

S02,M24	MaltParser (Nivre et al., 2007b) is a language independent system for data-driven dependency parsing, based on a transition-based parsing model (McDonald and Nivre, 2007). ::: C08-1081_57:137

	This averaging effect has been shown to help overfit ting (Collins, 2002). ::: H05-1066_134:223

	For the comparison of English dependency accuracy excluding punctuation, MIRA and BPM are both statistically significantly better than the averaged perceptron result reported in (McDonald et al. , 2005a). ::: N06-1021_145:189

	Examples include the margin perceptron (Duda et al. , 2001), ALMA (Gentile, 2001), and MIRA (which is used to train the parser in (McDonald et al. , 2005a)). ::: N06-1021_80:189

M01	Unlike most previous work on data-driven dependency parsing (Eisner, 1996; Collins et al. , 1999; Yamada and Matsumoto, 2003; Nivre, 2003), we assume that dependency graphs are labeled with dependency types, although the evaluation will give results for both labeled and unlabeled representations. ::: W04-2407_10:153

	Therefore, the integration of both techniques as in Nivre and McDonald (2008) seems to be very promising. ::: C10-1011_229:243

S01,M12,P29	Table 5 reports experimental results by using the first order decoding method in which an MST parsing algorithm (McDonald et al. , 2005b) is applied for non-projective parsing and the Eisners method is used for projective language data. ::: D07-1126_90:136

	We briefly describe the tagger (see (Ciaramita & Altun, 2006) for more details), a Hidden Markov Model trained with the perceptron algorithm introduced in (Collins, 2002). ::: W07-2217_160:259

V01,M37	To study the influence of lan968 guage and treebank specific properties we will use data from Arabic, Czech, Dutch, and Slovene, taken from the CoNLL-X shared task on multilingual dependency parsing (Buchholz and Marsi, 2006). ::: P07-1122_15:184

S01,S02	Parsers and settings All experiments were performed using two datadriven parsers, MaltParser6 (Nivre et al., 2007b), and MSTParser7 (McDonald et al., 2006). ::: W10-1403_120:262

	The feature types are essentially those described in (McDonald et al. , 2005a). ::: N06-1021_102:189

	This distinction roughly corresponds to the distinction made by Buchholz and Marsi (2006) between stepwise and all-pairs approaches. ::: D07-1096_231:410

	The baseline parser resembles the architecture of McDonald and Pereira (2006). ::: C10-1011_48:243

	Table 3 shows the results achieved by our method and other researchers (UAS with p), where Wang05 refers to (Wang et al., 2005), Wang07 refers 92 to (Wang et al., 2007), and McDonald&Pereira06 5 refers to (McDonald and Pereira, 2006). ::: I08-1012_150:180

	We build an ISBN model of dependency parsing using the parsing order proposed in (Nivre et al. , 2004). ::: W07-2218_25:279

	Previous work has shown that conditioning on neighboring decisions can lead to significant improvements in accuracy (Yamada and Matsumoto, 2003; Charniak, 2000). ::: E06-1011_18:205

	We extend the projective parsing algorithm of Eisner (1996) for our case, and train models using the averaged perceptron. ::: D07-1101_5:204

M33	Greedy local search (Yamada and Matsumoto, 2003; Sagae and Lavie, 2005; Nivre and Scholz, 2004) has typically been used for decoding in shift-reduce parsers, while beam-search has recently been applied as an alternative to reduce error-propagation (Johansson and Nugues, 2007; Zhang and Clark, 2008; Zhang and Clark, 2009; Huang et al., 2009). ::: P11-1069_71:201

M33	Specifically, we make the following contributions: we develop a baseline shift-reduce dependency parser using the less popular, but classical, arc-standard style (Section 2), and achieve similar state-of-the-art performance with the the dominant but complicated arceager style of Nivre and Scholz (2004); we propose bilingual features based on word alignment information to prefer target-side contiguity in resolving shift-reduce conflicts (Section 3); we verify empirically that shift-reduce conflicts are the major source of errors, and correct shift-reduce decisions strongly correlate with the above bilingual contiguity conditions even with automatic alignments (Section 5.3); finally, with just three bilingual features, we improve dependency parsing accuracy by 0.6% for both English and Chinese over the state-of-the-art baseline with negligible (6%) efficiency overhead (Section 5.4). ::: D09-1127_34:211

	We demonstrate the application of our procedure to comparing dependency parsing results on different versions of the Penn Treebank (Marcus et al., 1993). ::: D11-1036_158:232

	Computation of the marginals and partition function can also be achieved in O(n3) time, using a variant of the inside-outside algorithm (Baker, 1979) applied to the Eisner (1996) data structures (Paskin, 2001). ::: D07-1015_74:443

	Sagae and Lavie (2006) later used weighted voting and reparsing, i.e. using CLE to find the dependency tree that reflects the maximum number of votes. ::: C10-1120_117:233

M22	Indeed, the result of Collins (2002) that including low support features helps a voted perceptron model but harms a maximum entropy model is undone once the weights of the maximum entropy model are regularized. ::: N03-1033_179:202

