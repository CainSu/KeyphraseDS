M03	Capturing question or answer dependencies can be cast as a straightforward process of mapping syntactic trees to sets of binary head modifier relationships, as first noted in (Collins, 1996). ::: W01-1206_37:178

M22	IBMs Statistical QA (Ittycheriah et al. , 2001a) system uses a probabilistic model trainable from Question-Answer sentence pairs. ::: N03-2029_9:115

M06,M03	In (Echihabi and Marcu, 2003) another form of combining strategies for advanced QA is proposed: (1) a knowledge-based Q/A implementation based on syntactic/semantic processing is combined using a maximum-entropy framework with (2) a statistical noisy-channel algorithm for Q/A and (3) a pattern-based approach that learn from Web data. ::: W04-2501_7:251

M06	But it is almost impossible to learn such surface text patterns following (Ravichandran and Hovy, 2002). ::: H05-1075_55:189

	This method was first described in Ravichandran and Hovy (2002). ::: N03-2029_23:115

M03	For example, syntactic information has been deployed to reformul+ate questions (Hermjakob et al. , 2002) or to replace questions by syntactically similar ones (Lin and Pantel, 2001); lexical ontologies such as Wordnet1 have been used to find synonyms for question words (Burke et al. , 1997; Hovy et al. , 2000; Prager et al. , 2001; Harabagiu et al. , 2001), and statistical machine translation (SMT) models trained on question-answer pairs have been used to rank candidate answers according to their translation probabilities (Berger et al. , 2000; Echihabi and Marcu, 2003; Soricut and Brill, 2006). ::: P07-1059_9:239

M10	(Echihabi and Marcu, 2003)); or (4) a constraint satisfaction problem, where sets of auxiliary questions are used to provide more information and better constrain the answers to individual questions (cf. ::: P06-1114_9:248

P10	To evaluate our learning approach, we trained AQUAREA$ on the same development set of stories and tested it on the same test set of stories as those used in all past work on the reading comprehension task (Hirschman et al. , 1999; Charniak et al. , 2000; Riloffand Thelen, 2000; Wang et al. , 2000). ::: W00-1316_165:209

P10	Naturally, our current work on question answering for the reading comprehension task is most related to those of (Hirschman et al. , 1999; Charniak et al. , 2000; Riloffand Thelen, 2000; Wang et al. , 2000). ::: W00-1316_73:209

	Our model considers two sets of features: Feature Set 1 (FS1): features used in the work reported in (Gildea and Palmer, 2002) and (Gildea and Jurafsky, 2002) ; and Feature Set 2 (FS2): a novel set of features introduced in this paper. ::: P03-1002_77:187

M03	Several works attempt to extend WordNet with additional lexical semantic information (Moldovan and Rus, 2001; Snow et al., 2006; Suchanek et al., 2007; Clark et al., 2008). ::: W09-2504_223:246

M10	The LCC system (Moldovan & Rus, 2001) uses a Logic Prover to establish the connection between a candidate answer passage and the question. ::: P06-1135_45:214

	Gildea and Jurafsky (2002) describe a statistical system trained on the data from the FrameNet project to automatically assign semantic roles. ::: J05-1004_360:501

	As a guideline for interpreting these results, with 8,167 observations, the threshold for statistical significance with p < .05 is a 1.0% absolute difference in performance (Gildea and Jurafsky 2002). ::: J05-1004_399:501

M06	For example, the year of birth of a person is typically expressed using one of these phrases: <name> was born in <birthyear> <name> (<birthyear><deathyear>) We have developed a method to learn such patterns automatically from text on the web (Ravichandran and Hovy, 2002). ::: C02-1042_49:113

	Gildea and Jurafsky (2002) used a supervised learning method to learn both the identifier of the semantic roles defined in FrameNet such as theme, target, goal, and the boundaries of the roles (Baker et al. , 2003). ::: W04-0509_56:234

M03	Another use of natural language processing has been the deployment of SMT models on question-answer pairs for (re)ranking candidate answers which were either assumed to be contained in FAQ pages (Berger et al. , 2000) or retrieved by baseline systems (Echihabi and Marcu, 2003; Soricut and Brill, 2006). ::: P07-1059_30:239

	We load the COGEX logic prover (Moldovan et al. , 2003) which operates by reductio ad absurdum with Hs negated form and Ts predicates. ::: H05-1047_46:194

P10	The work of (Hirschman et al. , 1999) initiated this series of work, and it reported an accuracy of 36.3% on answering the questions in the test stories. ::: W00-1316_75:209

	Narayanan and Harabagiu (2004) were the first to stress the importance of semantic roles in answering complex questions. ::: D07-1002_49:241

M12	However, in the context of factual questions that are of interest to us here, conceptual categories do not seem to be helpful; instead, our goal is to semantically classify questions, as in earlier work on TREC (Singhal et al. , 2000; Hovy et al. , 2001; Harabagiu et al. , 2001; Ittycheriah et al. , 2001). ::: C02-1150_52:193

M06	The use of lexical patterns to identify answers in corpus-based QA received lots of attention after a team taking part in one of the earlier QA Tracks at TREC showed that the approach was competitive at that stage (Soubbotin and Soubbotin, 2002; Ravichandran and Hovy, 2002). ::: C04-1188_38:193

M03	By implementing our own version of the publicly available Collins parser (Collins, 1996), we also learned a dependency model that enables the mapping of parse trees into sets of binary relations between the head-word of each constituent and its sibling-words. ::: P01-1037_48:175

M07	In prior work (Hirschman et al. , 1999; Charniak et al. , 2000; Riloffand Thelen, 2000) the number and type of information sources used for computation is specific to and rlifFerent for each question type. ::: W00-1316_91:209

P10	In a related but independent effort, a group at MITRE has investigated question answering in the context of the reading comprehension task (Hirschman et al. , 1999). ::: W00-1316_18:209

M22	Many studies on QA have focused on discriminative models to predict a function of matching features between each question and candidate passage (set of sentences), namely q/a pairs, e.g., (Ng et al., 2001; Echihabi and Marcu, 2003; Harabagiu and Hickl, 2006; Shen and Klakow, 2006; Celikyilmaz et al., 2009). ::: W10-1201_13:185

M22	The statistical model introduced in Gildea and Jurafsky (2002) uses predicate lexical information at most levels in the probability lattice, hence its scalability to unknown predicates is limited. ::: P03-1002_136:187

M06,P05	Although semantics-poor techniques, such as surface pattern matching (Soubbotin, 2002; Ravichandran and Hovy, 2002) or statistical methods (Ittycheriah et al. , 2002), have been successful in answering factoid questions, more complex tasks require a consideration of text meaning. ::: W03-0908_5:134

	We convert each text into logic form (Moldovan and Rus, 2001). ::: H05-1047_27:194

M22	To achieve high accuracy and resolve the data sparsity problem the method reported in (Gildea and Palmer, 2002; Gildea and Jurafsky, 2002) employed a backoff solution based on a lattice that combines the model features. ::: P03-1002_62:187

P05	In this field as well, the use of pre-defined sets of relation patterns has proved fairly reliable, particularly in the case of factoid type queries (Brill et al. , 2002; Ravichandran and Hovy, 2002; Hovy et al. , 2002; Soubbotin and Soubbotin, 2002). ::: H05-1088_12:163

	The LCC system (Moldovan & Rus, 2001; Harabagiu et al. 2004) uses a logic prover to select answer from related passages. ::: P06-1147_32:189

M22	In our first set of experiments, the features and probability model of the Gildea and Jurafsky (2002) system were applied to the PropBank corpus. ::: J05-1004_367:501

M22	In previous work using the PropBank corpus, (Gildea and Palmer, 2002) proposed a model predicting argument roles using the same statistical method as the one employed by (Gildea and Jurafsky, 2002) for predicting semantic roles based on the FrameNet corpus (Baker et al. , 1998). ::: P03-1002_47:187

V01	Data are collected from four sources: 4,500 English questions published by USC (Hovy et al. , 2001), about 500 manually constructed questions for a few rare classes, 894 TREC 8 and TREC 9 questions, and also 500 questions from TREC 10 which serves as our test set 3 . These questions were manually labeled according to our question hierarchy. ::: C02-1150_138:193

	Examples of 22 cases where the bag-of-words approach fails abound inQAliterature; here we borrow an example used by Echihabi and Marcu (2003). ::: D07-1003_24:243

M06	Such patterns (e.g. president George Bush) are very productive and occur 40 times more often than patterns employed by Hearst (1992). ::: P03-1001_44:179

P10	We evaluates Quarc on the same data set that was used to evaluate the DeepRead reading comprehension system (Hirschman et al. , 1999). ::: W00-0603_174:214

M03	Our approach is less domain dependent and resource intensive than Narayanan and Harabagiu (2004), it solely employs a dependency parser and the FrameNet database. ::: D07-1002_61:241

M06	Hearst (1992) explored the use of lexical patterns for extracting hyponym relations, with patterns such as such as. Berland and Charniak (1999) extract part-of relations. ::: C04-1188_36:193

	(Moldovan et al. , 2003) built a logic prover for Question Answering. ::: P06-1113_24:268

M03	To help our model learn that it is desirable to copy answer words into the question, we add to each corpus a list of identical dictionary word pairs w iw i . For each corpus, we use GIZA (Al-Onaizan et al. , 1999), a publicly available SMT package that implements the IBM models (Brown et al. , 1993), to train a QA noisy-channel model that maps flattened answer parse trees, obtained using the cut procedure described in Section 3.1, into questions. ::: P03-1003_87:185

	This is referred to as the 10-90 rule (Moldovan and Rus 2001). ::: N03-1022_52:176

M22	Recently, some pioneering studies have investigated approaches to automatically construct QA components from scratch by applying machine learning techniques to training data (Ittycheriah et al. , 2001a)(Ittycheriah et al. , 2001b)(Ng et al. , 2001) (Pasca and Harabagiu)(Suzuki et al. , 2002)(Suzuki 215 Table 1: Number of Questions in Question Types of CRL QA Data # of Questions # of Question Types Example 1-9 74 AWARD, CRIME, OFFENSE 10-50 32 PERCENT, N PRODUCT, YEAR PERIOD 51-100 6 COUNTRY, COMPANY, GROUP 100-300 3 PERSON, DATE, MONEY Total 115 et al. , 2003) (Zukerman and Horvitz, 2001)(Sasaki et al. , 2004). ::: P05-1027_20:223

	Question answering (QA) is often cited as an obvious beneficiary of semantic 12 role labeling (Gildea and Jurafsky, 2002; Palmer et al. , 2005; Narayanan and Harabagiu, 2004). ::: D07-1002_23:241

M22	(Berger et al. , 1996) is to nd a model p = argmax pC H(p), which means a probability model p(y|x) that maximizes entropy H(p). ::: P05-1027_46:223

	Similarly, Murdock and Croft (2005) adopted a simple translation model from IBM model 1 (Brown et al. , 1990; Brown et al. , 1993) and applied it to QA. ::: D07-1003_56:243

M22	We propose to study and develop several kernel methods that can operate in Support Vector Machines for determining the optimal strategies and compare the results with the Maximum Entropy combinations reported in (Echihabi and Marcu, 2003). ::: W04-2501_48:251

P10,P11	Open-domain question answering (Lehnert, 1986; Harabagiu et al. , 2001; Light et al. , 2001) and story comprehension (Hirschman et al. , 1999) have become important directions in natural language processing. ::: C02-1150_6:193

M03	(Moldovan et al. , 2003)); (3) a noisy-channel model which selects the most likely answer to a question (cf. ::: P06-1114_8:248

M06	Ravichandran and Hovy (2002) present an alternative ontology for type preference and describe a method for using this alternative ontology to extract particular answers using surface text patterns. ::: W02-1111_59:167

M06	Hearst (1992) examined extracting hyponym data by taking advantage of lexical patterns in text. ::: P03-1001_26:179

M06	Hearst (1992) used textual patterns (e.g. such as) to identify common class members. ::: W02-1111_125:167

M06	Several QA systems have investigated the use of text patterns for QA (Soubbotin and Soubbotin, 2001), (Soubbotin and Soubbotin, 2002), (Ravichandran and Hovy, 2002). ::: N03-2029_5:115

M06	The web has been employed for pattern acquisition (Ravichandran et al. , 2003), document retrieval (Dumais et al. , 2002), query expansion (Yang et al. , 2003), structured information extraction, and answer validation (Magnini et al. , 2002). ::: P04-3018_51:80

	We also used the answer keys created by the DeepRead developers (Hirschman et al. , 1999). ::: W00-0603_176:214

M03	Being inspired by the success of noisy-channel-based approaches in applications as diverse as speech recognition (Jelinek, 1997), part of speech tagging (Church, 1988), machine translation (Brown et al. , 1993), information retrieval (Berger and Lafferty, 1999), and text summarization (Knight and Marcu, 2002), we develop a noisy channel model for QA. ::: P03-1003_20:185

	Systems for addressing complex information needs are interesting because they provide an opportunity to explore the role of semantic structures in question answering, e.g., (Narayanan and Harabagiu, 2004). ::: W06-0704_14:191

M10	Answer types are determined using classification rules similar to Li and Roth (2002). ::: D07-1002_147:241

M03	One way to address this problem is to learn question-to-answer transformations using a translation model (Berger et al., 2000; Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007). ::: P08-1082_91:236

M19	WordNet (Fellbaum, 1998) is perhaps the most popular resource and has been employed in a variety of QA-related tasks ranging from query expansion, to axiom-based reasoning (Moldovan et al. , 2003), passage scoring (Paranjpe et al. , 2003), and answer filtering (Leidner et al. , 2004). ::: D07-1002_44:241

M21	Moldovan and Rus (Moldovan and Rus, 2001) transformed the question and the candidate answers into logic forms and used a logic prover to determine if the candidate answer logic form (ALF) entails the question logic form(QLF). ::: P06-1113_22:268

M22	This probability is computed using IBMs Model 1 (Brown et al., 1993): P(Q|A) = product display qQ P(q|A) (3) P(q|A) = (1)Pml(q|A)+Pml(q|C) (4) Pml(q|A) = summation display aA (T(q|a)Pml(a|A)) (5) where the probability that the question term q is generated from answer A, P(q|A), is smoothed using the prior probability that the term q is generated from the entire collection of answers C, Pml(q|C). ::: P08-1082_93:236

M03	Porting the translation model to QA is not straightforward; it involves parse-tree pruning heuristics (the first two deterministic steps in Echihabi and Marcu, 2003) and also replacing the lexical translation table with a monolingual dictionary which simply encodes the identity relation. ::: D07-1003_57:243

M06	Automatic pattern derivation is more appealing (Ravichandran and Hovy, 2002). ::: H05-1039_33:207

M06	Ravichandran and Hovy (2002) proposed automatically learning surface text patterns for answer extraction. ::: H05-1075_50:189

	(Berger et al. , 1996) gave a good description of ME model. ::: P06-1112_151:260

	In the first stage of our representation process, COGEX converts a1 and a0 into logic forms (Moldovan and Rus, 2001). ::: P06-2105_33:163

	This metric is originally proposed by (Hirschman et al. , 1999). ::: W00-1316_172:209

M22	Next, the probabilities Pr i j F i,p are combined with the probabilities Pfr 1 :::n gjp for a set of roles appearing in a sentence given a predicate, using the following formula: Pr 1 :::n jF 1 :::n, p, Pfr 1 :::n gjp Y i Pr i jF i,p Pr i jp This approach, described in more detail in Gildea and Jurafsky (2002), allows interaction among the role assignments for individual constituents while making certain independence assumptions necessary for efficient probability estimation. ::: J05-1004_387:501

M04	While probabilistic or web-based methods for answer validation have been previously explored in the literature (Magnini et al. , 2002), these approaches have modeled the relationship between a question and a (correct) answer in terms of relevance and have not tried to approximate the deeper semantic phenomena that are involved in determining answerhood. ::: P06-1114_173:248

M03	Recent years have witnessed significant progress in developing methods for the automatic identification and labeling of semantic roles conveyed by sentential constituents.1 The success of these methods, often referred to collectively as shallow semantic parsing (Gildea and Jurafsky, 2002), is largely due to the availability of resources like FrameNet (Fillmore et al. , 2003) and PropBank (Palmer et al. , 2005), which document the surface realization of semantic roles in real world corpora. ::: D07-1002_8:241

M03	It is interesting to note that the semantic frames are a helpful way of generalizing between predicates; words in the same frame have been found frequently to share the same syntactic argument structure (Gildea and Jurafsky 2002). ::: J05-1004_276:501

M22	These distributions are modeled using a maximum entropy formulation (Berger et al. , 1996), using training data which consists of human judgments of question answer pairs. ::: N03-1004_51:187

M06	These patterns could be manually generated, such as the ones described here, or learned from text, as described in Ravichandran and Hovy (2002). ::: P03-1001_157:179

	Our feature representation was designed to capture the information sources that prior work (Hirschman et al. , 1999; Cha_niak et al. , 2000; Riloff and Thelen, 2000) used in their computations or rules. ::: W00-1316_100:209

	Although there were a few other scoring met129 rics originally proposed in (Hirschman et al. , 1999), all the metrics were found to correlate well with one another. ::: W00-1316_174:209

M03,M22	Wordnets and ontologies are very common resources and are employed in a wide variety of direct and indirect QA tasks, such as reasoning based on axioms extracted from WordNet (Moldovan et al. , 2003), probabilistic inference using lexical relations for passage scoring (Paranjpe et al. , 2003), and answer filtering via WordNet constraints (Leidner et al. , 2003). ::: P04-3018_29:80

M10	ANSWER SELECTION modules typically work either by trying to prove the answer is correct (Moldovan & Rus, 2001) or by giving them a weight produced by summing a collection of heuristic features (Radev et al. , 2000); in the latter case candidates having a larger number of matching query terms, even if they do not exactly match the context in the question, might generate a larger score than a correct passage with fewer matching terms. ::: P06-1135_21:214

M22	The second answering agent takes a statistical approach to question answering (Ittycheriah, 2001; Ittycheriah et al. , 2001). ::: N03-1004_47:187

M03	Echihabi and Marcu (2003) presented a noisy channel approach in which they adapted the IBM model 4 from statistical machine translation (Brown etal. ::: D07-1003_53:243

	Scenario knowledge was also included in the form of axiomatic logic transformation developed in (Moldovan et al. , 2003). ::: W06-0705_14:205

