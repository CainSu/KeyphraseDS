As mentioned by (Pradhan et al 2004) argument identification plays a bottleneck role in improving the performance of a SRL system 
This approach was soon followed by other researchers (Surdeanu et al 2003 Pradhan et al 2004 Xue and Palmer 2004) focus21 ing on improved sets of features improved machine learning methods or both and SRL became a shared task at the CoNLL 2004 2005 and 2008 conferences1 
Traditionally either constituent or dependency based semantic role labeling is done in two steps argument identification and classification (Gildea and Jurafsky 2002) 
One advantage of log-linear models over SVMs for us is that they produce probability distributions and thus identification 590 Standard Features (Gildea and Jurafsky 2002) PHRASE TYPE Syntactic Category of node PREDICATE LEMMA Stemmed Verb PATH Path from node to predicate POSITION Before or after predicate? 
For example indicators of the immediate syntactic types that form the argument flags raised by punctuation tokens in or nearby the argument or the governing category feature of Gildea and Jurafsky (2002) 
To develop an automatic SRL system for the biomedical domain it is necessary to train the system with an annotated corpus called proposition bank (Palmer et al  2005) 
Others have applied frame-semantic structures to question answering paraphrase/entailment recognition and information extraction (Narayanan and Harabagiu 2004 Shen and Lapata 2007 Pado and Erk 2005 Burchardt 2006 Moschitti et al 2003 Surdeanu et al 2003) 
(Xue and Palmer 2004) fount out that different features suited for different sub-tasks of SRL ie argument identification and classification 
The partial trees output by these systems were merged with the parse trees returned by (Charniak 2000)s parser 
This assumption is a simple adaptation of the pruning algorithm by Xue and Palmer (2004) and it holds for the vast majority of arguments in the CoNLL-2008 data in the training set we measured that this covers 9904% of the arguments of verbs and 9755% of the argu2Since our algorithm needs to know the positions of the predicates we trained a separate classifier using the LIBLINEAR toolkit (Fan et al 2008) to identify the predicate words 
Conversely researchers interested in producing richer semantic outputs have concentrated on two-stage systems where the semantic labelling task is performed on the output of a parser in a pipeline architecture divided in several stages (Gildea and Jurafsky 2002 Nielsen and Pradhan 2004 Xue and Palmer 2004) 
Moschitti (2004) The work was mainly done when the author was a visiting student at I2R and Che et al 
We create a baseline system using a subset of features introduced by Gildea and Jurafsky (2002) which are directly applicable to nominal predicates 
Our model considers two sets of features Feature Set 1 (FS1) features used in the work reported in (Gildea and Palmer 2002) and (Gildea and Jurafsky 2002)  and Feature Set 2 (FS2) a novel set of features introduced in this paper 
Shallow approaches to semantic processing are making large advances in the direction of efficiently and effectively deriving application relevant explicit semantic information from text (Pradhan et al 2003 Gildea and Palmer 2002 Pradhan et al 2004 Gildea and Jurafsky 2002 Xue and Palmer 2004 Chen and Rambow 2003 Carreras and Marquez 2005 Moschitti 2004 Moschitti et al 2005 Diab et al 2008) 
It is well known that dependency trees extracted from lexicalized phrase structure parsers (Collins 1999 Charniak 2000) typically are more accurate than those produced by pure dependency parsers (Yamada and Matsumoto 2003) 
Researchers were able to construct a reasonable IE system by simply mapping specific Arg labels for a set of verbs to template slots completely avoiding the necessity of building explicit regular expression pattern matchers (Surdeanu et al 2003) 
It should be noted that our results of kernel combinations on FrameNet are in contrast with (Moschitti 2004) where no improvement was obtained 
Our evaluation criteria which is based on predicting the SRL for constituents in the parse tree is based on the evaluation used in (Toutanova et al  2005) 
This is very valuable as previous work showed that tree kernels (TK) alone perform lower than models based on manually engineered features for SRL tasks eg (Moschitti 2004 Giuglea and Moschitti 2004 Giuglea and Moschitti 2006 Moschitti 2006b Che et al 2006 Moschitti et al 2008) 
A more realistic way to score the performance is to score tags assigned to head words of constituents rather than considering the exact boundaries of the constituents as reported by Gildea and Hockenmaier (2003) 
Argument filtering Previous work in shallow semantic parsing has demonstrated that not all nodes in a tree are equally probable as semantic roles for a given predicate (Xue and Palmer 2004) 
Ten systems contributed to the task which was evaluated using the PropBank corpus (Palmer et al  2005) 
Unfortunately ar667 NP PP NP NP NP Peters laughter about the joke Figure 1 Parse tree for example sentence gument recognition at least within predicates relies heavily on syntactic features with the grammatical function (or alternatively syntactic path) feature as the single most important predictor (Gildea and Jurafsky 2002) 
The task of semantic role labeling in the context of PropBank (Palmer et al 2005) is to label tree nodes with semantic roles in a syntactic parse tree 
We formulate the labeling task as a classification problem as initiated by Gildea and Jurafsky (2002) and use Support Vector Machine (SVM) classifiers (2005) 
(Pradhan et al  2004) train a language model over label sequences 
Gildea and Jurafsky (2002) first tackled SRL as an independent task which is divided into several sub-tasks such as argument identification argument classification global inference etc Some researchers (Xue and Palmer 2004 Koomen et al 2005 Cohn and Blunsom 2005 Punyakanok et al 2008 Toutanova et al 2005 Toutanova et al 2008) used a pipelined approach to attack the task 
In addition Moschitti (2004)s Tree Kernel Tool is used to compute the tree kernel function 
We also compared our proposed feature set against predicate/argument features (PAF) proposed by (Moschitti 2004) 
To put our results in the context of previous work other results on core arguments using the same input features have been reported the best being 914% for an SVM with a degree 2 polynomial kernel (Pradhan et al  2005a)3 The highest reported result for independent classification of core arguments is 960% for a log-linear model using more than 20 additional basic features (Toutanova et al  2005) 
Note that this pruning algorithm is slightly different from that of (Xue and Palmer 2004) the predicate itself is also included in the argument candidate list as the nominal predicate sometimes takes itself as its argument 
The data consist of sections of the Wall Street Journal (WSJ) part of the Penn TreeBank (Marcus et al  1993) with information on predicate-argument structures extracted from the PropBank corpus (Palmer et al  2005) 
Again we concentrate on the PropBank corpus (Palmer et al  2005) which is the Wall Street Journal part of the Penn TreeBank corpus enriched with predicate argument structures 
Things become worse still in a parser like the one described in Charniak (2000) because it conditions on (and hence splits the dynamic programming states according to) features of the grandparent node in addition to the parent thus multiplying the number of possible dynamic programming states even more 
(Gildea and Jurafsky 2002) used the empirical probability of the set of proposed arguments as a prior distribution 
Here the arguments for the predicate gave are defined in the PropBank Frame Scheme (Palmer Gildea and Kingsbury 2005) as V verb A2 beneficiary A0 giver AM-TMP temporal A1 thing given Recognizing and labeling semantic arguments is a key task for answering Who WhenWhat Where Why etc questions in Information Extraction Question Answering Summarization (Melli et al 2005) and in general in all NLP tasks in which some kind of semantic interpretation is needed 
Whole Label Sequence As observed in previous work (Gildea and Jurafsky 2002 Pradhan et al  2004) including information about the set or sequence of labels assigned to argument nodes should be very helpful for disambiguation 
The features incorporated in the proposed model are inspired from the work of (Gildea and Jurafsky 2002 Surdeanu et al  2003 Pradhan et al  2005 Collins 1999) and can be classified into five classes (a) features that capture the internal structure of the candidate argument (b) features extracted The syntactic label of the candidate constituent 
The motivation behind is that structured syntactic information plays a critical role in negation scope finding and should be paid much more attention as indicated by previous studies in shallow semantic parsing (Gildea and Palmer 2002 Punyakanok et al 2005) 
(Xue and Palmer 2004) or (Carreras and M`arquez 2005) 
Instead we borrowed most of them from the existing literature (Gildea and Jurafsky 2002 Carreras et al  2004 Xue and Palmer 2004) 
While all these systems perform quite well on the WSJ test data they show significant performance degradation (approximately 10 point drop in F-score) when applied to label test data that is different than the genre that WSJ represents (Pradhan et al  2004 Carreras and M`arquez 2005) 
Although state-of-the-art SRL systems use sophisticated statistical models to perform these two tasks jointly (eg Toutanova et al 2005 Johansson and Nugues 2008) we implemented them as two independent support vector classifiers to be able to analyze the impact of syntactic representation on each task separately 
The term is most commonly used to describe the automatic identification and labeling of the semantic roles conveyed by sentential constituents (Gildea and Jurafsky 2002) 
The best phrase-structure parsing models represent generatively the joint probability P(xy) of sentence x having the structure y (Collins 1999 Charniak 2000) 
In English predicate argument structure analysis large corpora such as FrameNet (Fillmore et al 2001) PropBank (Palmer et al 2005) and NomBank (Meyers et al 2004) have been created and utilized 
One such augmented parser trained on data available from the PropBank project has been recently presented in (Gildea and Palmer 2002) 
The systems for the other languages follow the successful models devised for English (Gildea and Jurafsky 2002 95 Xue and Palmer 2004 Pradhan et al  2003) 
For example the SRL system described in (Pradhan et al  2005b Pradhan et al  2005a) achieves an Fscore of 81% when tested on the same genre as it is trained on (WSJ) but that score drops to 685% when the same system is tested on a different genre (the Brown corpus) 
Moschitti (2004) only selected the relative portion between a predicate and an argument 
Features extracted from LTAG derivations are different and provide distinct information when compared to predicate-argument features (PAF) or subcategorization features (SCF) used in (Moschitti 2004) or even the later use of argument spanning trees (AST) in the same framework 
Of special interest here Moschitti (2004) proposed Predicate Argument Feature (PAF) kernel for SRL under the framework of convolution tree kernel 
Here solid lines correspond to surface syntactic structure produced by Charniaks parser (Charniak 2000) and dashed lines are an encoding of the Proposition Bank annotation of the semantic roles with respect to the verb stopped 
We adopt the same algorithm described in (Toutanova et al  2005) 
Content words which add informative lexicalized information different from the head word were detected using the heuristics of (Surdeanu et al  2003) 
It is generally formulated as a semantic role labeling (SRL) task where each argument of the predicate is assigned a label that represents the semantic role it plays with regard to its predicate (Gildea and Jurafsky 2002 Hacioglu et al  2003 Pradhan et al  2004b Xue and Palmer 2004 Toutanova et al  2005 Koomen et al  2005) 
First argument candidates are generated from the input constituent parse tree using the prevalent heuristic-based pruning algorithm in (Xue and Palmer 2004) 
Most approaches to the problem of SRL follow the Gildea and Jurafsky (2002) model 
PropBank encodes propositional information by adding a layer of argument structure annotation to the syntactic structures of the Penn Treebank (Marcus et al  1993) 
Though PropBank defines semantic roles on a verb by verb basis for a particular verb Arg0 is generally the argument exhibiting features of a prototypical Agent while Arg1 is a prototypical Patient or Theme (Palmer et al 2005) 
Propbank (Palmer et al 2005) is a hand-annotated corpus 
Approaches include incorporating a subcategorization feature (Gildea & Jurafsky 2002 Xue & Palmer 2004) such as the one used in our baseline and building a model which jointly classifies all arguments of a verb (Toutanova et al 2005) 
For the test data we report on results using the gold-standard Tree bank dataand in addition we also report results on automatically parsed data using the Charniak parser (Charniak 2000) as provided by the CoNLL 2005 shared task 
State-of-the-art models for the subproblem of classification of core arguments additionally use other features of individual nodes (Xue and Palmer 2004 Pradhan et al  2005a) as well as global features including the labels of other nodes in parse tree 
It happens that in the case of full parses this node selection strategy is equivalent to the pruning process defined by Xue and Palmer (2004) which selects sibling nodes along the path of ancestors from the verb predicate to the root of the tree2 
Their corpora are used to train supervised models for semantic role labelling (SRL) of new text (Gildea and Jurafsky 2002 Carreras and M`arquez 2005) 
The evaluations were carried out with the SVMLight-TK2 software (Moschitti 2004) which extends the SVM-Light package (Joachims 1999) with tree kernel functions 
SRL approach The SRL approach that we adopt is based on the deep syntactic parse (Charniak 2000) of the sentence that we intend to annotate semantically 
Top-10 combined features for SRI and SRC ranked by z-score Table 1 shows that the commonly used combined features such as 'predicate+head word' (c1+c6) and 'position+voice' (c4+c5) proposed by (Xue and Palmer 2004) are also included 
There is a growing body of work on statistical learning for different versions of the semantic parsing problem (eg (Gildea and Jurafsky 2002 Zettlemoyer and Collins 2005 Ge and Mooney 2005 Mooney 2007)) however most of these methods rely on human annotation or some weaker forms of supervision (Kate and Mooney 2007 Liang et al 2009 Titov and Kozhevnikov 2010 Clarke et al 2010) and very little research has considered the unsupervised setting 
Our experiments working with a flat chunked representation of the input sentence described in more detail in Gildea and Palmer (2002) test this finite-state hypothesis 
While CRFs have not been used to date for SRL their close cousin the maximum entropy model has been with strong generalisation performance (Xue and Palmer 2004 Lim et al  2004) 
Arguments to Syntactic Constituents Our approach maps each argument label to one syntactic constituent using a strategy similar to (Surdeanu et al  2003) 
The second task we concentrate on is semantic role labeling in the context of PropBank (Palmer et al  2005) 
Among them we select the following subset (a) Phrase Type Predicate Word Head Word 134 Position and Voice as defined in (Gildea and Jurafsky 2002) (b) Partial Path No Direction Path Head Word POS First and Last Word/POS in Constituent and SubCategorization as proposed in (Pradhan et al  2003) and (c) Syntactic Frame as designed in (Xue and Palmer 2004) 
The training set is extracted from TreeBank (Marcus et al  1993) section 1518 the development set used in tuning parameters of the system from section 20 and the test set from section 21 
Constituent path as described in (Gildea and Jurafsky 2002) All 3/4/5-grams of path constituents beginning at the verb predicate or ending at the constituent 
For our experiments we use Feb 2004 release of PropBank1 (Kingsbury and Palmer 2002 Palmer et al  2005) a corpus in which predicate argument relations are marked for verbs in the Wall Street Journal (WSJ) part of the Penn TreeBank (Marcus et al  1994) 
In order to speed up the learning process we use a four-stage learning architecture Stage 1 To save time we use a pruning stage (Xue and Palmer 2004) to filter out the constituents that are clearly not semantic arguments to the predicate 
Note that the result here is not comparable with the best in this domain (Pradhan et al  2004) where the full parse tree is assumed given 
Neighboring arguments The research of (Jiang et al  2005 Toutanova et al  2005) has shown the importance of capturing information of the global argument frame in order to correctly classify the local argument 
Whereas many more global constraints were taken into account in (Punyakanok et al 2004 Koomen et al 2005) 
And previous research on English SRL shows that combination is a robust and effective method to alleviate SRLs dependency on parsing results (M`arquez et al 2005 Koomen et al 2005 Pradhan et al 2005 Surdeanu et al 2007 Toutanova et al 2008) 
Methods used include bootstrapping approaches (Gildea and Jurafsky 2002 Kate and Mooney 2007) where large unannotated corpora were tagged with SRL annotation later to be used to retrain the SRL model 
About 10% of the fragments contain both the predicate and the argument node while about 1% encode the Path feature traditionally used in explicit semantic role labeling models (Gildea and Jurafsky 2002) 
For example the backoff lattice in (Gildea and Palmer 2002) consists of eight connected nodes for a five-feature set 
The statistical model introduced in Gildea and Jurafsky (2002) uses predicate lexical information at most levels in the probability lattice hence its scalability to unknown predicates is limited 
Our standard feature set is a combination of features proposed by (Gildea and Jurafsky 2002) (Surdeanu et al  2003 Pradhan et al  2004 Pradhan et al  2005b) and (Xue and Palmer 2004) 
More details of this system can be found in Pradhan et al  (2005) 
Local classifiers Following the standard practice in semantic role labeling we divided the case prediction task into the tasks of identification and classification (Gildea and Jurafsky 2002 Pradhan et al  2004) 
These systems all use (Charniak 2000)s parse trees both for training and testing as well as various other information sources including sets of n-best parse trees chunks or named entities 
In this paper we apply the traditional boundary (TBC) and role (TRC) classifiers (Pradhan et al  2005a) which are based on binary predicate/argument relations to label all parse tree nodes corresponding to potential arguments 
This greedy combination method is very simple and has been adopted in previous research (Pradhan et al 2005 M`arquez et al 2005) 
Next sentences are analyzed by a state-of-the-art syntactic parser (Charniak 2000) the output of which provides useful information for the main SRL module 
Hence the process is divided into framing and annotation (Palmer et al 2005) 
Semantic Role labeling (SRL) was first defined in Gildea and Jurafsky (2002) 
For example in their inclusion of voice Gildea and Palmer (2002) note that this deep syntax feature plays an important role in connecting semantic role with surface grammatical function 
A constituent-based system using Charniaks parser (Charniak 2000) 
Our experiments on joint syntactic and semantic parsing use data that is produced automatically by merging the Penn Treebank (PTB) with PropBank (PRBK) (Marcus et al 1993 Palmer et al 2005) as shown in Figure 1 
We also experimented with various feature combinations inspired by the features used in (Xue and Palmer 2004) 
PropBank (PB) (Palmer et al 2005) is a widely used corpus providing SRL annotation for the entire WSJ Penn Treebank 
We find the exact top N consistent1 most likely local model labelings using a simple dynamic program described in (Toutanova et al  2005) 
In recent years corpora annotated with semantic and function labels have seen the light (Palmer et al  2005 Baker et al  1998) and semantic role labelling has taken centre-stage as a challenging new task 
We tested several kernels over standard features (Gildea and Jurafsky 2002 Pradhan et al 2005) and structured features (Moschitti et al 2008) the Polynomial Kernel (PK with a degree of 3) the Tree Kernel (TK) and its combination with the bag of word kernel on the tree leaves (TKL) 
This is from a general belief that each step requires a different set of features (Xue and Palmer 2004) and training these steps in a pipeline takes less time than training them as a joint-inference task 
In this paper we decompose the Moschitti (2004)s predicate-argument feature (PAF) kernel into a Path kernel and a Constituent Structure ker1http//wwwcsuntedu/rada/senseval/senseval3/ 73 nel and then compose them into a hybrid convolution tree kernel 
As with many other statistical parsers (Collins 1999 Charniak 2000) SSN parsers use a history-based model of parsing 
However except from a few tentative experiments (Toutanova et al 2005) grammatical function is not explicitly used by current automatic SRL systems but instead emulated from constituent trees by features like the constituent position and the governing category 
Shallow Semantic Parsing using Support Vector Machines Sameer Pradhan Wayne Ward Kadri Hacioglu James H Martin Center for Spoken Language Research University of Colorado Boulder CO 80303 fspradhanwhwhacioglumarting@cslrcoloradoedu Dan Jurafsky Department of Linguistics Stanford University Stanford CA 94305 jurafsky@stanfordedu Abstract In this paper we propose a machine learning algorithm for shallow semantic parsing extending the work of Gildea and Jurafsky (2002) Surdeanu et al 
ENGLISH GERMAN CHINESE (Marcus et al  1993) (Skut et al  1997) (Xue et al  2002) TrainSet Section 2-21 Sentences 1-18602 Articles 26-270 DevSet Section 22 18603-19602 Articles 1-25 TestSet Section 23 19603-20602 Articles 271-300 Table 3 Experimental setup 
Such kernel engineering as shown in (Moschitti 2004) allows us to experiment with many syntactic/semantic features seamlessly 
In the meanwhile the syntactic structure features hidden in a parse tree have been suggested as an important feature for SRL and need to be further explored in SRL (Gildea and Palmer 2002 Punyakanok et al 2005) 
While some of these models are based on full parse trees (Gildea and Jurafsky 2002 Gildea and Palmer 2002) other methods have been proposed that eschew the need for a full 1There are thirteen semantic role labels for modifiers 
In the first step we adopt the definitions found in PropBank (Palmer et al  2005) defining our own framesets for verbs not in PropBank such as phosphorylate 
To solve these errors we need to explore more such as using n-best parses and the use of several syntactic views (Pradhan et al  2005b) 
On the arg-verb relation rp relative position di distance based on words (w) chunks (c) or the syntactic tree (t) ps standard path pv path variations pi scalar indicator variables on the path (of chunks clauses or other phrase types) common ancestor etc sf syntactic frame (Xue and Palmer 2004) On the complete proposition as sequence of arguments of a proposition 
The top-performing system in the task (Johansson and Nugues 2008) applied a very simple reranking scheme by means of a k-best syntactic output similar to previous attempts (Gildea and Jurafsky 2002 Toutanova et al 2005) to improve semantic role labeling performance by using mul561 tiple parses 
Driven by annotation resources such as FrameNet (Baker et al 1998) and PropBank (Palmer et al 2005) many systems developed in these studies have achieved argument F1 scores near 80% in large-scale evaluations such as the one reported by Carreras and M`arquez (2005) 
We use the standard training development and 580 Feature Types (Gildea and Jurafsky 2002) PHRASE TYPE Syntactic Category of node PREDICATE LEMMA Stemmed Verb PATH Path from node to predicate POSITION Before or after predicate? 
This corpus contains annotations of semantic PASs superimposed on the Penn Treebank (PTB) (Marcus et al  1993 Marcus et al  1994) 
Statistical dependency parsers of English must therefore rely on dependency structures automatically converted from a constituent corpus such as the Penn Treebank (Marcus et al 1993) 
Research has proceeded for decades on manually created lexicons grammars and other semantic resources (Hirst 1987 Pustejovsky 1995 Copestake and Flickinger 2000) in support of deep semantic analysis of language input but such approaches have been labor-intensive and often restricted to narrow domainsThe 1990s saw a growth in the development of statistical machine learning methods across the eld of computational linguistics enabling systems to learn complex linguistic knowledge rather than requiring manual encodingThese methods were shown to be effective in acquiring knowledge necessary for semantic interpretation such as the properties of predicates and the relations to their arguments for example learning subcategorization frames (Briscoe and Carroll 1997) or classifying verbs according to argument structure properties (Merlo and Stevenson 2001 Schulte im Walde 2006)Recently medium-to-large corpora have been manually annotated with semantic roles in FrameNet (Fillmore Ruppenhofer and Baker 2004) PropBank (Palmer Gildea and Kingsbury 2005) and NomBank (Meyers et al2004) enabling the development of statistical approaches specically for SRL 
Features derived from a syntactic parse of the sentence have proven particularly useful (Gildea & Jurafsky 2002) 
And it has been widely reported that in feature-based SRL the performance can be improved by adding several combined features each of which is composed by two single features (Xue and Palmer 2004 Toutanova et al 2005 Zhao et al 2009) 
These systems share two ideas which make them different from the approach presented here they all analyse verb-argument relations and they all use machine learning or probabilistic approaches (Pradhan et al  2005) to assign a label to a new instance 
Most recently the Gildea and Palmer (2002) scores presented here have been improved markedly through the use of support-vector machines as well as additional features for named entity tags headword POS tags and verb clusters for back-off (Pradhan et al 2003) and using maximum-entropy classifiers (He and Gildea 2004 Xue and Palmer 2004) 
In our implementation SVM-Light-TK1 (Moschitti 2004) is modified 
Gildea and Jurafsky (2002) showed barely significant improvements in semantic role classification of NPs for FrameNet roles using distributional clusters 
The availability of annotated corpora like PropBank and FrameNet (Fillmore et al 2001) have provided rapid development of research into SRL (Gildea and Jurafsky 2002 Gildea and Palmer 2002 Surdeanu et al 2003 Chen and Rambow 2003 Gildea and Hockenmaier 2003 Xue and Palmer 2004 Pradhan et al 2004 Pradhan et al 2005) 
Methods for SRL most features used in prior SRL research are generally extended from Gildea and Jurafsky (2002) who used a linear interpolation method and extracted basic flat features from a parse tree to identify and classify the constituents in the FrameNet (Baker et al  1998) 
We have chosen to work with a corpus with parse information the Wall Street Journal WSJ part of the Penn Treebank II corpus (Marcus et al  1993) and to extract chunk information from the parse trees in this corpus 
The significance of syntactic analysis in SRL has been proven by (Gildea and Palmer 2002 Punyakanok et al 2005) and syntactic parsing has been applied by almost all current studies 
Most approaches rely on VerbNet (Kipper et al  2000) and FrameNet (Baker et al  1998) to provide associations between verbs and semantic roles that are then mapped onto the current instance as shown by the systems competing in semantic role labelling competitions (Carreras and Marquez 2004 Carreras and Marquez 2005) and also (Gildea and Jurafsky 2002 Pradhan et al  2005 Shi and Mihalcea 2005) 
The overall performance of our semantic role labeling approach is not competitive with leading contemporary systems which typically employ support vector machine learning algorithms with syntactic features (Pradhan et al  2005) or syntactic tree kernels (Moschitti et al  2006) 
In previous work using the PropBank corpus Gildea and Palmer (2002) developed a system to predict semantic roles from sentences and their parse trees as determined by the statistical parser of Collins (1999) 
There has been plenty of work on machine learning models for semantic role labeling starting with the work of Gildea and Jurafsky (2002) and including CoNLL shared tasks (Carreras and M`arquez 2005) 
In case of the CCG parses as reported by Gildea and Hockenmaier (2003) the mismatch was about 23% 
Two studies that compared the respective performances of constituent-based and dependency-based SRL systems (Pradhan et al 2005 Swanson and Gordon 2006) both using automatic parsers reported that the constituent-based systems outperformed the dependency-based ones by a very wide margin 
The bulk of this recent work views semantic analysis as a tagging or labeling problem and has applied various supervised machine learning techniques to it (Gildea and Jurafsky (2000 2002) Gildea and Palmer (2002) Surdeanu et al 
EmpiricalEvaluationsofSRLSystems Many experimental studies have been conducted since the work of Gildea and Jurafsky (2002) including seven international evaluation tasks in ACL-related conferences and workshops the SIGNLL CoNLL shared tasks in 2004 and 2005 (Carreras and M`arquez 2004 2005) the SIGLEX Senseval-3 in 2004 (Litkowski 2004) and four tasks in the SIGLEX SemEval in 2007 (Pradhan et al2007 M `arquez et al2007 Baker Ellsworth and Erk 2007 Litkowski and Hargraves 2007)In the subsequent sections we summarize their main features results and conclusions although note that the scores are not directly comparable across different exercises due to differences in scoring and in the experimental methodologies 
Nombank extends the general annotation framework of the English Proposition Bank (Palmer et al  2005) and the English Nombank (Meyers et al  2004) to the annotation of nominalized predicates in Chinese 
On the probabilistic modeling front Gildea and Jurafsky (2002) presented a discriminative model for arguments given the frame Thompson et al 
Semantic role labeling based on predicate argument structure was first explored in detail by (Gildea and Jurafsky 2002) 
It turns out the heuristics that are first proposed in Xue and Palmer (2004) to prune out non-arguments for verbal predicates can be easily adapted to detect arguments for the nominalized predicates as well so in our experiments we adopt the latter approach 
From the point of view of learning architectures and study of feature relevance it is also worth mentioning the following recent works (Punyakanok et al  2004 Moschitti 2004 Xue and Palmer 2004 Pradhan et al  2005a) 
We evaluate against PropBank (Palmer et al 2005) obtaining roughly 70% accuracy when evaluated on the prepositional arguments and more than 80% for the entire argument set 
We used three features that were introduced by Gildea and Hockenmaier (2003) Phrase type This is the category of the maximal projection between the two words the predicate and the dependent word 
Semantic Role Labeling is the process of annotating the predicate-argument structure in text with seThis research was partially supported by the ARDA AQUAINT program via contract OCG4423B and by the NSF via grants IS-9978025 and ITR/HCI 0086132 mantic labels (Gildea and Jurafsky 2000 Gildea and Jurafsky 2002 Gildea and Palmer 2002 Surdeanu et al  2003 Hacioglu and Ward 2003 Chen and Rambow 2003 Gildea and Hockenmaier 2003 Pradhan et al  2004 Hacioglu 2004) 
We first experiment with the set of features described in Gildea and Palmer (2002) Pred HW Arg HW Phrase Type Position Path Voice 
Conversion to Dependencies 321 Syntactic Dependencies There exists no large-scale dependency treebank for English and we thus had to construct a dependency-annotated corpus automatically from the Penn Treebank (Marcus et al 1993) 
The data consists of six sections of the Wall Street Journal part of the Penn Treebank (Marcus et al  1993) and follows the setting of past editions of the CoNLL shared task training set (sections 15-18) development set (section 20) and test set (section 21) 
This is a purely syntactic resource but we can also include this treebank in the category of multi stratal resources since the PropBank (Palmer et al 2005) and NomBank (Meyers et al 2004) projects have annotated shallow semantic structures on top of it 
In particular the well-defined semantic role labeling (SRL) task has been drawing more and more attention in recent years due to its importance in deep NLP applications such as question answering (Narayanan and Harabagiu 2004) information extraction (Surdeanu et al 2003) and co-reference resolution (Ponzetto and Strube 2006) 
Automatic semantic role labeling was first introduced by Gildea and Jurafsky (2002) 
Here we formulate an ILP model whose form is different from the model in (Punyakanok et al 2004 Koomen et al 2005) 
We used the default linear (Linear) and polynomial (Poly) kernels for the evaluations with the standard features defined in (Gildea and Jurafsky 2002) 
The Proposition Bank (PropBank) corpus (Palmer et al  2005) avoids this issue by using theory-agnostic labels (Arg0 Arg1  Arg5) and by defining those labels to have verb-specific meanings 
It has been a popular task since the availability of the PropBank and FrameNet annotated corpora (Palmer et al 2005) the seminal work of (Gildea and Jurafsky 2002) and the successful CoNLL evaluation campaigns (Carreras and M`arquez 2005) 
In addition Moschitti (2004) only study the task of argument classification while in our experiment we report the experimental results on both identification and classification 
The results from CoNLL shared tasks in 2005 and 2008 (Carreras and Marquez 2005 Koomen et al 2005 Surdeanu et al 2008 Johansson and Nugues 2008) further show that SRL pipeline may be one of the standard to achieve a state-of-the-art performance in practice 
Comparing semantic role labelling based on chunked input to the better semantic role labels retrieved based on parsed trees (Gildea and Palmer 2002) conclude that parsing is necessary 
Predicate NP expansion rule This is the noun equivalent of the verb sub-categorization feature used by Gildea and Jurafsky (2002) 
Its architecture is based on the top system in the 2005 CoNLL shared task (Koomen et al  2005) modified to process raw text using lower level processors but maintaining 6 good real time performance 
The approach proposed in (Moschitti 2004) selects the minimal subtree that includes a predicate with its argument 
Second we emphasize that the F1 of PK is surprisingly high since it exploits the set of standard SRL feature (Gildea and Jurafsky 2002 Pradhan et al 2005) originally developed for English and left unmodified for Italian 
In the following we report on the selection of biomedical verbs and explain the difference between their meaning in PropBank (Palmer et al  2005) developed by the University of Pennsylvania and their meaning in BioProp (a biomedical proposition bank) 
It has previously been noted (Pradhan et al 2005) that a segment-based evaluation may be unfavorable to a dependency-based system and that an evaluation that scores argument heads maybe more indicative of its true performance 
Their features are usually extended from Gildea and Jurafsky (2002)s work which uses flat information derived from a parse tree 
A syntactic parse is however a representation that is very closely tied with the surface-form of natural language in contrast to Semantic Role Labeling (SRL) which adds a layer of predicate-argument information that generalizes across different syntactic alternations (Palmer et al 2005) 
The ones denoted with asterisks (*) were not present in (Toutanova et al  2005) 
A similar drawback can be found in (Gildea and Hockenmaier 2003) where a parse tree path was defined in terms of Combinatory Categorial Grammar (CCG) types using grammatical relations between predicate and arguments 
Governing category as described in (Gildea and Jurafsky 2002) 
ILP method was first applied to SRL in (Punyakanok et al 2004) 
The above problems are particularly critical for frame-based shallow semantic parsing where as opposed to more syntactic-oriented semantic labeling schemes (as Propbank (Palmer et al 2005)) a significant mismatch exists between the semantic descriptors and the underlying syntactic annotation level 
The standard features at the top of the table were defined by (Gildea and Jurafsky 2002) and the rest are other useful lexical and structural features identified in more recent work (Pradhan et al  2004 Surdeanu et al  2003 Xue and Palmer 2004) 
Regarding the learning component of the systems we find pure probabilistic models (Gildea and Jurafsky 2002 Gildea and Palmer 2002 Gildea and Hockenmaier 2003) Maximum Entropy (Fleischman et al  2003) generative models (Thompson et al  2003) Decision Trees (Surdeanu et al  2003 Chen and Rambow 2003) and Support Vector Machines (Hacioglu and Ward 2003 Pradhan et al  2003a Pradhan et al  2003b) 
We used a regularization parameter (option -c) equal to 1 and = 04 (see (Moschitti 2004)) 
Many researchers (Gildea and Jurafsky 2002 Pradhan et al  2005a) use feature-based methods Figure 1 Semantic role labeling in a phrase structure syntactic tree representation for argument identification and classification in building SRL systems and participating in evaluations such as Senseval-3 1 CoNLL-2004 and 2005 shared tasks SRL (Carreras and M`arquez 2004 Carreras and M`arquez 2005) where a flat feature vector is usually used to represent a predicate-argument structure 
Likelihood (PML) Estimation Gildea & Jurafsky (2002) Gildea & Hockenmaier (2003) and Palmer et al  (2005) use a statistical approach based on Maximum Likelihood method for SRL with different backoff comb in a Predicate Arg0 Argm-LOC 181 P(r | hw pt prepp) P(r | pt pa pr pp) P(r | pt di vo pr pp) P(r | hw pr pp) P(r | pt pr pp) P(r | pr pp) Local Global P(r | hw pp) P(r | pt di vo pp) tion methods in which selected probabilities are combined with linear interpolation 
We formulated this task after the well-studied task of semantic role labeling in English (eg  Gildea and Jurafsky 2002 Carreras and Mrques 2005) whose goal is to assign one of 20 semantic role labels to each phrase in a sentence with respect to a given predicate based on the annotations provided by PropBank (Palmer et al  2005) 
As a result if we do not compare the machine learning methods involved in the two approaches but rather the features used in learning our features are a natural generalization of (Chen and Rambow 2003) 
Such systems extract information from some types of syntactic units (clauses in (Fillmore and Atkins 1998 Gildea and Jurafsky 2002 Hull and Gomez 1996) noun phrases in (Hull and Gomez 1996 Rosario et al  2002)) 
Most of the following work focused on feature engineering (Xue and Palmer 2004 Jiang et al  2005) and machine learning models (Nielsen and Pradhan 2004 Pradhan et al  2005a) 
Related work Gildea and Jurafsky (2002) were the first to describe a statistical system trained on the data from the FrameNet project to automatically assign semantic roles 
In particular the well-defined semantic role labeling (SRL) task has been drawing increasing attention in recent years due to its importance in natural language processing (NLP) lications such as question answering (Narayanan and Harabagiu 2004) information extraction (Surdeanu et al 2003) and co-reference resolution (Kong et al 2009) 
Next the probabilities Pr i j F ip are combined with the probabilities Pfr 1 
These include models for part-of-speech tagging (Toutanova et al  2003) semantic-role labeling (Punyakanok et al  2005 Pradhan et al  2005b) and Penn Treebank parsing (Charniak and Johnson 2005) 
In this paper we describe a domain-independent IE paradigm that is based on predicate-argument structures identified automatically by two different methods (1) the statistical method reported in (Gildea and Palmer 2002) and (2) a new method based on inductive learning which obtains 17% higher Fscore over the first method when tested on the same data 
In the second row (PARA+PML) is trained on all datasets (WSJ 02 to 21) for the BR+RL task (to recognize argument boundaries and label arguments) on the test data WSJ 23 with an improvement of F1828 in comparison to the result of Palmer et al  (2005) given in the 185 first row 
There have been many different proposals on how to maintain syntactic locality (Xia 1999 Chen and VijayShanker 2000) and SRL locality (Chen and Rambow 2003 Shen and Joshi 2005) when extracting LTAG etrees from a Treebank 
P01R01R03R05		In both English and Chinese PropBank (Palmer et al 2005 Xue and Palmer 2003) and English and Chinese NomBank (Meyers et al 2004 Xue 2006) these semantic arguments include core arguments (eg Arg0 for agent and Arg1 for recipient) and adjunct arguments (eg ArgM-LOC for locative argument and ArgM-TMP for temporal argument) 
Many research efforts utilize machine learning (ML) approaches such as support vector machines (Moschitti et al  2004 Pradhan et al  2004) perceptrons (Carreras et al  2004) the SNoW learning architecture (Punyakanok et al  2004) EMbased clustering (Baldewein et al  2004) transformation-based learning (Higgins 2004) memory-based learning (Kouchnir 2004) and inductive learning (Surdeanu et al  2003) 
The system introduced in (Toutanova et al  2005) implements a joint model that captures dependencies among arguments of a predicate using log-linear models in a discriminative re-ranking framework 
On the other hand the previous researches (Gildea and Palmer 2002 Punyakanok et al  2005) have also recognized the 74 
The problem is given that a node has a core argument label decide what the correct label is Other researchers have also looked at this subproblem (Gildea and Jurafsky 2002 Toutanova et al  2005 Pradhan et al  2005a Xue and Palmer 2004) 
For this task we utilized the August 2005 release of the Charniak parser with the default speed/accuracy settings (Charniak 2000) which required roughly 360 hours of processor time on a 25 GHz PowerPC G5 
Most of the features we designed are features that have become standard for the SRL task (Gildea and Jurafsky 2002 Xue and Palmer 2004 Carreras and M`arquez 2004 Carreras and M`arquez 2005) 
Unlike (Poon and Domingos 2009) we do not use the lambda calculus formalism to define our task but rather treat it as an instance of frame-semantic parsing or a specific type of semantic role labeling (Gildea and Jurafsky 2002) 
This result is better than (Xue and Palmer 2004) and better on gold parses compared to (Toutanova et al  2005 Punyakanok et al  2005b) 
With the unique exception of the exploration inside sibling PP constituents proposed by (Xue and Palmer 2004) 
In (Toutanova et al  2005) it was observed that there are strong dependencies among the labels of the semantic argument nodes of a verb 
This is part of the explanation of why (Charniak 2000) finds that early generation of head tags as in (Collins 1999) is so beneficial 
For the experiments we use SVM-Light-TK toolkit8 (Moschitti 2004 Moschitti 2006) and its SVM-Light default parameters 
Most of the features we use are described in more detail in (Toutanova et al  2005) 
Similarly to Xue and Palmer (2004) Argument identification FE Supp Cop Asp Exist Null Argument None Self_mover Path etc classification Figure 3 FE extraction steps 
With the efforts of many researchers (Carreras and Mrquez 2004 2005 Moschitti 2004 Pradhan et al 2005 Zhang et al 2007) different machine learning methods and linguistics resources are applied in this task which has made SRL task progress fast 
An important baseline study of this process has recently appeared in the literature (Gildea and Jurafsky 2002) 
This can be done with high accuracy when the machine-learning algorithm is powerful and is provided with appropriate features (Hacioglu et al  2003 Pradhan et al  2004b) 
Chen and Rambow (2003) make use of extracted tree-adjoining grammars 
Similar to the heuristic algorithm as proposed in Xue and Palmer (2004) for argument pruning in common shallow semantic parsing the argument pruning algorithm adopted here starts from designating the negation signal as the current node and collects its siblings 
Additionally the dashed lines show those edges which were pruned following Xue and Palmer (2004) only nodes which are siblings to a node on the path from the verb to the root are included in the tree 
A crucial difference from similar approaches such as SRL with PropBank roles (Pradhan et al  2004) is that by identifying relations as part of a frame you have identi ed a gestalt of relations that enables far more inference and sentences from the same passage that use other words from the same frame will be easier to link together 
(Xue and Palmer 2004)) and our experiments show that 91% of the SRL targets can be recovered despite this aggressive pruning 
Recently Palmer et al  (2005) have PropBanked a significant portion of the Treebanked Brown corpus which enables us to perform experiments to analyze the reasons behind the performance degradation and suggest potential solutions 
We use the inference process introduced by (Punyakanok et al  2004) 
It has obvious applications for template-filling tasks such as information extraction and question answering (Surdeanu et al 2003 Moschitti et al 2003) 
To fully evaluate the influence of the LTAG-based featureswe report the identification results on both Gold Standard parses and on Charniak parser output (Charniak 2000)5 
Gildea and Jurafsky (2002) presented a compact set of features across these three types which has served as the core of most of the subsequent SRL work (1) the phrase type headword and governing category of the constituent (2) the lemma voice and subcategorization pattern of the verb and (3) the left/right position of the constituent with respect to the verb and the category path between themExtensions to these features have been proposed in various directionsExploiting the ability of some machine learning algorithms to work with very large feature spaces some authors have largely extended the representation of the constituent and its context including among others rst and last words (and part-of-speech) in the constituent bag-of-words n-grams of part of speech and sequence of top syntactic elements in the constituentParent and sibling constituents in the tree may also be codied with all the previous structural and lexical features (Pradhan et al2005a Surdeanu et al2007)Other authors have designed new features with specic linguistic motivationsFor instance Surdeanu et al(2003) generalized the concept of headword with the content word featureThey also used named entity labels as featuresXue and Palmer (2004) presented the syntactic frame feature which captures the overall sentence structure using the verb predicate and the constituent as pivotsAll these features resulted in a signicant increase in performance 
The PropBank corpus adds a semantic layer to parse trees from the Wall Street Journal section of the Penn Treebank II corpus (Marcus et al 1993) 
Of special interest here Moschitti (2004) proposed Predicate Argument Feature (PAF) kernel under the framework of convolution tree kernel for SRL 
Moschitti (2004) and Che et al 
The task of Semantic Role Labeling (SRL) ie the process of detecting basic event structures such as who did what to whom when and where has received considerable interest in the past few years (Gildea and Jurafsky 2002 Surdeanu et al  2003 Xue and Palmer 2004 Pradhan et al  2005a Carreras and M`arquez 2005) 
Gildea and Palmer (2002) developed a system to predict semantic roles (as defined in PropBank) from sentences and their parse trees as determined by the statistical parser of Collins (1999) 
To achieve high accuracy and resolve the data sparsity problem the method reported in (Gildea and Palmer 2002 Gildea and Jurafsky 2002) employed a backoff solution based on a lattice that combines the model features 
Their effectiveness has been previously shown by (Pradhan et al  2004 Surdeanu et al  2003 Xue et al  2004) 
Syntactic Frame as designed in (Xue and Palmer 2004) 
Our work suggests that feature generalization based on verb-similarity may compliment approaches to generalization based on role-similarity (Gildea and Jurafsky 2002 Baldewein et al  2004) 
To avoid this problem generative models for NLP tasks have often been manually designed to achieve an appropriate representation of the joint distribution such as in the parsing models of (Collins 1997 Charniak 2000) 
We investigate ways to combine hypotheses generated from semantic role taggers trained using different syntactic views one trained using the Charniak parser (Charniak 2000) another on a rule-based dependency parser Minipar (Lin 1998) and a third based on a flat shallow syntactic chunk representation (Hacioglu 2004a) 
In our implementation we use the binary SVMLight (Joachims 1998) and modify the Tree Kernel Tools (Moschitti 2004) to a grammar driven one 
Syntactic frame as described by Xue and Palmer (2004) 2 Experimental Setting and Results We trained the classification models using the complete training set (sections from 02 to 21) 
Moreover by combining polynomial and SST kernels we can improve the classification accuracy (Moschitti 2004) ie tree kernels provide the learning algorithm with many relevant fragments which hardly can be designed by hand 
As for the former (hereafter it is referred to synPth) we continue to use a dependency version of the pruning algorithm of (Xue and Palmer 2004) 
The error rate 100% is lower than that reported by Gildea and Palmer (2002) 172% 
Bank I (PropBank) The PropBank annotation (Palmer et al 2005) classifies the arguments of all the main verbs in the Penn Treebank corpus other than be 
This module is retrained in our SRC experiments using parameters described in (Koomen et al 2005) 
Our results also confirm the findings in (Palmer et al  2005) 
Proposition Bank I (PropBank) The PropBank annotation (Palmer et al 2005) classifies the arguments of all the main verbs in the Penn Treebank corpus other than be 
For SRL high accuracy has been achieved by (i) proposing new types of features (see Table 1 in Section 3 for previously proposed features) (ii)modeling the predicate frame set by capturing dependencies between arguments (Gildea and Jurafsky 2002 Pradhan et al  2004 Toutanova et al  2005 Punyakanok et al  2005a) (iii) dealing with incorrect parser output by using more than one parser (Pradhan et al  2005b) 
Most state-of-the-art methods for the latter two tasks use a cascaded architecture they employ syntactic parsers and re-cast the corresponding tasks as pattern matching (Johnson 2002) or classification (Pradhan et al  2005) problems 
Classifier Accuracy (%) SVM 88 Decision Tree (Surdeanu et al  2003) 79 Gildea and Palmer (2002) 77 Table 11 Argument classification using same features but different classifiers 
Many researchers have investigated applying machine learning to corpus specifically annotated with this task in mind PropBank since 2000 (Chen and Rambow 2003 Gildea and Hockenmaier 2003 Hacioglu et al  2003 Moschitti 2004 Yi and Palmer 2004 Pradhan et al  2005b Punyakanok et al  2005 Toutanova et al  2005) 
Indeed the existence of semantically annotated resources in English such as FrameNet (Baker et al 1998) and PropBank (Kingsbury and Palmer 2003 Palmer et al 2005) corpora have marked a surge in efficient approaches to automatic se 1 In this paper we use Arabic to refer to Modern Standard Arabic (MSA) 
This paper describes a simple yet novel method for constructing sets of 50-best parses based on a coarse-to-fine generative parser (Charniak 2000) 
Figure 1 shows an example of a semantic role labeling annotation in PropBank (Palmer et al  2005) 
Constituent path as described in (Gildea and Jurafsky 2002) and all 3/4/5-grams of path constituents beginning at the verb predicate or ending at the constituent 
The best system presented by the most experienced group on the task (Hacioglu et al  2004) achieved a moderate performance of 6949 at the F1 measure 
SRL bootstrapping a corpus with semantic roles Ever since the pioneering article of Gildea and Jurafsky (2002) there has been an increasing interest in automatic semantic role labeling (SRL) 
To represent the Fpa pairs we used the following features the Phrase Type Predicate Word Head Word Governing Category Position and Voice defined in (Gildea and Jurasfky 2002) the Partial Path Compressed Path No Direction Path Constituent Tree Distance Head Word POS First and Last Word/POS in Constituent SubCategorization and Head Word of Prepositional Phrases proposed in (Pradhan et al  2005) and the Syntactic Frame designed in (Xue and Palmer 2004) 
On the other hand probabilistic inference processes which have been successfully used for SRL (Koomen et al  2005) mandate that each individual candidate argument be associated with its raw activation or confidence in the given model 
With the advent of faster and powerful computers more effective machine learning algorithms and importantly large data resources annotated with relevant levels of semantic information FrameNet (Baker et al  1998) and Prob Bank corpora (Palmer et al  2005) we are seeing a surge in efficient approaches to SRL (Carreras and M`arquez 2005) 
Gildea and Jurafsky (2002) presented an early FrameNet-based SRL system that targeted both verbal and nominal predicates 
In the February 2004 version of the PropBank corpus annotations are done on top of the Penn TreeBank II parse trees (Marcus et al  1993) 
The evaluations were carried out with the SVMlight-TK software (Moschitti 2004) available at http//ai-nlpinfouniroma2it/moschitti/ which encodes the tree kernels in the SVM-light software (Joachims 1999) 
(Gildea and Jurafsky 2002) define this shallow semantic task as a classification problem where the semantic role to be assigned to each constituent is inferred on the basis of probability distributions of syntactic features extracted from parse trees 
However (Pradhan et al  2005a) uses some additional information since it deals with incorrect parser output by using multiple parsers 
During testing the algorithm of enforcing nonoverlapping arguments by (Toutanova et al  2005) is used 
In this section we apply the graph transformation method to the task of identification of semantic roles as annotated in the Proposition Bank (Palmer et al  2005) PropBank for short 
Some other work paid much attention to the robust SRL (Pradhan et al  2005b) and post inference (Punyakanok et al  2004) 
Baseline Features (Pradhan et al  2005) b1 replacement b2 NP NP NN b3 NP b4 Bernanke b5 NPSVPVPPPNPNN b6 left b11 Ben Bernanke NNP NNP b12 NULL VP b13 NULL NULL was VBD b14 S b15 was VBD b16 NULL b17 NULL NULL b18 NP-7 b19 NULL b20 NPS b21 NPSVPVPPPNP Baseline Combined Features (Xue and Palmer 2004) b31 replacement & NP b32 replacement & Bernanke b33 replacement & NPSVPVPPPNPNN b34 replacement & left Table 2 Baseline feature instantiations assuming the current constituent is NP-Ben Bernanke in Figure 1 
For the AST-based classifiers we used a equal to 04 (see (Moschitti 2004)) 
However state-ofthe-art semantic role labelling systems (CoNLL 2005) use parse trees output by state-of-the-art parsers (Collins 1999 Charniak 2000) both for training and testing and return partial trees annotated with semantic role labels 
These works include (Gildea and Jurafsky 2002 Carreras and Marquez 2005 Koomen et al 2005 Marquez et al 2005 Dang and Palmer 2005 Pradhan et al 2005 Toutanova et al 2005 Jiang and Ng 2006 Liu and Ng 2007 Surdeanu et al 2007 Johansson and Nugues 2008 Che et al 2008) 
Finally as CoNLL 2005 has shown that the most important contribution relates on re-ranking predicate argument structures based on one single tree (Toutanova et al  2005) or several trees (Punyakanok et al  2005) we would like to use tree kernels for the re-ranking task 
We trained the parser on the Penn Treebank (Marcus et al  1993) 
Some other works paid much attention to the robust SRL (Pradhan et al  2005b) and post inference (Punyakanok et al  2004) 
In the PropBank1 corpus (Palmer et al  2005) predicate argument relations are marked for the verbs in the text 
As with many other statistical parsers (Collins 1999 Charniak 2000) SSN parsers use a history-based model of parsing 
The organization provided training development and test sets derived from the standard sections of the Penn TreeBank (Marcus et al  1993) and PropBank (Palmer et al  2005) corpora 
In the original formulation for English in Gildea and Jurafsky (2002) it answers the question Is the NP governed by IP or VP? 
Therefore our system exploits the heuristic rules introduced by Xue and Palmer (2004) to filter out simple constituents that are unlikely to be arguments 
For instance many systems used the pruning strategy described in (Xue and Palmer 2004) (x&p) and other systems used the soft pruning rules described in (Pradhan et al  2005a) (softp) 
Recent successes in statistical syntactic parsing based on supervised learning techniques trained on a large corpus of syntactic trees (Collins 1999 Charniak 2000 Henderson 2003) have brought forth the hope that the same approaches could be applied to the more ambitious goal of recovering the propositional content and the frame semantics of a sentence 
The term is most commonly used to describe the automatic identification and labeling of the semantic roles conveyed by sentential constituents (Gildea and Jurafsky 2002) 
In a recent paper on the SRL on verbal predicates for English (Toutanova et al  2005) pointed out that one potential flaw in a SRL system where each argument is considered on its own is that it does not take advantage of the fact that the arguments (not the adjuncts) of a predicate are subject to the hard constraint that they do not have the same label3 
Not only did they achieve new performance benchmarks on parsing the Penn Treebank (Marcus Santorini and Marcinkiewicz 1993) and not only did they serve as the basis of Collins own future work (Collins 2000 Collins and Duffy 2002) but they also served as the basis of important work on parser selection (Henderson and Brill 1999) an investigation of corpus variation and the effectiveness of bilexical dependencies (Gildea 2001) sample selection (Hwa 2001) bootstrapping non-English parsers (Hwa Resnik and Weinberg 2002) and the automatic labeling of semantic roles and predicate-argument extraction (Gildea and Jurafsky 2000 Gildea and Palmer 2002) as well as that of other research efforts 
Following the seminal work of Gildea and Jurafsky (2002) there have been many extensions in machine learning models feature engineering (Xue and Palmer 2004) and inference procedures (Toutanova et al 2005 Surdeanu et al 2007 Punyakanok et al 2008) 
Most current semantic role labeling (SRL) approaches can be classified in one of two classes approaches that take advantage of complete syntactic analysis of text pioneered by (Gildea and Jurafsky 2002) and approaches that use partial syntactic analysis championed by the previous CoNLL shared task evaluations (Carreras and M`arquez 2004) 
These systems use (Charniak 2000)s parse trees both for training and testing as well as various other information sources including sets of n-best parse trees (Punyakanok et al  2005 Haghighi et al  2005) or chunks (Marquez et al  2005 Pradhan et al  2005) and named entities (Surdeanu and Turmo 2005) 
However state-of-theart semantic role labelling systems (CoNLL 2005) use parse trees output by state-of-the-art parsers (Collins 1999 Charniak 2000) both for training and testing and return partial trees annotated with semantic role labels 
SPs in a SRL system For these experiments we modified SwiRL (Surdeanu et al 2007) (a) we matched the gold boundaries against syntactic constituents predicted internally using the Charniak parser (Charniak 2000) and (b) we classified these constituents with their semantic role using a modified version of SwiRLs feature set 
The Path features are designed as a sequential collection of phrase tags by (Gildea and Jurafsky 2002) 
Consider the task of recovering non-local dependencies (such as control WH-extraction topicalization) in the surface syntactic phrase trees produced by the state-of-the-art parser of (Charniak 2000) 
On the other hand despite the relevant work of Gildea and Jurafsky (2002) it is still an open issue whether FrameNet classes and frame elements can be obtained and used automatically because of the richness of the semantic structures employed (Dzikovska et al  2004) 
A related work is reported in (Gildea and Hockenmaier 2003) 
In our experiments we adopt the three-step strategy proposed by (Xue and Palmer 2004) 
In our first set of experiments the features and probability model of the Gildea and Jurafsky (2002) system were applied to the PropBank corpus 
With very few exceptions (eg Collobert and Weston 2007) published SRL methods have used some sort of syntactic structure as input (Gildea and Palmer 2002 Punyakanok et al 2008) 
Governing category as in (Gildea and Jurafsky 2002) 
Third we point out that the polynomial kernel on flat features is more accurate than tree kernels but the design of such effective features required noticeable knowledge and effort (Gildea and Jurafsky 2002) 
It has been shown that maximum entropy models achieve state-of-the-art results on SRL (Xue and Palmer 2004 Toutanova et al 2008) 
Syntactic frame as described by Xue and Palmer (2004) Table 3 Predicate constituent features Models 1/2 The syntactic label of the candidate constituent 
As a result some of the features that undo long distance movement via trace information in the TreeBank as used in (Chen and Rambow 2003) cannot be exploited in our model 
In addition our system exploits the heuristic introduced by (Xue and Palmer 2004) to filter out very unlikely constituents 
The candidate generation stage involves using the heuristic of Xue and Palmer (2004) to generate an over-complete set of argument candidates for each predicate 
We trained and tested on automatic parse trees from Charniaks parser (Charniak 2000) 
Finally we note that 50-best parsing is only a fac1Charniak in (Charniak 2000) cites an accuracy of 895% 
SRL feature extraction has relied on various syntactic representations of input sentences such as syntactic chunks (Hacioglu et al  2004) and full syntactic parses (Gildea and Jurafsky 2002) 
By habit most systems for automatic role-semantic analysis have used Pennstyle constituents (Marcus et al 1993) produced by Collins (1997) or Charniaks (2000) parsers 
Note that the meaning of support verb is slightly different between (Toutanova et al 2005) and (Xue 2006 Jiang and Ng 2006) 32 first includes all syntactic children (children) the second also includes all but excludes the left most and the right most children (noFarChildren) 
VOICE Active or passive relative to predicate HEAD WORD OF PHRASE SUB-CAT CFG expansion of predicates parent Additional Features (Pradhan et al  2004) FIRST/LAST WORD LEFT/RIGHT SISTER PHRASE-TYPE LEFT/RIGHT SISTER HEAD WORD/POS PARENT PHRASE-TYPE PARENT POS/HEAD-WORD ORDINAL TREE DISTANCE Phrase Type with appended length of PATH feature NODE-LCA PARTIAL PATH Path from constituent to Lowest Common Ancestor with predicate node PP PARENT HEAD WORD If parent is a PP return parents head word PP NP HEAD WORD/POS For a PP retrieve the head Word / POS of its rightmost NP Selected Pairs (Xue and Palmer 2004) PREDICATE LEMMA & PATH PREDICATE LEMMA & HEAD WORD PREDICATE LEMMA & PHRASE TYPE VOICE & POSITION PREDICATE LEMMA & PP PARENT HEAD WORD Table 1 Baseline Features and classification models can be chained in a principled way as in Equation 1 
This is the feature introduced by (Xue and Palmer 2004) 
A recent release of the PropBank (Palmer et al  2005) corpus of semantic role annotations of Treebank parses contained 112917 labeled instances of 4250 rolesets corresponding to 3257 verbs as illustrated by this example for the verb buy 
Hence we now prune our set by keeping only the siblings of all of the verbs ancestors as is common in supervised SRL (Xue and Palmer 2004) 
Typical tags include Agent Patient Source etc and some adjuncts such as Temporal Manner Extent etc Since the arguments can provide useful semantic information the SRL is crucial to many natural language processing tasks such as Question and Answering (Narayanan and Harabagiu 2004) Information Extraction (Surdeanu et al 2003) and Machine Translation(Boas 2002) 
However this mismatch is significantly less than the 23% mismatch reported in (Gildea and Hockenmaier 2003) between the CCGBank and an earlier version of the PropBank 
While some of these models are based on full parse trees (Gildea and Jurafsky 2002 Blaheta 2004) other methods have been proposed that eschew the need for a full parse (CoNLL 2004 CoNLL 2005) 
These spans were proposed using a high-recall heuristic (Xue and Palmer 2004) 
Predicate the given predicate lemma Voice whether the predicate is realized as an active or passive construction (Pradhan et al  2004 claim approximately 11% of the sentences in PropBank use a passive instantiation) Phrase Type the syntactic category (NP PP S etc) 
An alternative tree kernel representation proposed in (Moschitti 2004) is the selection of the minimal tree subset that includes a predicate with only one of its arguments 
We also experimented with various feature combinations inspired by the features used in (Xue and Palmer 2004) 
Our results also provide some new insights into the discussion about the necessity of parsing for function or semantic role labelling (Gildea and Palmer 2002 Punyakanok et al  84 2005) showing that parsing is beneficial 
The two main resources are PropBank (Palmer et al 2005) and FrameNet (Ruppenhofer et al 2006) 
We used two different corpora PropBank (wwwcisupennedu/ace) along with PennTree bank 2 (Marcus et al  1993) and FrameNet 
The features are listed as follows Path The path features are similar to the path feature which is designed by (Gildea and Jurafsky 2002)A path is a sequential collection of phrase tags 
Table 3 compares the performance comparison among our Hybrid convolution tree kernel Moschitti (2004)s PAF kernel standard flat features with Linear kernels and Poly kernel (d = 2) 
The same as Moschitti (2004) we also set the = 04 in the computation of convolution tree kernels 
If the full feature set is selected the probability is calculated by P (r | pr vo pt di hw pa pp) = # (r pr vo pt di hw pa pp) / # (pr vo pt di hw pa pp) Gildea & Jurafsky (2002) claims there is a trade-off between more-specific distributions which have higher accuracy but lower coverage and less-specific distributions which have lower accuracy but higher coverage and that the selection of feature subsets is exponential and that selection of combinations of different feature subsets is doubly exponential which is NPcomplete 
Content words which add informative lexicalized information different from the head word were detected using the heuristics of (Surdeanu et al  2003) 
Our results confirm the findings in (Palmer et al  2005) 
NSTs with Tree Kernels To implement the re-ranking model we follow the approach described in (Toutanova et al  2005) 
The data consists of sections of the Wall Street Journal part of the Penn TreeBank (Marcus et al  1993) with information on predicate-argument structures extracted from the PropBank corpus (Palmer et al  2005) 
It is shown that applying the Tree-Based Predicate Argument Recognition Algorithm (PARA) to the data as a preprocessing stage allows kNN and PML to deliver F1 6861 and 7102 respectively on the WSJ23 and F1 5696 and 6055 on the Brown Corpus an increase of 828 in F1 measurement over the most recent published PML results for this problem (Palmer et al  2005) 
In (Pradhan et al  2005b) some experiments were conducted on SRL systems trained using different syntactic views 
The probabilities P(r i jF i p) are combined with the probabilities P(fr 1n gjp) for a set of roles appearing in a sentence given a predicate using the following formula P(r 1n jF 1n p) P(fr 1n gjp) Y i P(r i jF i p) P(r i jp) This approach described in more detail in Gildea and Jurafsky (2002) allows interaction between the role assignments for individual constituents while making certain independence assumptions necessary for efficient probability estimation 
To some extent function labels overlap with semantic role labels as defined in PropBank (Palmer et al  2005) 
In recent years tree kernels have been shown to be interesting approaches for the modeling of syntactic information in natural language tasks eg syntactic parsing (Collins and Duffy 2002) relation extraction (Zelenko et al  2003) Named Entity recognition (Cumby and Roth 2003 Culotta and Sorensen 2004) and Semantic Parsing (Moschitti 2004) 
In recent years many advances have been made on SRL using singular syntactic view such as constituent (Gildea and Jurafsky 2002 Xue and Palmer 2004 Surdeanu et al 2007) dependency (Hacioglu 2004 Johansson and Nugues 2008 Zhao et al 2009) and CCG (Chen and Rambow 2003 Boxwell et al 2009) 
Initially 7 features were proposed by (Gildea and Jurafsky 2002) and all following research has used these features and some additional ones 
Several systems have incorporated such dependencies for example (Gildea and Jurafsky 2002 Pradhan et al  2004 Thompson et al  2003) and several systems submitted in the CoNLL-2004 shared task (Carreras and M`arquez 2004) 
Features and feature selection 41 Baseline NomBank SRL features Table 1 lists the baseline features we adapted from previous PropBank-based SRL systems (Pradhan et al  2005 Xue and Palmer 2004) 
The rationale for using a simple DL learner is given in (Gildea and Jurafsky 2002) where essentially it based on their experience with the setting of backoff weights for smoothing it is stated that the most specific single feature matching the training data is enough to predict the SRL on test data 
Because predicate lexical information is used for less than 5% of the branching decisions the generated classifier scales better than the statistical method from (Gildea and Palmer 2002) to unknown predicates 
Though several pruning algorithms have been raised (Xue and Palmer 2004) the policies are all in global style 
Due to the sparsity of the head word feature we also use the part-of-speech of the head word following Surdeanu et al (2003) 
Looking at the description of the different systems it becomes clear that the general type of features used in this edition is strongly based on previous work on the SRL task (Gildea and Jurafsky 2002 Surdeanu et al  2003 Pradhan et al  2005a Xue and Palmer 2004) 
Regarding the design of features for predicate argument pairs we can use the attribute-values defined in (Gildea and Jurasfky 2002) or tree structures (Moschitti 2004) 
We also compare with the CCG-based SRL presented in (Gildea and Hockenmaier 2003)7 which has a similar motivation as this paper except they use the Combinatory Categorial Grammar formalism and the CCGBank syntactic Treebank which was converted from the Penn Treebank 
In order to generalize the path feature (see Table 1 in Section 3) which is probably the most salient (while being the most data sparse) feature for SRL previous work has extracted features from other syntactic representations such as CCG derivations (Gildea and Hockenmaier 2003) and dependency trees (Hacioglu 2004) or integrated features from different parsers (Pradhan et al  2005b) 
As previously observed (Pradhan et al  2004) including modifying arguments in sequence features is not helpful 
Gildea and Jurafsky (2002) describe a statistical system trained on the data from the FrameNet project to automatically assign semantic roles 
As a guideline for interpreting these results with 8167 observations the threshold for statistical significance with p < 05 is a 10% absolute difference in performance (Gildea and Jurafsky 2002) 
Previous approaches to the SRL task have made use of a full syntactic parse of the sentence in order to define argument boundaries and to determine the role labels (Gildea and Palmer 2002 Chen and Rambow 2003 Gildea and Hockenmaier 2003 Pradhan et al  2003 Pradhan et al  2004 Surdeanu et al  2003) 
On the one hand SSTs provide learning algorithms with richer information which may be critical to capture syntactic properties of parse trees as shown for example in (Zelenko et al  2003 Moschitti 2004) 
For example we found that the SCF tree kernel (Moschitti 2004) improves the AX multi classifier AX AM CX RX # train 
The PropBank superimposes an annotation of semantic predicate-argument structures on top of the Penn Treebank (PTB) (Marcus et al  1993 Marcus et al  1994) 
Instead of extracting a typical standard path feature from the derived tree (Chen and Rambow 2003) uses the path within the elementary tree from the predicate to the constituent argument 
In addition our system exploits a heuristic modified from that introduced by (Xue and Palmer 2004) to filter out very unlikely constituents 
Previous research (Gildea and Jurafsky 2002 Pradhan et al  2004 Carreras and M`arquez 2004) has identified many useful features for local identification and classification 
Two frameworks for semantic roles have found wide use in the community PropBank (Palmer et al 2005) and FrameNet (Fillmore et al 2003) 
While almost all systems use the standard path of (Gildea and Jurafsky 2002) many have explored variations of it 
By then "Earthquake" was trending on Twitter Search with thousands of updates 2  However it is a daunting task for people to find out information they are interested in from such a huge number of news tweets thus motivating us to conduct some kind of information 2 http//blogtwittercom/2008/07/twitter-as-news-wirehtml 698 extraction such as event mining where SRL plays a crucial role (Surdeanu et al 2003) 
PropBank contains about 53700 sentences and a xed split between training and testing which has been used in other researches eg (Gildea and Palmer 2002 Surdeanu et al  2003 Hacioglu et al  2003) 
(Palmer et al  2005) is an annotation of one million words of the Wall Street Journal portion of the Penn Treebank II (Marcus et al  1994) with predicate-argument structures for verbs using semantic role labels for each verb argument 
For a fair comparison our system was among the best at CoNLL-04 where the best system (Hacioglu et al  2004) achieve a 6949 F1 score 
The first work to tackle SRL as an independent task is (Gildea and Jurafsky 2002) which presented a supervised model trained and evaluated on FrameNet 
Annotations similar to these have been used to create automated semantic role labeling systems (Pradhan et al  2005 Moschitti et al  2006) for use in natural language processing applications that require only shallow semantic parsing 
The Release 3 of the Penn Treebank contains the hand parsed syntactic trees of a subset of the Brown Corpus sections F G K L M N P and R Palmer et al  (2005) have recently PropBanked a significant portion of this Treebanked Brown corpus 
In contrast with features from shallow parsing previous work (Gildea and Palmer 2002 Punyakanok et al  2005b) has shown the necessity of full syntactic parsing for SRL 
While the Path feature has been identified to be very important for the argument identification task it is one of the most sparse features and may be difficult to train or generalize (Pradhan et al  2004 Xue and Palmer 2004) 
Given the variability in the sets of roles used across the computational resources an important issue is the extent to which different role sets affect the SRL task as well as subsequent use of the output in other NLP applicationsGildea and Jurafsky (2002) initiated this type of investigation by exploring whether their results were dependent on the set of semantic roles they usedTo this end they mapped the FrameNet frame elements into a set of abstract thematic roles (ie more general roles such as Agent Theme Location) and concluded that their system could use these thematic roles 149 Computational Linguistics Volume 34 Number 2 without degradationSimilar questions must be investigated in the context of PropBank where the framesets for the verbs may have signicant domain-specic meanings and arguments due to the dependence of the project on WSJ dataGiven the uncertainty in the linguistic status of semantic role lists and the lack of evidence about which types of roles would be most useful in various NLP tasks an important ongoing focus of attention is the value of mapping between the role sets of the different resources (Swier and Stevenson 2005 Loper Yi and Palmer 2007 Yi Loper and Palmer 2007) 
Accordingly our basic system is similar to the one proposed in (Pradhan et al  2005a) and it is hereby described 
In recent years the availability of large human-labeled corpora such as PropBank (Palmer et al  2005) and FrameNet (Baker et al  1998) has made possible a statistical approach of identifying and classifying the arguments of verbs in natural language texts 
Generally speaking these SRL approaches use a two-stage architecture i) argument identification ii) argument classification to solve the task as a derivation of Gildea and Jurafskys pioneer work (Gildea and Jurafsky 2002) 
Kernels on complete predicate argument structures The type of a target argument strongly depends on the type and number of the predicates arguments1 (Punyakanok et al  2005 Toutanova et al  2005) 
In (Punyakanok et al 2004) several more constraints are considered 
As this task is recognized as an important step after (or the last step of) syntactic analysis many studies have been conducted to achieve accurate semantic role labeling (Gildea and Jurafsky 2002 Moschitti 2004 Hacioglu et al  2004 Punyakanok et al  2004 Pradhan et al  2005a Pradhan et al  2005b Toutanova et al  2005) 
For NomBank SRL fea140 Baseline Features (Pradhan et al  2005) b1 predicate stemmed noun b2 subcat grammar rule that expands the predicates parent b3 phrase type syntactic category of the constituent b4 head word syntactic head of the constituent b5 path syntactic path from the constituent to the predicate b6 position to the left or right of the predicate b11 first or last word/POS spanned by the constituent (b11FW b11LW b11FP b11LP) b12 phrase type of the left or right sister (b12L b12R) b13 left or right sisters head word/POS (b13LH b13LP b13RH b13RP) b14 phrase type of parent b15 parents head word or its POS (b15H b15P) b16 head word of the constituent if its parent has phrase type PP b17 head word or POS tag of the rightmost NP node if the constituent is PP (b17H b17P) b18 phrase type appended with the length of path b19 temporal keyword eg Monday b20 partial path from the constituent to the lowest common ancestor with the predicate b21 projected path from the constituent to the highest NP dominating the predicate Baseline Combined Features (Xue and Palmer 2004) b31 b1 & b3 b32 b1 & b4 b33 b1 & b5 b34 b1 & b6 Table 1 Baseline features for NomBank SRL tures we use this set of more specific mappings to replace the morphological mappings based on WordNet 
Automatic accurate and wide-coverage techniques that can annotate naturally occurring text with semantic argument structure play a key role in NLP applications such as Information Extraction (Surdeanu et al  2003 Harabagiu et al  2005) Question Answering (Narayanan and Harabagiu 2004) and Machine Translation (Boas 2002 Chen and Fung 2004) 
It includes several Wall Street Journal sections with parse-trees from both Charniaks (2000) parser and Collins (1999) parser 
The release of semantically annotated corpora such as FrameNet (Baker et al  1998) and PropBank (Palmer et al  2003) has made it possible to develop high-accuracy statistical models for automated semantic role labeling (Gildea and Jurafsky 2002 Pradhan et al  2004 Xue and Palmer 2004) 
Several machine learning approaches for argument identi cation and classi cation have been developed (Gildea and Jurasfky 2002 Gildea and Palmer 2002 Surdeanu et al  2003 Hacioglu et al  2003) 
Features Arg P Arg R Arg a3a5a4 Role A FS1 8496 8426 8461 7876 FS1 + hPos 9224 8450 8820 7904 FS1 + cw cPos 9219 8467 8827 8080 FS1 + cNE 8393 8569 8480 7985 FS1 + NE flags 8778 8571 8673 8128 FS1 + pvcSum + 8488 8277 8381 7862 pvcMax FS1 + FS2 9162 8506 8822 8305 Table 1 Inductive learning results for argument identification and role assignment Model Implementation Arg a3a5a4 Role A Statistical (Gildea and Palmer) 828 This study 7186 7887 Decision Trees FS1 8461 7876 FS1 + FS2 8898 8374 Table 2 Comparison of statistical and decision tree learning models of the decision-tree-based method against the results obtained by the statistical approach reported in (Gildea and Palmer 2002) 
The best performance was obtained by the SVMbased IOB tagger of (Hacioglu et al  2004) which almost reached the performance of 70 in F1 on the test 
Its potential arguments A are extracted according to (Xue and Palmer 2004) (3) For each pair < pa > P A if a covers exactly the words of semantic role of p put minimal subtree < pa > into positive example set (T+r ) else put it in the negative examples (Tr ) In our experiments we set = 05 
While previous programs with similar goals (Gildea and Jurafsky 2002) were statistics-based this tool will be based completely on hand-coded rules and lexical resources 
For generating constituency trees we used the Charniak parser (Charniak 2000) whereas we applied LTH syntactic parser (described in (Johansson and Nugues 2008a)) to generate dependency trees 
As for SRL on news most researchers used the pipelined approach ie dividing the task into several phases such as argument identification argument classification global inference etc and conquering them individually (Xue and Palmer 2004 Koomen et al 2005 Cohn and Blunsom 2005 Punyakanok et al 2008 Toutanova et al 2005 Toutanova et al 2008) 
Pradhan et al 2005) analyzing the complex input syntax trees (Moschitti 2004 Liu and Sarkar 2007) exploiting the complicated output the predicate-structure (Toutanova et al 2005) as well as capturing paradigmatic relations between predicates (Gordon and Swanson 2007) 
The rst step in SRL typically consists of ltering (or pruning) the set of argument candidates for a given predicateBecause arguments may be a continuous or discontinuous sequence of words any subsequence of words in the sentence is an argument candidateExhaustive exploration of this space of candidates is not feasible because it is both very large and imbalanced (ie the vast majority of candidates are not actual arguments of the verb)The simple heuristic rules of Xue and Palmer (2004) are commonly used to perform ltering because they greatly reduce the set of candidate arguments while maintaining a very high recall 
Recent releases of the Charniak parser (Charniak 2000) have included an option to provide the top k parses of a given sentence according to the probability model of the parser 
Rich annotations of corpora has allowed for the development of techniques for recovering deep linguistic structures syntactic non-local dependencies (Johnson 2002 Hockenmaier 2003 Dienes 2004 Jijkoun and de Rijke 2004) and semantic arguments (Gildea 2001 Pradhan et al  2005 Toutanovaetal 
We train our models with verb instances extracted from three parsed corpora (1) the Wall Street Journal section of the Penn Treebank (PTB) which was parsed by human annotators (Marcus et al  1993) (2) the Brown Laboratory for Linguistic Information Processing corpus of Wall Street Journal text (BLLIP) which was parsed automatically by the Charniak parser (Charniak 2000) and (3) the Gigaword corpus of raw newswire text (GW) which we parsed ourselves with the Stanford parser 
Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling 
In the statistical NLP community the most widely used grammatical resource is the Penn Treebank (Marcus et al 1993) 
Our method is similar to the Moschitti (2004)s predicate-argument feature (PAF) kernel 
Consider for example a sentence such as The authority dropped at midnight Tuesday to $ 280 trillion (taken from section 00 of PropBank (Palmer et al  2005)) 
The growing interest in learning deeper information is to a large extent supported and due to the recent development of semantically annotated databases such as FrameNet (Baker et al  1998) or the Proposition Bank (Palmer et al  2005) that can be used as training resources for a number of supervised learning paradigms 
Details of them can be found in (Koomen et al  2005) 
Parse tree paths were used for semantic role labeling by Gildea and Jurafsky (2002) as descriptive features of the syntactic relationship between predicates and their arguments in the parse tree of a sentence 
A careful analysis of such features reveals that most of them are syntactic tree fragments of training sentences thus a natural way to represent them is the adoption of tree kernels as described in (Moschitti 2004) 
Gildea and Hockenmaier (2003) report that using features extracted from a Combinatory Categorial Grammar (CCG) representation improves semantic labeling performance on core arguments 
Regarding the classification properties we studied the argument labeling accuracy of ST and SST kernels and their combinations with the standard features (Gildea and Jurafsky 2002) 
In this paper we find that the use of deep linguistic representations to predict these semantic labels are more effective than the generally more surface-syntax representations previously employed (Gildea and Palmer (2002)) 
We have experimentally evaluated 30 features based on the previous work in semantic role labelling (Gildea and Jurafsky 2002 Pradhan et al  2004 Xue and Palmer 2004) Lexical features (5) predicate (verb) first phrase word last phrase word and words immediately before and after the phrase 
SRL systems (Gildea and Jurafsky 2002 Gildea and Palmer 2002) have extensively used features defined over Penn Treebank phrase structure trees 
To avoid explicit feature engineering on trees (Moschitti 2004) used convolution kernels on selective portions of syntactic trees 
These features are common to many SRL systems and are described in Xue and Palmer (2004) 
Section 2 reports on the parser that produces predicate-argument labels and compares it against the parser introduced in (Gildea and Palmer 2002) 
Kernels for SRL Moschitti (2004) proposed to apply convolution tree kernels (Collins and Duffy 2001) to SRL 
The word segmenter achieves the performance of 961 in F1-measure while the Berkeley parser gives a performance of 825 and 855 in F1measure on golden and automatic word segmentation respectively 2  1 In addition SVMLight with the tree kernel function (Moschitti 2004) 3 is selected as our classifier 
However there is not much research on combined use of different syntactic views (Pradhan et al 2005) on the feature level of SRL 
Moreover we modeled SRL as a re-ranking task in line with (Toutanova et al  2005) 
In addition while the system described here is based on pipelined classification recent research on semantic role labeling has shown that significant performance improvements can be gained by exploiting interdependencies between arguments (Toutanova et al  2005) 
Finally we would like to experiment with more sophisticated kernels the tree kernels described in (Moschitti 2004) ie models that have shown a lot of promise for the English SRL process 
They are a combination of features introduced by Gildea and Jurafsky (2002) ones proposed in Pradhan et al 
To address these errors we added two additional parse representations i) Minipar dependency parser and ii) chunking parser (Hacioglu et al  2004) 
As proposition banks are semantically annotated versions of a Penn-style treebank they provide consistent semantic role labels across different syntactic realizations of the same verb (Palmer et al  2005) 
Inspired by this idea different resources were constructed including FrameNet (Baker et al 1998) and PropBank (Palmer et al 2005) 
Most of the following works focused on feature engineering (Xue and Palmer 2004 Jiang et al  2005) and machine learning models (Nielsen and Pradhan 2004 Pradhan et al  2005a) 
The concept of support verb was broadly used (Toutanova et al 2005 Xue 2006 Jiang and Ng 2006)4 we here extend it to nouns and prepositions 
SVM-light with linear kernel is used to train on a standard feature set (Xue and Palmer 2004) 
In previous work using the PropBank corpus (Gildea and Palmer 2002) proposed a model predicting argument roles using the same statistical method as the one employed by (Gildea and Jurafsky 2002) for predicting semantic roles based on the FrameNet corpus (Baker et al  1998) 
Note that the form of the ILP model in this paper is different from that in (Punyakanok et al 2004 Koomen et al 2005) in three aspects (1) A special label class null which means no label is assigned was added to the label set in (Punyakanok et al 2004 Koomen et al 2005) 
Indeed the analysis produced by existing semantic role labelers has been shown to benefit a wide spectrum of applications ranging from information extraction (Surdeanu et al 2003) and question answering (Shen and Lapata 2007) to machine translation (Wu and Fung 2009) and summarization (Melli et al 2005) 
These results are comparable to the results from Gildea and Palmer (2002) but only roughly because of differences in corpora 
Gildea and Hockenmaier (2003) present a system for labeling PropBanks semantic roles based on a statistical parser for combinatory categorial grammar (CCG) (Steedman 2000) 
The data consists of sections of the Wall Street Journal part of the Penn TreeBank (Marcus et al  1993) with information on predicate-argument structures extracted from the PropBank corpus (Palmer et al  2005) 
The model of Gildea and Palmer (2002) For the Treebank-based system we use the probability model of Gildea and Palmer (2002) 
We should note that Xue and Palmer (2004) define a similar feature template called syntactic frame which often captures similar information 
(Charniak 2000) describes a different method which achieves very similar performance to (Collins 2000) 
In Table 4 we compare our system for semantic System P R F1 (Pradhan et al  2005) 809 768 788 Here 810 704 753 Table4 Evaluation of our methods for semantic role identification with Propbank (12 first iterations) 
As a result they show that the oracle f-score improves by over 2 points over the (Gildea and Hockenmaier 2003) oracle results for the numbered arguments only (A0  A5) 
Recent successes in statistical syntactic parsing based on supervised techniques trained on a large corpus of syntactic trees (Collins 1999 Charniak 2000 Henderson 2003) have brought the hope that the same approach could be applied to the more ambitious goal of recovering the propositional content and the frame semantics of a sentence 
List of verbs for experiments # of Arg Freq senses number /set up 1 2 106 /emerge 1 1 80 /publish 1 2 113 /give 2 3/2 41 /build into 2 2/3 113 /enter 1 2 123 /take place 1 2 230 /pass 3 2 75 /hope 1 2 90 /increase 1 2 167 Table 2 Verb Test adverbial beneficiary(eg give support [to the plan]) object to be compared condition companion (eg talk [with you]) degree frequency location manner purpose or reason range(eg help you [in this aspect]) result(eg increase [to $100]) source(eg increase [from $50] to $100) temporal topic 3 Semantic Parsing 31 Architecture and Classifier Following the architecture of earlier semantic parsers like Gildea and Jurafsky (2002) we treat the semantic parsing task as a 1-of-N classification problem 
Examples are linearly interpolated relative frequency models (Gildea and Jurafsky 2002) SVMs (Pradhan et al  2004) decision trees (Surdeanu et al  2003) and log-linear models (Xue and Palmer 2004) 
Charniak (2000) shows the value his parser gains from parent annotation of nodes suggesting that this information is at least partly complementary to information derivable from lexicalization and Collins (1999) uses a range of linguistically motivated and carefully hand-engineered subcategorizations to break down wrong context-freedom assumptions of the naive Penn treebank covering PCFG such as differentiating base NPs from noun phrases with phrasal modifiers and distinguishing sentences with empty subjects from those where there is an overt subject NP 
Our accuracy is most closely comparable to the 7863% accuracy achieved on the full task by (Pradhan et al  2005a) 
Many classification techniques SVM (Pradhan et al  2004b) perceptrons (Carreras and M`arquez 2004a) Maximum Entropy (Xue and Palmer 2004) etc have been successfully used to solve SRL problems 
In recent work a number of researchers have cast this problem as a tagging problem and have applied various supervised machine learning techniques to it (Gildea and Jurafsky (2000 2002) Blaheta and Charniak (2000) Gildea and Palmer (2002) Surdeanu et al 
We note that (a) the highest performance is reached for d = 3 (b) for PropBank our maximal accuracy (905%) 7f1 assigns equal importance to Precision P and Recall R ie f1 = 2P RP+R  is substantially equal to the SVM performance (88%) obtained in (Hacioglu et al  2003) with degree 2 and (c) the accuracy on FrameNet (852%) is higher than the best result obtained in literature ie 820% in (Gildea and Palmer 2002) 
Most current SRL approaches can be classified in one of two classes approaches that take advantage of complete syntactic analysis of text pioneered by Gildea and Jurafsky (2002) and approaches that use partial syntactic analysis championed by previous evaluations performed within the Conference on Computational Natural Language Learning (CoNLL) (Carreras and M`arquez 2004) 
A0A1R01 stand for ARG0 ARG1 in PropBank (Palmer et al 2005) 
During testing the maxent model computes Baseline Features (Gildea and Jurafsky 2002) pred predicate lemma path path from constituent to predicate ptype syntactic category (NP PP etc) pos relative position to the predicate voice active or passive voice hw syntactic head word of the phrase sub-cat rule expanding the predicates parent Advanced Features (Pradhan et al 2005) hw POS POS of the syntactic head word PP hw/POS head word and POS of the rightmost NP child if the phrase is a PP first/last word first/last word and POS in the constituent parent ptype syntactic category of the parent node parent hw/POS head word and POS of the parent sister ptype phrase type of left and right sister sister hw/POS head word and POS of left and right sister temporal temporal key words present partPath partial path predicate proPath projected path without directions Feature Combinations (Xue and Palmer 2004) pred & ptype predicate and phrase type pred & hw predicate and head word pred & path predicate and path pred & pos predicate and relative position Table 1 SRL features for the baseline model the conditional probability P(a|tpv) of the argument label a given the parse tree t predicate p and constituent node v The classifier outputs the semantic role with the highest probability a = argmax a P(a|tpv) (2) = argmax a P(a|(tpv)) (3) where () is a feature map to an appropriate feature representation 
Predicate argument clustering Some studies showed that verb clustering information could improve performance in semantic role labeling (Gildea and Jurafsky 2002 Pradhan et al 2008) 
For the initial experiments we adopted the approach described by Gildea and Jurafsky (2002) (G&J) and evaluated a series of modifications to improve its performance 
For our baseline SRL model we adopt the features used in other state-of-the-art SRL systems which include the seven baseline features from the original work of Gildea and Jurafsky (2002) additional features taken from Pradhan et al 
The LTAG-spinal Treebank can be used to overcome some of the limitations of the previous work on SRL using LTAG (Liu and Sarkar 2007) uses LTAG-based features extracted from phrase-structure trees as an additional source of features and combined them with features from a phrase-structure based SRL framework (Chen and Rambow 2003) only considers those complement/adjunct semantic roles that can be localized in LTAG elementary trees which leads to a loss of over 17% instances of semantic roles even from gold-standard trees 
Basic features from (Gildea and Jurafsky 2002) predicate lemma and voice phrase type and head word path from phrase to predicate 1 position phrase relative to predicate before or after sub-cat records the immediate structure that expands from predicates parent 2 Additional features proposed by (Surdeanu et al 2003 Pradhan et al  2004 2005) predicate POS head word POS first/last word/POS POS of word immediately before/after phrase path length 1 LCA(Lowest CommonAnce stor) path from phrase to its lowest common ancestor with predicate punctuation immediately before/after phrase path trigrams up to 9 are considered head word named entity label such as PER ORG LOC content word named entity label for PP parent node Additional features proposed by (Xue and Palmer 2004) predicate phrase type predicate head word voice position syntactic frame 1 In Fig 
We tested our model on a Semantic Role Labeling (SRL) benchmark using PropBank annotations (Palmer et al 2005) and automatic Charniak parse trees (Charniak 2000) as provided for the CoNLL 2005 evaluation campaign (Carreras and M`arquez 2005) 
Instead of using the typical parse tree features used in typical SRL models (Chen and Rambow 2003) uses the path within the elementary tree from the predicate to the constituent argument 
It was shown that the identification of such event frames has a significant contribution for many Natural Language Processing (NLP) applications such as Information Extraction (Surdeanu et al  2003) and Question Answering (Narayanan and Harabagiu 2004) 
Different from the widely used 127 feature functions that are based on the syntactic parse tree (Gildea and Jurafsky 2002) we explore the use of LTAG-based features in a simple discriminative decision-list learner 
The baseline feature set is a combination of features introduced by Gildea and Jurafsky (2002) and ones proposed in Pradhan et al  (2004) Surdeanu et al  (2003) and the syntactic-frame feature proposed in (Xue and Palmer 2004) 
(Chen and Rambow 2003) discuss a model for SRL that uses LTAG-based decomposition of parse trees (as is typically done for statistical LTAG parsing) 
All features but the 22 Split path feature are taken from existing semantic role labeling systems see for example (Gildea and Jurafsky 2002 Lim et al 2004 Thompson et al 2006) 
We use data from the CoNLL-2004 shared taskthe PropBank (Palmer et al  2005) annotations of the Penn Treebank (Marcus et al  1993) with sections 1518 as the training set and section 20 as the development set 
In CoNLL-2005 full parsing trees are provided by two full parsers the Collins parser (Collins 1999) and the Charniak parser (Charniak 2000) 
PropBank encodes propositional information by adding a layer of argument structure annotation to the syntactic structures of the Penn Treebank (Marcus et al  1993) 
The task of semantic role labelling(SRL)ashas been defined by previous researchers (Gildea and Jurafsky 2002) requires collecting all the arguments that together with a verb form a predicate argument structure 
Compared to the wealth of studies on verbal SRL (eg Gildea and Jurafsky (2002) Fleischman and Hovy (2003)) there is relatively little work that specifically addresses nominal SRL 
The PML method used here utilizes a modification of the backoff lattice method used by Gildea & Jurafsky (2002) to use a set of basic features specifically the features employed for learning in this paper are Predicate (pr) Voice (vo) Phrase Type (pt) Distance (di) Head Word (hw) Path (pa) Preposition in a PP (pp) and an Actor heuristic 
Most systems for automatic role-semantic analysis have used constituent syntax as in the Penn Treebank (Marcus et al 1993) although there has also been much research on the use of shallow syntax (Carreras and Mrquez 2004) in SRL 
For example Xue and Palmer (2004) reported that SRL performance dropped more than 10% when they used syntactic features from an automatic parser instead of the gold standard parsing trees 
Gildea and Palmer (2002) achieve a recall of 050 a precision of 058 and an F-measure of 054 when using the full parser of Collins (1999) 
(Xue and Palmer 2004)) and our experiments show that 91% of the SRL targets can be recovered despite this aggressive pruning 
The impact of our head-word based scoring is analyzed in Table 3 which compares results when only the head word must be correctly identified (as in Table 2) and to results when both the beginning and end of the argument must be correctly identified in the sentence (as in Gildea and Palmer (2002)) 
Charniak parser (Charniak 2000) is used for POS tagging and full parsing 
During the past few years verbal SRL has dominated the research on SRL with the availability of FrameNet (Baker et al 1998) PropBank (Palmer et al 2005) and the consecutive CoNLL shared tasks (Carreras and Mrquez 2004 & 2005) in English language 
Some of these systems use features based on syntactic constituents produced by a Charniak parser (Pradhan et al  2003 Pradhan et al  2004) and others use only a flat syntactic representation produced by a syntactic chunker (Hacioglu et al  2003 Hacioglu and Ward 2003 Hacioglu 2004 Hacioglu et al  2004) 
To find a smaller set of effective features we start with all the features considered in (Jiang and Ng 2006) in (Xue and Palmer 2004) and various combinations of them for a total of 52 features 
The re-ranking approach is the most promising one as suggested in (Toutanova et al  2005) but it does not clearly reveal if tree kernels can be used to learn the difference between correct or incorrect argument structures 
Introduction Robust syntactic parsers made possible by new statistical techniques (Ratnaparkhi 1997 Collins 1999 2000 Bangalore and Joshi 1999 Charniak 2000) and by the availability of large hand-annotated training corpora (Marcus Santorini and Marcinkiewicz 1993 Abeille 2003) have had a major impact on the field of natural language processing in recent years 
Kernel Setup We use the Constituent Predicate and Predicate-Constituent related features which are reported to get the best-reported performance (Pradhan et al  2005a) as the baseline features 
An important consideration within this general SRL architecture is the combination of systems and input annotationsMost SRL systems include some kind of combination to increase robustness gain coverage and reduce effects of parse errorsOne may combine (1) the output of several independent SRL basic systems (Surdeanu et al2007 Pradhan et al2005b) or (2) several outputs from the same SRL system obtained by changing input annotations or other internal parameters (Koomen et al 2005 Toutanova Haghighi and Manning 2005)The combination can be as simple as selecting the best among the set of complete candidate solutions but usually consists of combining fragments of alternative solutions to construct the nal outputFinally the combination component may involve machine learning or notThe gain in performance from the combination step is consistently between two and three F 1 pointsHowever a combination approach increases system complexity and penalizes efciency 
It has previously been shown that SRL systems need a syntactic structure as input (Gildea and Palmer 2002 Punyakanok et al 2008) 
This follows the pruning heuristic of (Xue and Palmer 2004) often used by SRL algorithms 
In the recent years most works on SRL including two CoNLL shared task in 2004 and 2005 focus on verbal predicates with the availability of PropBank (Palmer et al 2005) 
A wide range of features have been shown to be useful in previous work on semantic role labeling for verbal predicates (Gildea and Jurafsky 2002 Pradhan et al  2004b Xue and Palmer 2004) and our experiments show most of them are also effective for SRL of nominalized predicates 
Perhaps the closest work to that of ours is that of (Pradhan et al  2004a) where they reported preliminary work for analyzing the predicate-argument structure of Chinese nominalizations using a small data set of 630 proposition for 22 nominalizations taken from the Chinese Treebank 
Bank (PropBank) (Palmer et al  2005) annotates the Penn TreeBank with verb argument structure 
FrameNet thus consists of both a computational lexicon and a role-annotated corpusThe existence of such a corpus enabled Gildea and Jurafsky (2002) to develop the rst statistical machine learning approach to SRL using various lexical and syntactic features such as phrase type and grammatical function calculated over the annotated constituentsAlthough this research spurred the current wave of SRL work that has 148 M`arquez Carreras Litkowski and Stevenson Semantic Role Labeling rened and extended Gildea and Jurafskys approach the FrameNet data has not been used extensivelyOne issue is that the corpus is not a representative sample of the language but rather consists of sentences chosen manually to illustrate the possible role assignments for a given lexical itemAnother issue is that the semantic roles are situation-specic rather than general roles like Agent Theme and Location that can be used across many situations and genres 
Full parser of Charniak (2000) 
For instance Gildea and Hockenmaier (2003) reported that a CCG-based parser gives improved results over the Collins parser 
The key of the well-known pruning algorithm raised in (Xue and Palmer 2004) is extracting sisters of ancestors as role candidates 
For each argument we started with the set of features introduced by (Gildea and Jurafsky 2002) 
Methodologically these initial results on a joint solution to parsing and semantic role labelling provide the first direct test of whether parsing is necessary for semantic role labelling (Gildea and Palmer 2002 Punyakanok et al  2005a) 
However inmost cases they can only provide a local dependency between predicate and argument for 87% of the argument constituents (Chen and Rambow 2003) which is too low to provide high SRL accuracy 
In Senseval-3 40 frames were selected for an SRL task with the goal of replicating Gildea and Jurafsky (2002) and improving on them (Litkowski 2004)Participants were evaluated on assigning semantic roles to given arguments with best F 1 of 92% and on the task of segmenting and labeling arguments with best F 1 of 83% 
Availability of linguistically annotated corpora such as the Penn Treebank (Bies et al  1995) Proposition Bank (Palmer et al  2005) and FrameNet (Johnson et al  2003) has stimulated much research on methods for automatic syntactic and semantic analysis of text 
Firstly the 26 constituent-based features used by others are y The seven "standard" features predicate (c1) path (c2) phrase type (c3) position (c4) voice (c5) head word (c6) and predicate subcategorization (c7) features proposed by (Gildea and Jurafsky 2002) 
Another approach used similarity measures either between verbs (Gordon and Swanson 2007) or between nouns (Gildea and Jurafsky 2002) to overcome lexical sparsity 
Regarding the use of tree kernels for SRL in (Moschitti 2004) two main drawbacks have been 49 pointed out Highly accurate boundary detection cannot be carried out by a tree kernel model since correct and incorrect arguments may share a large portion of the encoding trees ie they may share many substructures 
We use the standard training set consisting of sections 02-21 of the Wall Street Journal (WSJ) portion of the Penn Treebank labeled with PropBank (Palmer et al 2005) annotations for predicates and arguments 
This as suggested in (Pradhan et al 2005 Moschitti 2004) can help stressing the differ803 ences between different argument types 
We first used Polynomial Kernels over handcrafted linguistically-motivated standard SRL features (Gildea and Jurafsky 2002 Pradhan et al 2005 Xue and Palmer 2004) 
With the advent of supporting resources SRL has become a well-dened task with a substantial body of work and comparative evaluation (see among others Gildea and Jurafsky [2002] Surdeanu et al[2003] Xue and Palmer [2004] Pradhan et al[2005a] the CoNLL Shared Task in 2004 and 2005 and Senseval-3 and SemEval-2007)The identication of event frames may potentially benet many natural language processing (NLP) applications such as information extraction (Surdeanu et al2003) question answering (Narayanan and Harabagiu 2004) summarization (Melli et al2005) and machine translation (Boas 2002)Related work on classifying the semantic relations in noun phrases has also been encouraging for NLP tasks (Moldovan et al2004 Rosario and Hearst 2004) 
In (Gildea and Jurafsky 2002) seven different features2 which aim to capture the relation between the predicate and its arguments were proposed 
Automatic Semantic Role Labeling (SRL) systems made possible by the availability of PropBank (Kingsbury and Palmer 2003 Palmer et al  2005) and encouraged by evaluation efforts in (Carreras and Marquez 2005 Litkowski 2004) have been shown to accurately determine the argument structure of verb predicates 
State-of-the-art statistical parsers trained on the Penn Treebank (PTB) (Marcus et al  1993) proS a8a8 a8a8a8 a72a72 a72a72a72 NP-SBJ a16a16a16 a 80 a 80 a 80 the authority VP a16a16a16 a16a16a16a16 a0 a0a0 a64 a64a64 a80a80a80 a80a80a80a80 VBD dropped PP-TMP a8a8 a72a72IN at NP NN midnight NP-TMP NNP Tuesday PP-DIR a8a8 a72a72TO to NP QP a16a16a16 a80a80a80$ 280 trillion Figure 1 A sample syntactic structure with function labels 
The semantic roles in the examples are labeled in the style of PropBank (Palmer et al 2005) a broad-coverage human-annotated corpus of semantic roles and their syntactic realizations 
The Propbank (Palmer et al 2005) is an annotated corpus of verb subcategorization and alternations which was created by adding a layer of predicate-argument annotation over the phrase structure trees in the Penn Treebank 
Knowledge of semantic argument structure is essential for language understanding and thus important for applications such as information extraction (Moschitti et al 2003 Surdeanu et al 2003) question answering (Shen and Lapata 2007) or recognizing textual entailment (Burchardt et al 2009) 
Noun predicates also appear in FrameNet semantic role labeling (Gildea and Jurafsky 2002) and many FrameNet SRL systems are evaluated in Senseval-3 (Litkowski 2004) 
Furthermore corpora labelled with semantic role information can be used to train shallow semantic parsers (Gildea and Jurafsky 2002) which could in turn benefit applications in need of broad-coverage semantic analysis 
In addition a heuristic based pruning preprocessing (Xue and Palmer 2004) is used to filter out a lot of apparently inappropriate constituents at the beginning 
In standard SRL systems these path features usually consist of a sequence of constituent parse nodes representing the shortest path through the parse tree between a word and the predicate (Gildea and Jurafsky 2002) 
Corpora set-up The above kernels were experimented over two corpora PropBank (wwwcisupennedu/ ace) along with Penn TreeBank5 2 (Marcus et al  1993) and FrameNet 
The effectiveness of the proposed additional pruning techniques may be seen as a significant improvement over the original algorithm of (Xue and Palmer 2004) 
The usual approach (Toutanova et al  2005) uses a traditional boundary classifier (TBC) to select the set of potential argument nodes 
Many of the combinations depart from the manually selected conjunctions of Xue and Palmer (2004) 
The semantic roles in the examples are labeled in the style of PropBank (Palmer et al 2005) a broad coverage human-annotated corpus of semantic roles and their syntactic realizations 
However the performance decrease shows that negation scope finding is not as sensitive to automatic syntactic parsing as common shallow semantic parsing whose performance might decrease by about ~10 in F1-measure (Toutanova et al 2005) 
As regards i) recently there has been an increase in the number of papers dealing with nominalized predicates (Pradhan et al 2004) (Jiang and Ng 2006) (Xue 2006) (Liu and Ng 2007) 
On the WSJ data our results surpass with almost 6% the results obtained by the best SRL system that used partial syntax in the CoNLL 2004 shared task evaluation (Hacioglu et al  2004) 
See (Palmer et al  2005) for a detailed discussion of PropBank semantic roles labels 
In the following decade great success in terms of parse disambiguation and even language modeling was achieved by various lexicalized PCFG models (Magerman 1995 Charniak 1997 Collins 1999 Charniak 2000 Charniak 2001) 
(Chen and Rambow 2003) use LTAG-based decomposition of parse trees (as is typically done for statistical LTAG parsing) for SRL 
Previous research has shown the benefit of jointly learning semantic roles of multiple constituents (Toutanova et al 2008 Koomen et al 2005) 
Note also that the transformations which are taken into account are a superset of the transformations taken into account by Gildea and Palmer (2002) 
The 50-best parser is a probabilistic parser that on its own produces high quality parses the maximum probability parse trees (according to the parsers model) have an f-score of 0897 on section 23 of the Penn Treebank (Charniak 2000) which is still state-of-the-art 
The inter-annotator agreement for PropBank reported in (Palmer et al  2005) is above 09 in terms of the Kappa statistic (Sidney and Castellan Jr  1988) 
Using the CCGbased parser Gildea and Hockenmaier (2003) find a 2% absolute improvement over the Collins parser in identifying core or numbered PropBank arguments 
Train Devel tWSJ tBrown Sentences 39832 1346 2416 426 Tokens 950028 32853 56684 7159 Propositions 90750 3248 5267 804 Arguments 239858 8346 14077 2177 Table 2 Counts on the data set The preprocessing modules used in CONLL2005 include an SVM based POS tagger (Gimenez and M`arquez 2003) Charniak (2000)s full syntactic parser and Chieu and Ng (2003)s Named Entity recognizer 
The improvement achieved by the joint model relative to the local model is about 2 points absolute in F-Measure similar to the improvement when gold-standard syntactic parses are used (Toutanova et al  2005) 
Other syntactic representations such as CCG derivations (Gildea and Hockenmaier 2003) and dependency trees (Hacioglu 2004 Surdeanu et al 2008) have also been explored 
Punyakanok et al  (2004) further showed that constituent-by-constituent (Cby-C) tagging is better than P-by-P 
In our experiments we employ the features listed in Table 1 defined in (Gildea and Jurafsky 2002 Pradhan et al 2005 Xue and Palmer 2004) 
The alternative approach is to combine heuristic and machine-learning approaches (Xue and Palmer 2004) 
Hacioglu has previously described a chunk based semantic labeling method (Hacioglu et al  2004) 
In this category it is also noticeable the use of the syntactic frame feature proposed by Xue and Palmer (2004) 
We present performance results on the February 2004 version of PropBank on gold-standard parse trees as well as results on automatic parses generated by Charniaks parser (Charniak 2000) 
It is interesting to note that the semantic frames are a helpful way of generalizing between predicates words in the same frame have been found frequently to share the same syntactic argument structure (Gildea and Jurafsky 2002) 
Accordingly we do not maximize the probability of the entire labeled parse tree as in (Toutanova et al  2005) 
Detecting and classifying the arguments of predicates has been an active area of research in recent years driven by the availability of large-scale semantically annotated corpora such as the FrameNet (Baker et al  1998) and the Propbank (Palmer et al  2005) 
(Gildea and Palmer 2002) report the results listed on the first line of Table 2 
In spite of the above difficulties there are proposition banks in the newswire domain that are adequate for training SRL systems (Xue and Palmer 2004 Palmer et al  2005) 
