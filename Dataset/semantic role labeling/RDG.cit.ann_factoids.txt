P17	As mentioned by (Pradhan et al., 2004), argument identification plays a bottleneck role in improving the performance of a SRL system. ::: D09-1004_184:221
V01	This approach was soon followed by other researchers (Surdeanu et al., 2003; Pradhan et al., 2004; Xue and Palmer, 2004), focus21 ing on improved sets of features, improved machine learning methods or both, and SRL became a shared task at the CoNLL 2004, 2005 and 2008 conferences1. ::: D09-1003_23:203
M17,M02,P17	Traditionally, either constituent or dependency based, semantic role labeling is done in two steps, argument identification and classification (Gildea and Jurafsky, 2002). ::: W11-0906_17:205
M13,F01,F04,F02	One advantage of log-linear models over SVMs for us is that they produce probability distributions and thus identification 590 Standard Features (Gildea and Jurafsky, 2002) PHRASE TYPE: Syntactic Category of node PREDICATE LEMMA: Stemmed Verb PATH: Path from node to predicate POSITION: Before or after predicate? ::: P05-1073_45:185
F06	For example, indicators of the immediate syntactic types that form the argument, flags raised by punctuation tokens in or nearby the argument, or the governing category feature of Gildea and Jurafsky (2002). ::: W05-0620_252:334
R01	To develop an automatic SRL system for the biomedical domain, it is necessary to train the system with an annotated corpus, called proposition bank (Palmer et al. , 2005). ::: W06-0602_23:182
A01,A02,A05	Others have applied frame-semantic structures to question answering, paraphrase/entailment recognition, and information extraction (Narayanan and Harabagiu, 2004; Shen and Lapata, 2007; Pado and Erk, 2005; Burchardt, 2006; Moschitti et al., 2003; Surdeanu et al., 2003). ::: N10-1138_212:219
P17	(Xue and Palmer, 2004) fount out that different features suited for different sub-tasks of SRL, i.e. argument identification and classification. ::: D09-1004_14:221
M02	The partial trees output by these systems were merged with the parse trees returned by (Charniak, 2000)s parser. ::: W06-2303_139:147
F04,V01	This assumption is a simple adaptation of the pruning algorithm by Xue and Palmer (2004), and it holds for the vast majority of arguments in the CoNLL-2008 data; in the training set, we measured that this covers 99.04% of the arguments of verbs and 97.55% of the argu2Since our algorithm needs to know the positions of the predicates, we trained a separate classifier using the LIBLINEAR toolkit (Fan et al., 2008) to identify the predicate words. ::: D09-1059_98:170
	Conversely, researchers interested in producing richer semantic outputs have concentrated on two-stage systems, where the semantic labelling task is performed on the output of a parser, in a pipeline architecture divided in several stages (Gildea and Jurafsky, 2002; Nielsen and Pradhan, 2004; Xue and Palmer, 2004). ::: W05-1509_22:202
	Moschitti (2004) The work was mainly done when the author was a visiting student at I2R and Che et al. ::: I08-2109_9:168
	We create a baseline system using a subset of features introduced by Gildea and Jurafsky (2002), which are directly applicable to nominal predicates. ::: N04-4036_2:121
	Our model considers two sets of features: Feature Set 1 (FS1): features used in the work reported in (Gildea and Palmer, 2002) and (Gildea and Jurafsky, 2002) ; and Feature Set 2 (FS2): a novel set of features introduced in this paper. ::: P03-1002_77:187
O02	Shallow approaches to semantic processing are making large advances in the direction of efficiently and effectively deriving application relevant explicit semantic information from text (Pradhan et al., 2003; Gildea and Palmer, 2002; Pradhan et al., 2004; Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Chen and Rambow, 2003; Carreras and Marquez, 2005; Moschitti, 2004; Moschitti et al., 2005; Diab et al., 2008). ::: W10-1836_10:111
M17	It is well known that dependency trees extracted from lexicalized phrase structure parsers (Collins, 1999; Charniak, 2000) typically are more accurate than those produced by pure dependency parsers (Yamada and Matsumoto, 2003). ::: P05-1012_174:209
A01	Researchers were able to construct a reasonable IE system by simply mapping specific Arg labels for a set of verbs to template slots, completely avoiding the necessity of building explicit regular expression pattern matchers (Surdeanu et al. 2003). ::: J05-1004_487:501
R02	It should be noted that our results of kernel combinations on FrameNet are in contrast with (Moschitti, 2004), where no improvement was obtained. ::: E06-1015_181:212
M02	Our evaluation criteria which is based on predicting the SRL for constituents in the parse tree is based on the evaluation used in (Toutanova et al. , 2005). ::: D07-1062_120:201
M05,M16	This is very valuable as previous work showed that tree kernels (TK) alone perform lower than models based on manually engineered features for SRL tasks, e.g., (Moschitti, 2004; Giuglea and Moschitti, 2004; Giuglea and Moschitti, 2006; Moschitti, 2006b; Che et al., 2006; Moschitti et al., 2008). ::: D11-1096_228:238
M02,F05	A more realistic way to score the performance is to score tags assigned to head words of constituents, rather than considering the exact boundaries of the constituents as reported by Gildea and Hockenmaier (2003). ::: P05-1072_150:208
O02,M02,P06,P17	Argument filtering Previous work in shallow semantic parsing has demonstrated that not all nodes in a tree are equally probable as semantic roles for a given predicate (Xue and Palmer, 2004). ::: P06-1146_110:209
R01	Ten systems contributed to the task, which was evaluated using the PropBank corpus (Palmer et al. , 2005). ::: W05-0620_10:334
*F02,M02	Unfortunately, ar667 NP PP NP NP NP Peters laughter about the joke Figure 1: Parse tree for example sentence gument recognition at least within predicates relies heavily on syntactic features, with the grammatical function (or alternatively syntactic path) feature as the single most important predictor (Gildea and Jurafsky, 2002). ::: C08-1084_104:229
R01,D01,M02	The task of semantic role labeling in the context of PropBank (Palmer et al., 2005) is to label tree nodes with semantic roles in a syntactic parse tree. ::: D09-1047_45:202
M13	We formulate the labeling task as a classification problem as initiated by Gildea and Jurafsky (2002) and use Support Vector Machine (SVM) classifiers (2005). ::: N07-1070_40:232
	(Pradhan et al. , 2004) train a language model over label sequences. ::: P05-1073_179:185
P17	Gildea and Jurafsky (2002) first tackled SRL as an independent task, which is divided into several sub-tasks such as argument identification, argument classification, global inference, etc. Some researchers (Xue and Palmer, 2004; Koomen et al., 2005; Cohn and Blunsom, 2005; Punyakanok et al., 2008; Toutanova et al., 2005; Toutanova et al., 2008) used a pipelined approach to attack the task. ::: C10-2083_8:87
M16	In addition, Moschitti (2004)s Tree Kernel Tool is used to compute the tree kernel function. ::: P06-2010_146:184
	We also compared our proposed feature set against predicate/argument features (PAF) proposed by (Moschitti, 2004). ::: D07-1062_149:201
M13	To put our results in the context of previous work, other results on core arguments using the same input features have been reported, the best being 91.4% for an SVM with a degree 2 polynomial kernel (Pradhan et al. , 2005a).3 The highest reported result for independent classification of core arguments is 96.0% for a log-linear model using more than 20 additional basic features (Toutanova et al. , 2005). ::: W06-1668_202:239
M02	Note that this pruning algorithm is slightly different from that of (Xue and Palmer, 2004), the predicate itself is also included in the argument candidate list as the nominal predicate sometimes takes itself as its argument. ::: D09-1004_53:221
R01,R09	The data consist of sections of the Wall Street Journal (WSJ) part of the Penn TreeBank (Marcus et al. , 1993), with information on predicate-argument structures extracted from the PropBank corpus (Palmer et al. , 2005). ::: P06-2010_123:184
R01	Again, we concentrate on the PropBank corpus (Palmer et al. , 2005), which is the Wall Street Journal part of the Penn TreeBank corpus enriched with predicate argument structures. ::: W05-0620_22:334
M02	Things become worse still in a parser like the one described in Charniak (2000) because it conditions on (and hence splits the dynamic programming states according to) features of the grandparent node in addition to the parent, thus multiplying the number of possible dynamic programming states even more. ::: P05-1022_56:180
	(Gildea and Jurafsky, 2002) used the empirical probability of the set of proposed arguments as a prior distribution. ::: P05-1073_178:185
A01,A02,A03,P04,R01	Here, the arguments for the predicate gave are defined in the PropBank Frame Scheme (Palmer, Gildea and Kingsbury, 2005) as: V: verb A2: beneficiary A0: giver AM-TMP: temporal A1: thing given Recognizing and labeling semantic arguments is a key task for answering Who, When,What, Where, Why, etc. questions in Information Extraction, Question Answering, Summarization (Melli et al, 2005), and, in general, in all NLP tasks in which some kind of semantic interpretation is needed. ::: W06-1518_18:103
M19	Whole Label Sequence: As observed in previous work (Gildea and Jurafsky, 2002; Pradhan et al. , 2004), including information about the set or sequence of labels assigned to argument nodes should be very helpful for disambiguation. ::: P05-1073_106:185
	The features incorporated in the proposed model are inspired from the work of (Gildea and Jurafsky, 2002; Surdeanu et al. , 2003; Pradhan et al. , 2005; Collins, 1999) and can be classified into five classes: (a) features that capture the internal structure of the candidate argument, (b) features extracted The syntactic label of the candidate constituent. ::: W05-0635_22:78
O02	The motivation behind is that structured syntactic information plays a critical role in negation scope finding and should be paid much more attention, as indicated by previous studies in shallow semantic parsing (Gildea and Palmer, 2002; Punyakanok et al., 2005). ::: C10-1076_31:201
	(Xue and Palmer, 2004) or (Carreras and M`arquez, 2005). ::: W06-2607_15:203
	Instead, we borrowed most of them from the existing literature (Gildea and Jurafsky, 2002; Carreras et al. , 2004; Xue and Palmer, 2004). ::: W05-0628_33:100
E01	While all these systems perform quite well on the WSJ test data, they show significant performance degradation (approximately 10 point drop in F-score) when applied to label test data that is different than the genre that WSJ represents (Pradhan et al. , 2004; Carreras and M`arquez, 2005). ::: N07-1070_18:232
M07	Although state-of-the-art SRL systems use sophisticated statistical models to perform these two tasks jointly (e.g. Toutanova et al., 2005, Johansson and Nugues, 2008), we implemented them as two independent support vector classifiers to be able to analyze the impact of syntactic representation on each task separately. ::: C08-1050_64:214
	The term is most commonly used to describe the automatic identification and labeling of the semantic roles conveyed by sentential constituents (Gildea and Jurafsky, 2002). ::: D11-1122_8:233
	The best phrase-structure parsing models represent generatively the joint probability P(x,y) of sentence x having the structure y (Collins, 1999; Charniak, 2000). ::: P05-1012_5:209
R02,R05	In English predicate argument structure analysis, large corpora such as FrameNet (Fillmore et al., 2001), PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) have been created and utilized. ::: D08-1055_7:129
R01	One such augmented parser, trained on data available from the PropBank project has been recently presented in (Gildea and Palmer, 2002). ::: P03-1002_24:187
	The systems for the other languages follow the successful models devised for English, (Gildea and Jurafsky, 2002; 95 Xue and Palmer, 2004; Pradhan et al. , 2003). ::: W07-2017_109:201
	For example, the SRL system described in (Pradhan et al. , 2005b; Pradhan et al. , 2005a) achieves an Fscore of 81% when tested on the same genre as it is trained on (WSJ); but that score drops to 68.5% when the same system is tested on a different genre (the Brown corpus). ::: N07-1069_37:158
	Moschitti (2004) only selected the relative portion between a predicate and an argument. ::: P06-2010_68:184
F07,M02	Features extracted from LTAG derivations are different and provide distinct information when compared to predicate-argument features (PAF) or subcategorization features (SCF) used in (Moschitti, 2004) or even the later use of argument spanning trees (AST) in the same framework. ::: D07-1062_193:201
	Of special interest here, Moschitti (2004) proposed Predicate Argument Feature (PAF) kernel for SRL under the framework of convolution tree kernel. ::: P07-1026_50:194
	Here, solid lines correspond to surface syntactic structure, produced by Charniaks parser (Charniak, 2000), and dashed lines are an encoding of the Proposition Bank annotation of the semantic roles with respect to the verb stopped. ::: W07-0208_26:151
	We adopt the same algorithm described in (Toutanova et al. , 2005). ::: W06-2909_100:206
F05	Content words, which add informative lexicalized information different from the head word, were detected using the heuristics of (Surdeanu et al. , 2003). ::: H05-1081_92:230
D01	It is generally formulated as a semantic role labeling (SRL) task, where each argument of the predicate is assigned a label that represents the semantic role it plays with regard to its predicate (Gildea and Jurafsky, 2002; Hacioglu et al. , 2003; Pradhan et al. , 2004b; Xue and Palmer, 2004; Toutanova et al. , 2005; Koomen et al. , 2005). ::: N06-1055_9:170
M02	First, argument candidates are generated from the input constituent parse tree using the prevalent heuristic-based pruning algorithm in (Xue and Palmer, 2004). ::: C10-2076_103:199
	Most approaches to the problem of SRL follow the Gildea and Jurafsky (2002) model. ::: P09-1048_41:262
R01	PropBank encodes propositional information by adding a layer of argument structure annotation to the syntactic structures of the Penn Treebank (Marcus et al. , 1993). ::: N06-2026_32:89
P01,P02	Though PropBank defines semantic roles on a verb by verb basis, for a particular verb, Arg0 is generally the argument exhibiting features of a prototypical Agent while Arg1 is a prototypical Patient or Theme (Palmer et al., 2005). ::: P09-2064_35:110
	Propbank (Palmer et al, 2005) is a hand-annotated corpus. ::: P10-3014_15:146
F07,M07,R01	Approaches include incorporating a subcategorization feature (Gildea & Jurafsky, 2002; Xue & Palmer, 2004), such as the one used in our baseline; and building a model which jointly classifie,s all arguments of a verb (Toutanova et al., 2005). ::: P08-1040_201:218
V01	For the test data, we report on results using the gold-standard Tree bank data,and in addition we also report results on automatically parsed data using the Charniak parser (Charniak, 2000) as provided by the CoNLL 2005 shared task. ::: D07-1062_47:201
M02	State-of-the-art models for the subproblem of classification of core arguments additionally use other features of individual nodes (Xue and Palmer, 2004; Pradhan et al. , 2005a), as well as global features including the labels of other nodes in parse tree. ::: W06-1668_152:239
M02	It happens that, in the case of full parses, this node selection strategy is equivalent to the pruning process defined by Xue and Palmer (2004), which selects sibling nodes along the path of ancestors from the verb predicate to the root of the tree2. ::: W05-0628_20:100
	Their corpora are used to train supervised models for semantic role labelling (SRL) of new text (Gildea and Jurafsky, 2002; Carreras and M`arquez, 2005). ::: C08-1084_9:229
M13	The evaluations were carried out with the SVMLight-TK2 software (Moschitti, 2004) which extends the SVM-Light package (Joachims, 1999) with tree kernel functions. ::: W07-2062_37:67
	SRL approach The SRL approach that we adopt is based on the deep syntactic parse (Charniak, 2000) of the sentence that we intend to annotate semantically. ::: W06-2607_37:203
F03,F04,F05	Top-10 combined features for SRI and SRC ranked by z-score Table 1 shows that the commonly used combined features, such as 'predicate+head word' (c1+c6) and 'position+voice' (c4+c5) proposed by (Xue and Palmer, 2004) are also included. ::: C10-2076_120:199
M20	There is a growing body of work on statistical learning for different versions of the semantic parsing problem (e.g., (Gildea and Jurafsky, 2002; Zettlemoyer and Collins, 2005; Ge and Mooney, 2005; Mooney, 2007)), however, most of these methods rely on human annotation, or some weaker forms of supervision (Kate and Mooney, 2007; Liang et al., 2009; Titov and Kozhevnikov, 2010; Clarke et al., 2010) and very little research has considered the unsupervised setting. ::: P11-1145_199:216
	Our experiments working with a flat, chunked representation of the input sentence, described in more detail in Gildea and Palmer (2002), test this finite-state hypothesis. ::: J05-1004_438:501
M04,M19	While CRFs have not been used to date for SRL, their close cousin, the maximum entropy model has been, with strong generalisation performance (Xue and Palmer, 2004; Lim et al. , 2004). ::: W05-0622_13:69
	Arguments to Syntactic Constituents Our approach maps each argument label to one syntactic constituent, using a strategy similar to (Surdeanu et al. , 2003). ::: W05-0635_10:78
R01	The second task we concentrate on is semantic role labeling in the context of PropBank (Palmer et al. , 2005). ::: W06-1668_140:239
F01,F02,F03,F04,F05,F07	Among them, we select the following subset: (a) Phrase Type, Predicate Word, Head Word, 134 Position and Voice as defined in (Gildea and Jurafsky, 2002); (b) Partial Path, No Direction Path, Head Word POS, First and Last Word/POS in Constituent and SubCategorization as proposed in (Pradhan et al. , 2003); and (c) Syntactic Frame as designed in (Xue and Palmer, 2004). ::: W07-2026_47:93
	The training set is extracted from TreeBank (Marcus et al. , 1993) section 1518, the development set, used in tuning parameters of the system, from section 20, and the test set from section 21. ::: C04-1197_181:216
F02	Constituent path as described in (Gildea and Jurafsky, 2002); All 3/4/5-grams of path constituents beginning at the verb predicate or ending at the constituent. ::: W05-0628_47:100
R01	For our experiments, we use Feb 2004 release of PropBank1 (Kingsbury and Palmer, 2002; Palmer et al. , 2005), a corpus in which predicate argument relations are marked for verbs in the Wall Street Journal (WSJ) part of the Penn TreeBank (Marcus et al. , 1994). ::: P05-1072_23:208
P06	In order to speed up the learning process, we use a four-stage learning architecture: Stage 1: To save time, we use a pruning stage (Xue and Palmer, 2004) to filter out the constituents that are clearly not semantic arguments to the predicate. ::: P06-2010_135:184
	Note that the result here is not comparable with the best in this domain (Pradhan et al. , 2004) where the full parse tree is assumed given. ::: C04-1197_204:216
	Neighboring arguments The research of (Jiang et al. , 2005; Toutanova et al. , 2005) has shown the importance of capturing information of the global argument frame in order to correctly classify the local argument. ::: W06-1617_96:156
	Whereas many more global constraints were taken into account in (Punyakanok et al., 2004; Koomen et al., 2005). ::: C10-1153_132:255
	And previous research on English SRL shows that combination is a robust and effective method to alleviate SRLs dependency on parsing results (M`arquez et al., 2005; Koomen et al., 2005; Pradhan et al., 2005; Surdeanu et al., 2007; Toutanova et al., 2008). ::: C10-1153_13:255
	Methods used include bootstrapping approaches (Gildea and Jurafsky, 2002; Kate and Mooney, 2007), where large unannotated corpora were tagged with SRL annotation, later to be used to retrain the SRL model. ::: P09-1004_78:272
F02	About 10% of the fragments contain both the predicate and the argument node, while about 1% encode the Path feature traditionally used in explicit semantic role labeling models (Gildea and Jurafsky, 2002). ::: W09-1106_192:202
M11	For example, the backoff lattice in (Gildea and Palmer, 2002) consists of eight connected nodes for a five-feature set. ::: P03-1002_64:187
M11	The statistical model introduced in Gildea and Jurafsky (2002) uses predicate lexical information at most levels in the probability lattice, hence its scalability to unknown predicates is limited. ::: P03-1002_136:187
	Our standard feature set is a combination of features proposed by (Gildea and Jurafsky, 2002), (Surdeanu et al. , 2003; Pradhan et al. , 2004; Pradhan et al. , 2005b) and (Xue and Palmer, 2004). ::: D07-1062_98:201
	More details of this system can be found in Pradhan et al. , (2005). ::: N07-1070_43:232
M10	Local classifiers Following the standard practice in semantic role labeling, we divided the case prediction task into the tasks of identification and classification (Gildea and Jurafsky, 2002; Pradhan et al. , 2004). ::: P06-1132_86:211
M02	These systems all use (Charniak, 2000)s parse trees both for training and testing, as well as various other information sources including sets of n-best parse trees, chunks, or named entities. ::: N06-2026_72:89
M02	In this paper, we apply the traditional boundary (TBC) and role (TRC) classifiers (Pradhan et al. , 2005a), which are based on binary predicate/argument relations, to label all parse tree nodes corresponding to potential arguments. ::: W06-2909_18:206
	This greedy combination method is very simple and has been adopted in previous research (Pradhan et al., 2005; M`arquez et al., 2005). ::: C10-1153_108:255
	Next, sentences are analyzed by a state-of-the-art syntactic parser (Charniak, 2000) the output of which provides useful information for the main SRL module. ::: H05-2004_21:45
	Hence, the process is divided into framing and annotation (Palmer et al., 2005). ::: W10-1836_63:111
	Semantic Role labeling (SRL) was first defined in Gildea and Jurafsky (2002). ::: D08-1034_12:270
F03	For example, in their inclusion of voice, Gildea and Palmer (2002) note that this deep syntax feature plays an important role in connecting semantic role with surface grammatical function. ::: W03-1006_55:259
	A constituent-based system using Charniaks parser (Charniak, 2000). ::: C08-1050_55:214
M07	Our experiments on joint syntactic and semantic parsing use data that is produced automatically by merging the Penn Treebank (PTB) with PropBank (PRBK) (Marcus et al., 1993; Palmer et al., 2005), as shown in Figure 1. ::: W08-2101_33:190
	We also experimented with various feature combinations, inspired by the features used in (Xue and Palmer, 2004). ::: W06-1617_94:156
R01	PropBank (PB) (Palmer et al., 2005) is a widely used corpus, providing SRL annotation for the entire WSJ Penn Treebank. ::: P10-1024_38:365
M10	We find the exact top N consistent1 most likely local model labelings using a simple dynamic program described in (Toutanova et al. , 2005). ::: W05-0623_41:79
	In recent years, corpora annotated with semantic and function labels have seen the light (Palmer et al. , 2005; Baker et al. , 1998) and semantic role labelling has taken centre-stage as a challenging new task. ::: W05-1509_8:202
	We tested several kernels over standard features (Gildea and Jurafsky, 2002; Pradhan et al., 2005) and structured features (Moschitti et al., 2008): the Polynomial Kernel (PK, with a degree of 3), the Tree Kernel (TK) and its combination with the bag of word kernel on the tree leaves (TKL). ::: N09-2022_42:90
M07	This is from a general belief that each step requires a different set of features (Xue and Palmer, 2004), and training these steps in a pipeline takes less time than training them as a joint-inference task. ::: W11-0906_18:205
F02,V02	In this paper, we decompose the Moschitti (2004)s predicate-argument feature (PAF) kernel into a Path kernel and a Constituent Structure ker1http://www.cs.unt.edu/rada/senseval/senseval3/ 73 nel, and then compose them into a hybrid convolution tree kernel. ::: P06-2010_24:184
	As with many other statistical parsers (Collins, 1999; Charniak, 2000), SSN parsers use a history-based model of parsing. ::: W06-2303_39:147
F04,F06,M02	However, except from a few tentative experiments (Toutanova et al., 2005), grammatical function is not explicitly used by current automatic SRL systems, but instead emulated from constituent trees by features like the constituent position and the governing category. ::: C08-1050_30:214
O02	Shallow Semantic Parsing using Support Vector Machines Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James H. Martin Center for Spoken Language Research, University of Colorado, Boulder, CO 80303 fspradhan,whw,hacioglu,marting@cslr.colorado.edu Dan Jurafsky Department of Linguistics Stanford University Stanford, CA 94305 jurafsky@stanford.edu Abstract In this paper, we propose a machine learning algorithm for shallow semantic parsing, extending the work of Gildea and Jurafsky (2002), Surdeanu et al. ::: N04-1030_1:243
	ENGLISH GERMAN CHINESE (Marcus et al. , 1993) (Skut et al. , 1997) (Xue et al. , 2002) TrainSet Section 2-21 Sentences 1-18,602 Articles 26-270 DevSet Section 22 18,603-19,602 Articles 1-25 TestSet Section 23 19,603-20,602 Articles 271-300 Table 3: Experimental setup. ::: N07-1051_205:232
	Such kernel engineering, as shown in (Moschitti, 2004), allows us to experiment with many syntactic/semantic features seamlessly. ::: P08-1091_117:221
M02	In the meanwhile, the syntactic structure features hidden in a parse tree have been suggested as an important feature for SRL and need to be further explored in SRL (Gildea and Palmer, 2002; Punyakanok et al., 2005). ::: I08-2109_8:168
M02	While some of these models are based on full parse trees (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002), other methods have been proposed that eschew the need for a full 1There are thirteen semantic role labels for modifiers. ::: W06-2303_15:147
R01	In the first step, we adopt the definitions found in PropBank (Palmer et al. , 2005), defining our own framesets for verbs not in PropBank, such as phosphorylate. ::: W06-3308_26:196
	To solve these errors, we need to explore more, such as using n-best parses and the use of several syntactic views (Pradhan et al. , 2005b). ::: W06-2908_199:215
F01,F02,F04,M02	On the arg-verb relation: rp: relative position; di: distance, based on words (w), chunks (c) or the syntactic tree (t); ps: standard path; pv: path variations; pi: scalar indicator variables on the path (of chunks, clauses, or other phrase types), common ancestor, etc. sf: syntactic frame (Xue and Palmer, 2004); On the complete proposition: as: sequence of arguments of a proposition. ::: W05-0620_240:334
M01	The top-performing system in the task (Johansson and Nugues, 2008) applied a very simple reranking scheme by means of a k-best syntactic output, similar to previous attempts (Gildea and Jurafsky, 2002; Toutanova et al., 2005) to improve semantic role labeling performance by using mul561 tiple parses. ::: D09-1059_15:170
R01,R02	Driven by annotation resources such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005), many systems developed in these studies have achieved argument F1 scores near 80% in large-scale evaluations such as the one reported by Carreras and M`arquez (2005). ::: N09-1017_8:181
F01,F02,F04	We use the standard training, development, and 580 Feature Types (Gildea and Jurafsky, 2002) PHRASE TYPE: Syntactic Category of node PREDICATE LEMMA: Stemmed Verb PATH: Path from node to predicate POSITION: Before or after predicate? ::: W06-1668_154:239
	This corpus contains annotations of semantic PASs superimposed on the Penn Treebank (PTB) (Marcus et al. , 1993; Marcus et al. , 1994). ::: W06-0602_24:182
M17	Statistical dependency parsers of English must therefore rely on dependency structures automatically converted from a constituent corpus such as the Penn Treebank (Marcus et al., 1993). ::: C08-1050_37:214
*F07,P12,R01,R05	Research has proceeded for decades on manually created lexicons, grammars, and other semantic resources (Hirst 1987; Pustejovsky 1995; Copestake and Flickinger 2000) in support of deep semantic analysis of language input, but such approaches have been labor-intensive and often restricted to narrow domains.The 1990s saw a growth in the development of statistical machine learning methods across the eld of computational linguistics, enabling systems to learn complex linguistic knowledge rather than requiring manual encoding.These methods were shown to be effective in acquiring knowledge necessary for semantic interpretation, such as the properties of predicates and the relations to their arguments for example, learning subcategorization frames (Briscoe and Carroll 1997) or classifying verbs according to argument structure properties (Merlo and Stevenson 2001; Schulte im Walde 2006).Recently, medium-to-large corpora have been manually annotated with semantic roles in FrameNet (Fillmore, Ruppenhofer, and Baker 2004), PropBank (Palmer, Gildea, and Kingsbury 2005), and NomBank (Meyers et al.2004), enabling the development of statistical approaches specically for SRL. ::: J08-2001_15:86
	Features derived from a syntactic parse of the sentence have proven particularly useful (Gildea & Jurafsky, 2002). ::: P08-1040_16:218
	And it has been widely reported that, in feature-based SRL, the performance can be improved by adding several combined features each of which is composed by two single features (Xue and Palmer, 2004; Toutanova et al., 2005; Zhao et al., 2009). ::: C10-2076_20:199
	These systems share two ideas which make them different from the approach presented here: they all analyse verb-argument relations, and they all use machine learning or probabilistic approaches (Pradhan et al. , 2005) to assign a label to a new instance. ::: W06-3813_44:195
F05	Most recently, the Gildea and Palmer (2002) scores presented here have been improved markedly through the use of support-vector machines as well as additional features for named entity tags, headword POS tags, and verb clusters for back-off (Pradhan et al. 2003) and using maximum-entropy classifiers (He and Gildea 2004, Xue and Palmer 2004). ::: J05-1004_479:501
M13	In our implementation, SVM-Light-TK1 (Moschitti, 2004) is modified. ::: W09-0204_98:160
R02	Gildea and Jurafsky (2002) showed barely significant improvements in semantic role classification of NPs for FrameNet roles using distributional clusters. ::: N10-1058_23:103
R01,R02	The availability of annotated corpora like PropBank and FrameNet (Fillmore et al., 2001) have provided rapid development of research into SRL (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Surdeanu et al., 2003; Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Xue and Palmer, 2004; Pradhan et al., 2004; Pradhan et al., 2005). ::: W09-2601_9:186
M02,R02	Methods for SRL: most features used in prior SRL research are generally extended from Gildea and Jurafsky (2002), who used a linear interpolation method and extracted basic flat features from a parse tree to identify and classify the constituents in the FrameNet (Baker et al. , 1998). ::: P07-1026_32:194
M02	We have chosen to work with a corpus with parse information, the Wall Street Journal WSJ part of the Penn Treebank II corpus (Marcus et al. , 1993), and to extract chunk information from the parse trees in this corpus. ::: W00-0726_15:150
	The significance of syntactic analysis in SRL has been proven by (Gildea and Palmer, 2002; Punyakanok et al., 2005), and syntactic parsing has been applied by almost all current studies. ::: C10-2076_9:199
R02,R04	Most approaches rely on VerbNet (Kipper et al. , 2000) and FrameNet (Baker et al. , 1998) to provide associations between verbs and semantic roles, that are then mapped onto the current instance, as shown by the systems competing in semantic role labelling competitions (Carreras and Marquez, 2004; Carreras and Marquez, 2005) and also (Gildea and Jurafsky, 2002; Pradhan et al. , 2005; Shi and Mihalcea, 2005). ::: W06-3813_43:195
M02	The overall performance of our semantic role labeling approach is not competitive with leading contemporary systems, which typically employ support vector machine learning algorithms with syntactic features (Pradhan et al. , 2005) or syntactic tree kernels (Moschitti et al. , 2006). ::: P07-1025_200:217
M02,R01	In previous work using the PropBank corpus, Gildea and Palmer (2002) developed a system to predict semantic roles from sentences and their parse trees as determined by the statistical parser of Collins (1999). ::: W03-1008_48:141
V01	There has been plenty of work on machine learning models for semantic role labeling, starting with the work of Gildea and Jurafsky (2002), and including CoNLL shared tasks (Carreras and M`arquez, 2005). ::: W06-1668_143:239
M09	In case of the CCG parses, as reported by Gildea and Hockenmaier (2003), the mismatch was about 23%. ::: P05-1072_149:208
M17	Two studies that compared the respective performances of constituent-based and dependency-based SRL systems (Pradhan et al., 2005; Swanson and Gordon, 2006), both using automatic parsers, reported that the constituent-based systems outperformed the dependency-based ones by a very wide margin. ::: C08-1050_22:214
	The bulk of this recent work views semantic analysis as a tagging, or labeling problem, and has applied various supervised machine learning techniques to it (Gildea and Jurafsky (2000, 2002); Gildea and Palmer (2002); Surdeanu et al. ::: N04-4036_6:121
V01,V02,V03	EmpiricalEvaluationsofSRLSystems Many experimental studies have been conducted since the work of Gildea and Jurafsky (2002), including seven international evaluation tasks in ACL-related conferences and workshops: the SIGNLL CoNLL shared tasks in 2004 and 2005 (Carreras and M`arquez 2004, 2005), the SIGLEX Senseval-3 in 2004 (Litkowski 2004), and four tasks in the SIGLEX SemEval in 2007 (Pradhan et al.2007; M `arquez et al.2007; Baker, Ellsworth, and Erk 2007; Litkowski and Hargraves 2007).In the subsequent sections, we summarize their main features, results, and conclusions, although note that the scores are not directly comparable across different exercises, due to differences in scoring and in the experimental methodologies. ::: J08-2001_60:86
R03,R05	Nombank extends the general annotation framework of the English Proposition Bank (Palmer et al. , 2005) and the English Nombank (Meyers et al. , 2004) to the annotation of nominalized predicates in Chinese. ::: N06-1055_23:170
	On the probabilistic modeling front, Gildea and Jurafsky (2002) presented a discriminative model for arguments given the frame; Thompson et al. ::: N10-1138_201:219
	Semantic role labeling based on predicate argument structure was first explored in detail by (Gildea and Jurafsky, 2002). ::: C04-1186_20:110
	It turns out the heuristics that are first proposed in Xue and Palmer (2004) to prune out non-arguments for verbal predicates can be easily adapted to detect arguments for the nominalized predicates as well, so in our experiments we adopt the latter approach. ::: N06-1055_64:170
	From the point of view of learning architectures and study of feature relevance, it is also worth mentioning the following recent works (Punyakanok et al. , 2004; Moschitti, 2004; Xue and Palmer, 2004; Pradhan et al. , 2005a). ::: W05-0620_15:334
E02,R01	We evaluate against PropBank (Palmer et al., 2005), obtaining roughly 70% accuracy when evaluated on the prepositional arguments and more than 80% for the entire argument set. ::: P10-1024_32:365
F01	We used three features that were introduced by Gildea and Hockenmaier (2003): Phrase type This is the category of the maximal projection between the two words the predicate and the dependent word. ::: P05-1072_57:208
D01	Semantic Role Labeling is the process of annotating the predicate-argument structure in text with seThis research was partially supported by the ARDA AQUAINT program via contract OCG4423B and by the NSF via grants IS-9978025 and ITR/HCI 0086132 mantic labels (Gildea and Jurafsky, 2000; Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Surdeanu et al. , 2003; Hacioglu and Ward, 2003; Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Pradhan et al. , 2004; Hacioglu, 2004). ::: P05-1072_8:208
F01,F02,F03,F04	We first experiment with the set of features described in Gildea and Palmer (2002): Pred HW, Arg HW, Phrase Type, Position, Path, Voice. ::: W03-1006_155:259
M17	Conversion to Dependencies 3.2.1 Syntactic Dependencies There exists no large-scale dependency treebank for English, and we thus had to construct a dependency-annotated corpus automatically from the Penn Treebank (Marcus et al., 1993). ::: W08-2121_150:401
V01	The data consists of six sections of the Wall Street Journal part of the Penn Treebank (Marcus et al. , 1993), and follows the setting of past editions of the CoNLL shared task: training set (sections 15-18), development set (section 20) and test set (section 21). ::: W04-2412_58:292
R01,R05	This is a purely syntactic resource, but we can also include this treebank in the category of multi stratal resources since the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) projects have annotated shallow semantic structures on top of it. ::: D09-1059_9:170
A01	In particular, the well-defined semantic role labeling (SRL) task has been drawing more and more attention in recent years due to its importance in deep NLP applications, such as question answering (Narayanan and Harabagiu, 2004), information extraction (Surdeanu et al., 2003), and co-reference resolution (Ponzetto and Strube, 2006). ::: D09-1133_11:236
	Automatic semantic role labeling was first introduced by Gildea and Jurafsky (2002). ::: P06-2010_32:184
M08	Here we formulate an ILP model whose form is different from the model in (Punyakanok et al., 2004; Koomen et al., 2005). ::: C10-1153_111:255
	We used the default linear (Linear) and polynomial (Poly) kernels for the evaluations with the standard features defined in (Gildea and Jurafsky, 2002). ::: E06-1015_135:212
R01	The Proposition Bank (PropBank) corpus (Palmer et al. , 2005) avoids this issue by using theory-agnostic labels (Arg0, Arg1, , Arg5), and by defining those labels to have verb-specific meanings. ::: N07-1069_19:158
R01,R02,V01	It has been a popular task since the availability of the PropBank and FrameNet annotated corpora (Palmer et al., 2005), the seminal work of (Gildea and Jurafsky, 2002) and the successful CoNLL evaluation campaigns (Carreras and M`arquez, 2005). ::: P10-1025_11:248
	In addition Moschitti (2004) only study the task of argument classification while in our experiment, we report the experimental results on both identification and classification. ::: P06-2010_120:184
V01	The results from CoNLL shared tasks in 2005 and 2008 (Carreras and Marquez, 2005; Koomen et al., 2005; Surdeanu et al., 2008; Johansson and Nugues, 2008), further show that SRL pipeline may be one of the standard to achieve a state-of-the-art performance in practice. ::: D09-1004_15:221
M02	Comparing semantic role labelling based on chunked input to the better semantic role labels retrieved based on parsed trees, (Gildea and Palmer, 2002) conclude that parsing is necessary. ::: N06-2026_84:89
	Predicate NP expansion rule This is the noun equivalent of the verb sub-categorization feature used by Gildea and Jurafsky (2002). ::: N04-4036_66:121
V01	Its architecture is based on the top system in the 2005 CoNLL shared task (Koomen et al. , 2005), modified to process raw text using lower level processors but maintaining 6 good real time performance. ::: H05-2004_19:45
	The approach proposed in (Moschitti, 2004) selects the minimal subtree that includes a predicate with its argument. ::: W06-2607_66:203
	Second, we emphasize that the F1 of PK is surprisingly high, since it exploits the set of standard SRL feature (Gildea and Jurafsky, 2002; Pradhan et al., 2005), originally developed for English and left unmodified for Italian. ::: N09-2022_79:90
R01	In the following, we report on the selection of biomedical verbs, and explain the difference between their meaning in PropBank (Palmer et al. , 2005), developed by the University of Pennsylvania, and their meaning in BioProp (a biomedical proposition bank). ::: W06-0602_48:182
F05,M17	It has previously been noted (Pradhan et al., 2005) that a segment-based evaluation may be unfavorable to a dependency-based system, and that an evaluation that scores argument heads maybe more indicative of its true performance. ::: D08-1008_140:220
M02	Their features are usually extended from Gildea and Jurafsky (2002)s work, which uses flat information derived from a parse tree. ::: P06-2010_51:184
	A syntactic parse is however a representation that is very closely tied with the surface-form of natural language, in contrast to Semantic Role Labeling (SRL) which adds a layer of predicate-argument information that generalizes across different syntactic alternations (Palmer et al., 2005). ::: D09-1003_12:203
	The ones denoted with asterisks (*) were not present in (Toutanova et al. , 2005). ::: W05-0623_24:79
F02,M02,M09	A similar drawback can be found in (Gildea and Hockenmaier, 2003) where a parse tree path was defined in terms of Combinatory Categorial Grammar (CCG) types using grammatical relations between predicate and arguments. ::: D07-1062_183:201
F06	Governing category as described in (Gildea and Jurafsky, 2002). ::: W05-0628_40:100
M08	ILP method was first applied to SRL in (Punyakanok et al., 2004). ::: C10-1153_110:255
O02,R01	The above problems are particularly critical for frame-based shallow semantic parsing where, as opposed to more syntactic-oriented semantic labeling schemes (as Propbank (Palmer et al., 2005)), a significant mismatch exists between the semantic descriptors and the underlying syntactic annotation level. ::: P10-1025_32:248
	The standard features at the top of the table were defined by (Gildea and Jurafsky, 2002), and the rest are other useful lexical and structural features identified in more recent work (Pradhan et al. , 2004; Surdeanu et al. , 2003; Xue and Palmer, 2004). ::: P05-1073_49:185
M16	Regarding the learning component of the systems, we find pure probabilistic models (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Gildea and Hockenmaier, 2003), Maximum Entropy (Fleischman et al. , 2003), generative models (Thompson et al. , 2003), Decision Trees (Surdeanu et al. , 2003; Chen and Rambow, 2003), and Support Vector Machines (Hacioglu and Ward, 2003; Pradhan et al. , 2003a; Pradhan et al. , 2003b). ::: W04-2412_11:292
	We used a regularization parameter (option -c) equal to 1 and = 0.4 (see (Moschitti, 2004)). ::: W06-2607_125:203
M02,V01,V02	Many researchers (Gildea and Jurafsky, 2002; Pradhan et al. , 2005a) use feature-based methods Figure 1: Semantic role labeling in a phrase structure syntactic tree representation for argument identification and classification in building SRL systems and participating in evaluations, such as Senseval-3 1, CoNLL-2004 and 2005 shared tasks: SRL (Carreras and M`arquez, 2004; Carreras and M`arquez, 2005), where a flat feature vector is usually used to represent a predicate-argument structure. ::: P06-2010_18:184
	Likelihood (PML) Estimation Gildea & Jurafsky (2002), Gildea & Hockenmaier (2003) and Palmer et al. , (2005) use a statistical approach based on Maximum Likelihood method for SRL, with different backoff comb in a Predicate Arg0 Argm-LOC 181 P(r | hw, pt, pre,pp) P(r | pt, pa, pr, pp) P(r | pt, di, vo, pr, pp) P(r | hw, pr, pp) P(r | pt, pr, pp) P(r | pr, pp) Local Global P(r | hw, pp) P(r | pt, di, vo, pp) tion methods in which selected probabilities are combined with linear interpolation. ::: W06-1622_31:165
D01,R01	We formulated this task after the well-studied task of semantic role labeling in English (e.g. , Gildea and Jurafsky, 2002; Carreras and Mrques, 2005), whose goal is to assign one of 20 semantic role labels to each phrase in a sentence with respect to a given predicate, based on the annotations provided by PropBank (Palmer et al. , 2005). ::: P06-1132_26:211
	As a result, if we do not compare the machine learning methods involved in the two approaches, but rather the features used in learning, our features are a natural generalization of (Chen and Rambow, 2003). ::: W06-1518_94:103
	Such systems extract information from some types of syntactic units (clauses in (Fillmore and Atkins, 1998; Gildea and Jurafsky, 2002; Hull and Gomez, 1996); noun phrases in (Hull and Gomez, 1996; Rosario et al. , 2002)). ::: W06-3813_37:195
	Most of the following work focused on feature engineering (Xue and Palmer, 2004; Jiang et al. , 2005) and machine learning models (Nielsen and Pradhan, 2004; Pradhan et al. , 2005a). ::: P07-1026_34:194
R02	Related work Gildea and Jurafsky (2002) were the first to describe a statistical system trained on the data from the FrameNet project to automatically assign semantic roles. ::: D09-1003_22:203
A01	In particular, the well-defined semantic role labeling (SRL) task has been drawing increasing attention in recent years due to its importance in natural language processing (NLP) lications, such as question answering (Narayanan and Harabagiu, 2004), information extraction (Surdeanu et al., 2003), and co-reference resolution (Kong et al., 2009). ::: P10-1113_11:248
	Next, the probabilities Pr i j F i,p are combined with the probabilities Pfr 1 :::n gjp for a set of roles appearing in a sentence given a predicate, using the following formula: Pr 1 :::n jF 1 :::n, p, Pfr 1 :::n gjp Y i Pr i jF i,p Pr i jp This approach, described in more detail in Gildea and Jurafsky (2002), allows interaction among the role assignments for individual constituents while making certain independence assumptions necessary for efficient probability estimation. ::: J05-1004_387:501
	These include models for part-of-speech tagging (Toutanova et al. , 2003), semantic-role labeling (Punyakanok et al. , 2005; Pradhan et al. , 2005b) and Penn Treebank parsing (Charniak and Johnson, 2005). ::: W06-1668_8:239
	In this paper we describe a domain-independent IE paradigm that is based on predicate-argument structures identified automatically by two different methods: (1) the statistical method reported in (Gildea and Palmer, 2002); and (2) a new method based on inductive learning which obtains 17% higher Fscore over the first method when tested on the same data. ::: P03-1002_25:187
	In the second row (PARA+PML) is trained on all datasets (WSJ 02 to 21) for the BR+RL task (to recognize argument boundaries and label arguments) on the test data WSJ 23, with an improvement of F1:8.28 in comparison to the result of Palmer et al. , (2005) given in the 185 first row. ::: W06-1622_131:165
	There have been many different proposals on how to maintain syntactic locality (Xia, 1999; Chen and VijayShanker, 2000) and SRL locality (Chen and Rambow, 2003; Shen and Joshi, 2005) when extracting LTAG etrees from a Treebank. ::: D07-1062_31:201
P01,R01,R03,R05		In both English and Chinese PropBank (Palmer et al., 2005; Xue and Palmer, 2003), and English and Chinese NomBank (Meyers et al., 2004; Xue, 2006), these semantic arguments include core arguments (e.g., Arg0 for agent and Arg1 for recipient) and adjunct arguments (e.g., ArgM-LOC for locative argument and ArgM-TMP for temporal argument). ::: P10-1113_13:248
M14	Many research efforts utilize machine learning (ML) approaches; such as support vector machines (Moschitti et al. , 2004; Pradhan et al. , 2004), perceptrons (Carreras et al. , 2004), the SNoW learning architecture (Punyakanok et al. , 2004), EMbased clustering (Baldewein et al. , 2004), transformation-based learning (Higgins, 2004), memory-based learning (Kouchnir, 2004), and inductive learning (Surdeanu et al. , 2003). ::: W06-1622_8:165
M07	The system, introduced in (Toutanova et al. , 2005), implements a joint model that captures dependencies among arguments of a predicate using log-linear models in a discriminative re-ranking framework. ::: W05-0623_3:79
	On the other hand, the previous researches (Gildea and Palmer, 2002; Punyakanok et al. , 2005) have also recognized the 74 ::: P06-2010_55:184
	The problem is, given that a node has a core argument label, decide what the correct label is. Other researchers have also looked at this subproblem (Gildea and Jurafsky, 2002; Toutanova et al. , 2005; Pradhan et al. , 2005a; Xue and Palmer, 2004). ::: W06-1668_147:239
	For this task we utilized the August 2005 release of the Charniak parser with the default speed/accuracy settings (Charniak, 2000), which required roughly 360 hours of processor time on a 2.5 GHz PowerPC G5. ::: P07-1025_85:217
	Most of the features we designed are features that have become standard for the SRL task (Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Carreras and M`arquez, 2004; Carreras and M`arquez, 2005). ::: W07-2038_48:108
	Unlike (Poon and Domingos, 2009), we do not use the lambda calculus formalism to define our task but rather treat it as an instance of frame-semantic parsing, or a specific type of semantic role labeling (Gildea and Jurafsky, 2002). ::: P11-1145_42:216
	This result is better than (Xue and Palmer, 2004), and better on gold parses compared to (Toutanova et al. , 2005; Punyakanok et al. , 2005b). ::: D07-1062_131:201
	With the unique exception of the exploration inside sibling PP constituents proposed by (Xue and Palmer, 2004). ::: W05-0628_27:100
	In (Toutanova et al. , 2005), it was observed that there are strong dependencies among the labels of the semantic argument nodes of a verb. ::: W06-2909_198:206
F05	This is part of the explanation of why (Charniak, 2000) finds that early generation of head tags as in (Collins, 1999) is so beneficial. ::: P03-1054_198:233
M13	For the experiments, we use SVM-Light-TK toolkit8 (Moschitti, 2004; Moschitti, 2006) and its SVM-Light default parameters. ::: P08-1091_177:221
	Most of the features we use are described in more detail in (Toutanova et al. , 2005). ::: W05-0623_44:79
F02	Similarly to Xue and Palmer (2004), Argument identification FE Supp Cop Asp Exist Null Argument None Self_mover Path etc classification Figure 3: FE extraction steps. ::: W07-2048_62:92
	With the efforts of many researchers (Carreras and Mrquez 2004, 2005, Moschitti 2004, Pradhan et al 2005, Zhang et al 2007), different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress fast. ::: D08-1034_16:270
	An important baseline study of this process has recently appeared in the literature (Gildea and Jurafsky, 2002). ::: W04-0803_15:92
E02	This can be done with high accuracy when the machine-learning algorithm is powerful and is provided with appropriate features (Hacioglu et al. , 2003; Pradhan et al. , 2004b). ::: N06-1055_61:170
	Chen and Rambow (2003) make use of extracted tree-adjoining grammars. ::: J05-1004_478:501
O02	Similar to the heuristic algorithm as proposed in Xue and Palmer (2004) for argument pruning in common shallow semantic parsing, the argument pruning algorithm adopted here starts from designating the negation signal as the current node and collects its siblings. ::: C10-1076_87:201
F02,M02	Additionally, the dashed lines show those edges which were pruned, following Xue and Palmer (2004) only nodes which are siblings to a node on the path from the verb to the root are included in the tree. ::: W05-0622_24:69
R01	A crucial difference from similar approaches, such as SRL with PropBank roles (Pradhan et al. , 2004) is that by identifying relations as part of a frame, you have identi ed a gestalt of relations that enables far more inference, and sentences from the same passage that use other words from the same frame will be easier to link together. ::: W07-2018_96:162
	(Xue and Palmer, 2004)) and our experiments show that 91% of the SRL targets can be recovered despite this aggressive pruning. ::: W06-1518_32:103
R01	Recently, Palmer et al. , (2005), have PropBanked a significant portion of the Treebanked Brown corpus which enables us to perform experiments to analyze the reasons behind the performance degradation, and suggest potential solutions. ::: N07-1070_24:232
	We use the inference process introduced by (Punyakanok et al. , 2004). ::: W05-0625_54:104
A01,A02	It has obvious applications for template-filling tasks such as information extraction and question answering (Surdeanu et al., 2003; Moschitti et al., 2003). ::: D08-1008_11:220
	To fully evaluate the influence of the LTAG-based features,we report the identification results on both Gold Standard parses and on Charniak parser output (Charniak, 2000)5. ::: D07-1062_128:201
*F01,F02,F03,F04,F05,F06,F07,M02	Gildea and Jurafsky (2002) presented a compact set of features across these three types, which has served as the core of most of the subsequent SRL work: (1) the phrase type, headword, and governing category of the constituent; (2) the lemma, voice, and subcategorization pattern of the verb; and (3) the left/right position of the constituent with respect to the verb, and the category path between them.Extensions to these features have been proposed in various directions.Exploiting the ability of some machine learning algorithms to work with very large feature spaces, some authors have largely extended the representation of the constituent and its context, including among others: rst and last words (and part-of-speech) in the constituent, bag-of-words, n-grams of part of speech, and sequence of top syntactic elements in the constituent.Parent and sibling constituents in the tree may also be codied with all the previous structural and lexical features (Pradhan et al.2005a; Surdeanu et al.2007).Other authors have designed new features with specic linguistic motivations.For instance, Surdeanu et al.(2003) generalized the concept of headword with the content word feature.They also used named entity labels as features.Xue and Palmer (2004) presented the syntactic frame feature, which captures the overall sentence structure using the verb predicate and the constituent as pivots.All these features resulted in a signicant increase in performance. ::: J08-2001_53:86
M02,R01	The PropBank corpus adds a semantic layer to parse trees from the Wall Street Journal section of the Penn Treebank II corpus (Marcus et al., 1993). ::: D09-1047_46:202
	Of special interest here, Moschitti (2004) proposed Predicate Argument Feature (PAF) kernel under the framework of convolution tree kernel for SRL. ::: P06-2010_47:184
	Moschitti (2004) and Che et al. ::: P07-1026_18:194
D01	The task of Semantic Role Labeling (SRL), i.e. the process of detecting basic event structures such as who did what to whom, when and where, has received considerable interest in the past few years (Gildea and Jurafsky, 2002; Surdeanu et al. , 2003; Xue and Palmer, 2004; Pradhan et al. , 2005a; Carreras and M`arquez, 2005). ::: H05-1081_7:230
M02	Gildea and Palmer (2002) developed a system to predict semantic roles (as defined in PropBank) from sentences and their parse trees as determined by the statistical parser of Collins (1999). ::: W03-1008_11:141
E02,M11	To achieve high accuracy and resolve the data sparsity problem the method reported in (Gildea and Palmer, 2002; Gildea and Jurafsky, 2002) employed a backoff solution based on a lattice that combines the model features. ::: P03-1002_62:187
	Their effectiveness has been previously shown by (Pradhan et al. , 2004; Surdeanu et al. , 2003; Xue et al. , 2004). ::: W06-3308_82:196
	Syntactic Frame as designed in (Xue and Palmer, 2004). ::: W07-2062_27:67
	Our work suggests that feature generalization based on verb-similarity may compliment approaches to generalization based on role-similarity (Gildea and Jurafsky, 2002; Baldewein et al. , 2004). ::: P07-1025_207:217
	To avoid this problem, generative models for NLP tasks have often been manually designed to achieve an appropriate representation of the joint distribution, such as in the parsing models of (Collins, 1997; Charniak, 2000). ::: W06-1668_14:239
M17	We investigate ways to combine hypotheses generated from semantic role taggers trained using different syntactic views one trained using the Charniak parser (Charniak, 2000), another on a rule-based dependency parser Minipar (Lin, 1998), and a third based on a flat, shallow syntactic chunk representation (Hacioglu, 2004a). ::: P05-1072_21:208
M02,M13	In our implementation, we use the binary SVMLight (Joachims, 1998) and modify the Tree Kernel Tools (Moschitti, 2004) to a grammar driven one. ::: P07-1026_155:194
	Syntactic frame as described by Xue and Palmer (2004) 2 Experimental Setting and Results We trained the classification models using the complete training set (sections from 02 to 21). ::: W05-0628_49:100
E02,M02	Moreover, by combining polynomial and SST kernels, we can improve the classification accuracy (Moschitti, 2004), i.e. tree kernels provide the learning algorithm with many relevant fragments which hardly can be designed by hand. ::: E06-1015_171:212
M17	As for the former (hereafter it is referred to synPth), we continue to use a dependency version of the pruning algorithm of (Xue and Palmer, 2004). ::: D09-1004_49:221
	The error rate, 10.0%, is lower than that reported by Gildea and Palmer (2002), 17.2%. ::: W03-1006_157:259
R01	Bank I (PropBank) The PropBank annotation (Palmer et al., 2005) classifies the arguments of all the main verbs in the Penn Treebank corpus, other than be. ::: W08-2121_129:401
	This module is retrained in our SRC experiments, using parameters described in (Koomen et al., 2005). ::: P09-2064_78:110
	Our results also confirm the findings in (Palmer et al. , 2005). ::: W06-2303_125:147
	Proposition Bank I (PropBank) The PropBank annotation (Palmer et al., 2005) classifies the arguments of all the main verbs in the Penn Treebank corpus, other than be. ::: W09-1201_252:361
E02	For SRL, high accuracy has been achieved by: (i) proposing new types of features (see Table 1 in Section 3 for previously proposed features), (ii)modeling the predicate frame set by capturing dependencies between arguments (Gildea and Jurafsky, 2002; Pradhan et al. , 2004; Toutanova et al. , 2005; Punyakanok et al. , 2005a), (iii) dealing with incorrect parser output by using more than one parser (Pradhan et al. , 2005b). ::: D07-1062_11:201
	Most state-of-the-art methods for the latter two tasks use a cascaded architecture: they employ syntactic parsers and re-cast the corresponding tasks as pattern matching (Johnson, 2002) or classification (Pradhan et al. , 2005) problems. ::: W07-0208_6:151
E02,M13,M16	Classifier Accuracy (%) SVM 88 Decision Tree (Surdeanu et al. , 2003) 79 Gildea and Palmer (2002) 77 Table 11: Argument classification using same features but different classifiers. ::: N04-1030_205:243
R01	Many researchers have investigated applying machine learning to corpus specifically annotated with this task in mind, PropBank, since 2000 (Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Hacioglu et al. , 2003; Moschitti, 2004; Yi and Palmer, 2004; Pradhan et al. , 2005b; Punyakanok et al. , 2005; Toutanova et al. , 2005). ::: N07-1069_16:158
R01,R02	Indeed, the existence of semantically annotated resources in English such as FrameNet (Baker et al., 1998) and PropBank (Kingsbury and Palmer, 2003; Palmer et al., 2005) corpora have marked a surge in efficient approaches to automatic se 1 In this paper, we use Arabic to refer to Modern Standard Arabic (MSA). ::: W10-1836_11:111
	This paper describes a simple yet novel method for constructing sets of 50-best parses based on a coarse-to-fine generative parser (Charniak, 2000). ::: P05-1022_4:180
*R01	Figure 1 shows an example of a semantic role labeling annotation in PropBank (Palmer et al. , 2005). ::: P06-2010_16:184
F02	Constituent path as described in (Gildea and Jurafsky, 2002) and all 3/4/5-grams of path constituents beginning at the verb predicate or ending at the constituent. ::: H05-1081_87:230
	The best system, presented by the most experienced group on the task (Hacioglu et al. , 2004), achieved a moderate performance of 69.49 at the F1 measure. ::: W04-2412_277:292
	SRL: bootstrapping a corpus with semantic roles Ever since the pioneering article of Gildea and Jurafsky (2002), there has been an increasing interest in automatic semantic role labeling (SRL). ::: W07-1513_41:179
F01,F02,F03,F04,F05,F07,M02	To represent the Fp,a pairs we used the following features: the Phrase Type, Predicate Word, Head Word, Governing Category, Position and Voice defined in (Gildea and Jurasfky, 2002); the Partial Path, Compressed Path, No Direction Path, Constituent Tree Distance, Head Word POS, First and Last Word/POS in Constituent, SubCategorization and Head Word of Prepositional Phrases proposed in (Pradhan et al. , 2005); and the Syntactic Frame designed in (Xue and Palmer, 2004). ::: W05-0630_34:91
	On the other hand, probabilistic inference processes, which have been successfully used for SRL (Koomen et al. , 2005), mandate that each individual candidate argument be associated with its raw activation, or confidence, in the given model. ::: H05-1081_22:230
R02	With the advent of faster and powerful computers, more effective machine learning algorithms, and importantly, large data resources annotated with relevant levels of semantic information FrameNet (Baker et al. , 1998) and Prob Bank corpora (Palmer et al. , 2005), we are seeing a surge in efficient approaches to SRL (Carreras and M`arquez, 2005). ::: W07-2017_105:201
R02	Gildea and Jurafsky (2002) presented an early FrameNet-based SRL system that targeted both verbal and nominal predicates. ::: N09-1017_40:181
R01	In the February 2004 version of the PropBank corpus, annotations are done on top of the Penn TreeBank II parse trees (Marcus et al. , 1993). ::: P05-1073_21:185
M02,M13	The evaluations were carried out with the SVMlight-TK software (Moschitti, 2004) available at http://ai-nlp.info.uniroma2.it/moschitti/ which encodes the tree kernels in the SVM-light software (Joachims, 1999). ::: W05-0630_52:91
M02,O02	(Gildea and Jurafsky, 2002) define this shallow semantic task as a classification problem where the semantic role to be assigned to each constituent is inferred on the basis of probability distributions of syntactic features extracted from parse trees. ::: N06-2026_7:89
	However, (Pradhan et al. , 2005a) uses some additional information since it deals with incorrect parser output by using multiple parsers. ::: D07-1062_145:201
	During testing, the algorithm of enforcing nonoverlapping arguments by (Toutanova et al. , 2005) is used. ::: W06-1617_45:156
R01	In this section we apply the graph transformation method to the task of identification of semantic roles as annotated in the Proposition Bank (Palmer et al. , 2005), PropBank for short. ::: W07-0208_107:151
	Some other work paid much attention to the robust SRL (Pradhan et al. , 2005b) and post inference (Punyakanok et al. , 2004). ::: P07-1026_35:194
	Baseline Features (Pradhan et al. , 2005) b1 replacement b2 NP NP NN b3 NP b4 Bernanke b5 NPSVPVPPPNPNN b6 left b11 Ben, Bernanke, NNP, NNP b12 NULL, VP b13 NULL, NULL, was, VBD b14 S b15 was, VBD b16 NULL b17 NULL, NULL b18 NP-7 b19 NULL b20 NPS b21 NPSVPVPPPNP Baseline Combined Features (Xue and Palmer, 2004) b31 replacement & NP b32 replacement & Bernanke b33 replacement & NPSVPVPPPNPNN b34 replacement & left Table 2: Baseline feature instantiations, assuming the current constituent is NP-Ben Bernanke in Figure 1. ::: W06-1617_78:156
	For the AST-based classifiers we used a equal to 0.4 (see (Moschitti, 2004)). ::: W06-2909_129:206
M02,V01	However, state-ofthe-art semantic role labelling systems (CoNLL, 2005) use parse trees output by state-of-the-art parsers (Collins, 1999; Charniak, 2000), both for training and testing, and return partial trees annotated with semantic role labels. ::: W06-2303_135:147
	These works include (Gildea and Jurafsky, 2002; Carreras and Marquez, 2005; Koomen et al., 2005; Marquez et al., 2005; Dang and Palmer, 2005; Pradhan et al., 2005; Toutanova et al., 2005; Jiang and Ng, 2006; Liu and Ng, 2007; Surdeanu et al., 2007; Johansson and Nugues, 2008; Che et al., 2008). ::: D09-1004_68:221
M02,V01	Finally, as CoNLL 2005 has shown that the most important contribution relates on re-ranking predicate argument structures based on one single tree (Toutanova et al. , 2005) or several trees (Punyakanok et al. , 2005), we would like to use tree kernels for the re-ranking task. ::: W06-2607_202:203
	We trained the parser on the Penn Treebank (Marcus et al. , 1993). ::: W07-2048_21:92
	Some other works paid much attention to the robust SRL (Pradhan et al. , 2005b) and post inference (Punyakanok et al. , 2004). ::: P06-2010_36:184
R01	In the PropBank1 corpus (Palmer et al. , 2005), predicate argument relations are marked for the verbs in the text. ::: N07-1070_25:232
	As with many other statistical parsers (Collins, 1999; Charniak, 2000), SSN parsers use a history-based model of parsing. ::: W05-1509_52:202
R01	The organization provided training, development and test sets derived from the standard sections of the Penn TreeBank (Marcus et al. , 1993) and PropBank (Palmer et al. , 2005) corpora. ::: W05-0620_64:334
	In the original formulation for English in Gildea and Jurafsky (2002), it answers the question: Is the NP governed by IP or VP? ::: N04-1032_76:287
P06	Therefore, our system exploits the heuristic rules introduced by Xue and Palmer (2004) to filter out simple constituents that are unlikely to be arguments. ::: W05-0638_29:90
	For instance, many systems used the pruning strategy described in (Xue and Palmer, 2004) (x&p) and other systems used the soft pruning rules described in (Pradhan et al. , 2005a) (softp). ::: W05-0620_180:334
M02	Recent successes in statistical syntactic parsing based on supervised learning techniques trained on a large corpus of syntactic trees (Collins, 1999; Charniak, 2000; Henderson, 2003) have brought forth the hope that the same approaches could be applied to the more ambitious goal of recovering the propositional content and the frame semantics of a sentence. ::: W06-2303_3:147
	The term is most commonly used to describe the automatic identification and labeling of the semantic roles conveyed by sentential constituents (Gildea and Jurafsky, 2002). ::: P11-1112_8:246
	In a recent paper on the SRL on verbal predicates for English, (Toutanova et al. , 2005) pointed out that one potential flaw in a SRL system where each argument is considered on its own is that it does not take advantage of the fact that the arguments (not the adjuncts) of a predicate are subject to the hard constraint that they do not have the same label3. ::: N06-1055_139:170
	Not only did they achieve new performance benchmarks on parsing the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993), and not only did they serve as the basis of Collins own future work (Collins 2000; Collins and Duffy 2002), but they also served as the basis of important work on parser selection (Henderson and Brill 1999), an investigation of corpus variation and the effectiveness of bilexical dependencies (Gildea 2001), sample selection (Hwa 2001), bootstrapping non-English parsers (Hwa, Resnik, and Weinberg 2002), and the automatic labeling of semantic roles and predicate-argument extraction (Gildea and Jurafsky 2000; Gildea and Palmer 2002), as well as that of other research efforts. ::: J04-4004_8:602
	Following the seminal work of Gildea and Jurafsky (2002), there have been many extensions in machine learning models, feature engineering (Xue and Palmer, 2004), and inference procedures (Toutanova et al., 2005; Surdeanu et al., 2007; Punyakanok et al., 2008). ::: D08-1008_15:220
	Most current semantic role labeling (SRL) approaches can be classified in one of two classes: approaches that take advantage of complete syntactic analysis of text, pioneered by (Gildea and Jurafsky, 2002), and approaches that use partial syntactic analysis, championed by the previous CoNLL shared task evaluations (Carreras and M`arquez, 2004). ::: W05-0635_5:78
M02	These systems use (Charniak, 2000)s parse trees both for training and testing as well as various other information sources including sets of n-best parse trees (Punyakanok et al. , 2005; Haghighi et al. , 2005) or chunks (Marquez et al. , 2005; Pradhan et al. , 2005) and named entities (Surdeanu and Turmo, 2005). ::: W06-2303_140:147
M02,V01	However, state-of-theart semantic role labelling systems (CoNLL, 2005) use parse trees output by state-of-the-art parsers (Collins, 1999; Charniak, 2000), both for training and testing, and return partial trees annotated with semantic role labels. ::: N06-2026_68:89
	SPs in a SRL system For these experiments we modified SwiRL (Surdeanu et al., 2007): (a) we matched the gold boundaries against syntactic constituents predicted internally using the Charniak parser (Charniak, 2000); and (b) we classified these constituents with their semantic role using a modified version of SwiRLs feature set. ::: N10-1058_71:103
F01,F02	The Path features are designed as a sequential collection of phrase tags by (Gildea and Jurafsky, 2002). ::: P09-2064_51:110
M02	Consider the task of recovering non-local dependencies (such as control, WH-extraction, topicalization) in the surface syntactic phrase trees produced by the state-of-the-art parser of (Charniak, 2000). ::: W07-0208_30:151
R02	On the other hand, despite the relevant work of Gildea and Jurafsky (2002), it is still an open issue whether FrameNet classes and frame elements can be obtained and used automatically because of the richness of the semantic structures employed (Dzikovska et al. , 2004). ::: W07-1513_27:179
	A related work is reported in (Gildea and Hockenmaier, 2003). ::: C04-1186_39:110
	In our experiments, we adopt the three-step strategy proposed by (Xue and Palmer, 2004). ::: C10-2076_102:199
R01	In our first set of experiments, the features and probability model of the Gildea and Jurafsky (2002) system were applied to the PropBank corpus. ::: J05-1004_367:501
	With very few exceptions (e.g. Collobert and Weston, 2007), published SRL methods have used some sort of syntactic structure as input (Gildea and Palmer, 2002; Punyakanok et al., 2008). ::: D08-1008_16:220
F06	Governing category as in (Gildea and Jurafsky, 2002). ::: H05-1081_70:230
M02	Third, we point out that the polynomial kernel on flat features is more accurate than tree kernels but the design of such effective features required noticeable knowledge and effort (Gildea and Jurafsky, 2002). ::: E06-1015_169:212
	It has been shown that maximum entropy models achieve state-of-the-art results on SRL (Xue and Palmer, 2004; Toutanova et al., 2008). ::: D09-1047_103:202
M02	Syntactic frame as described by Xue and Palmer (2004) Table 3: Predicate constituent features: Models 1/2 The syntactic label of the candidate constituent. ::: H05-1081_89:230
	As a result some of the features that undo long distance movement via trace information in the TreeBank as used in (Chen and Rambow, 2003) cannot be exploited in our model. ::: W06-1518_79:103
P06	In addition, our system exploits the heuristic introduced by (Xue and Palmer, 2004) to filter out very unlikely constituents. ::: W05-0625_13:104
	The candidate generation stage involves using the heuristic of Xue and Palmer (2004) to generate an over-complete set of argument candidates for each predicate. ::: D11-1012_139:324
M02	We trained and tested on automatic parse trees from Charniaks parser (Charniak, 2000). ::: P05-1073_165:185
E02	Finally, we note that 50-best parsing is only a fac1Charniak in (Charniak, 2000) cites an accuracy of 89.5%. ::: P05-1022_104:180
M02	SRL feature extraction has relied on various syntactic representations of input sentences, such as syntactic chunks (Hacioglu et al. , 2004) and full syntactic parses (Gildea and Jurafsky, 2002). ::: D07-1062_17:201
	By habit, most systems for automatic role-semantic analysis have used Pennstyle constituents (Marcus et al., 1993) produced by Collins (1997) or Charniaks (2000) parsers. ::: C08-1050_16:214
	Note that the meaning of support verb is slightly different between (Toutanova et al., 2005) and (Xue, 2006; Jiang and Ng, 2006) 32 first includes all syntactic children (children), the second also includes all but excludes the left most and the right most children (noFarChildren). ::: D09-1004_88:221
*F01,F02,F03,F04,F05	VOICE: Active or passive relative to predicate HEAD WORD OF PHRASE SUB-CAT: CFG expansion of predicates parent Additional Features (Pradhan et al. , 2004) FIRST/LAST WORD LEFT/RIGHT SISTER PHRASE-TYPE LEFT/RIGHT SISTER HEAD WORD/POS PARENT PHRASE-TYPE PARENT POS/HEAD-WORD ORDINAL TREE DISTANCE: Phrase Type with appended length of PATH feature NODE-LCA PARTIAL PATH Path from constituent to Lowest Common Ancestor with predicate node PP PARENT HEAD WORD If parent is a PP return parents head word PP NP HEAD WORD/POS For a PP, retrieve the head Word / POS of its rightmost NP Selected Pairs (Xue and Palmer, 2004) PREDICATE LEMMA & PATH PREDICATE LEMMA & HEAD WORD PREDICATE LEMMA & PHRASE TYPE VOICE & POSITION PREDICATE LEMMA & PP PARENT HEAD WORD Table 1: Baseline Features and classification models can be chained in a principled way, as in Equation 1. ::: P05-1073_46:185
	This is the feature introduced by (Xue and Palmer, 2004). ::: W05-0625_48:104
R01	A recent release of the PropBank (Palmer et al. , 2005) corpus of semantic role annotations of Treebank parses contained 112,917 labeled instances of 4,250 rolesets corresponding to 3,257 verbs, as illustrated by this example for the verb buy. ::: P07-1025_6:217
	Hence we now prune our set, by keeping only the siblings of all of the verbs ancestors, as is common in supervised SRL (Xue and Palmer, 2004). ::: P09-1004_159:272
A01,A02,A04,P01,P02,P03,P05	Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc. Since the arguments can provide useful semantic information, the SRL is crucial to many natural language processing tasks, such as Question and Answering (Narayanan and Harabagiu 2004), Information Extraction (Surdeanu et al. 2003), and Machine Translation(Boas 2002). ::: D08-1034_15:270
M09,R01	However, this mismatch is significantly less than the 23% mismatch reported in (Gildea and Hockenmaier, 2003) between the CCGBank and an earlier version of the PropBank. ::: C04-1186_102:110
V01	While some of these models are based on full parse trees (Gildea and Jurafsky, 2002; Blaheta, 2004), other methods have been proposed that eschew the need for a full parse (CoNLL, 2004; CoNLL, 2005). ::: W05-1509_81:202
E01	These spans were proposed using a high-recall heuristic (Xue and Palmer, 2004). ::: N06-1054_128:249
F01,F03	Predicate the given predicate lemma Voice whether the predicate is realized as an active or passive construction (Pradhan et al. , 2004, claim approximately 11% of the sentences in PropBank use a passive instantiation) Phrase Type the syntactic category (NP, PP, S, etc). ::: W06-1622_17:165
M02	An alternative tree kernel representation, proposed in (Moschitti, 2004), is the selection of the minimal tree subset that includes a predicate with only one of its arguments. ::: E06-1015_107:212
	We also experimented with various feature combinations, inspired by the features used in (Xue and Palmer, 2004). ::: W06-1617_61:156
	Our results also provide some new insights into the discussion about the necessity of parsing for function or semantic role labelling (Gildea and Palmer, 2002; Punyakanok et al. , 84 2005), showing that parsing is beneficial. ::: W05-1509_45:202
R01,R02	The two main resources are PropBank (Palmer et al., 2005) and FrameNet (Ruppenhofer et al., 2006). ::: C10-2107_47:208
R01	We used two different corpora: PropBank (www.cis.upenn.edu/ace) along with PennTree bank 2 (Marcus et al. , 1993) and FrameNet. ::: E06-1015_122:212
F01,F02	The features are listed as follows: Path The path features are similar to the path feature which is designed by (Gildea and Jurafsky, 2002).A path is a sequential collection of phrase tags. ::: C08-1105_152:211
	Table 3 compares the performance comparison among our Hybrid convolution tree kernel, Moschitti (2004)s PAF kernel, standard flat features with Linear kernels, and Poly kernel (d = 2). ::: P06-2010_157:184
	The same as Moschitti (2004), we also set the = 0.4 in the computation of convolution tree kernels. ::: P06-2010_148:184
E02	If the full feature set is selected the probability is calculated by P (r | pr, vo, pt, di, hw, pa, pp) = # (r, pr, vo, pt, di, hw, pa, pp) / # (pr, vo, pt, di, hw, pa, pp) Gildea & Jurafsky (2002) claims there is a trade-off between more-specific distributions, which have higher accuracy but lower coverage, and less-specific distributions, which have lower accuracy but higher coverage and that the selection of feature subsets is exponential; and that selection of combinations of different feature subsets is doubly exponential, which is NPcomplete. ::: W06-1622_33:165
F05	Content words, which add informative lexicalized information different from the head word, were detected using the heuristics of (Surdeanu et al. , 2003). ::: W05-0635_25:78
	Our results confirm the findings in (Palmer et al. , 2005). ::: W05-1509_184:202
	NSTs with Tree Kernels To implement the re-ranking model, we follow the approach described in (Toutanova et al. , 2005). ::: W06-2909_95:206
R01	The data consists of sections of the Wall Street Journal part of the Penn TreeBank (Marcus et al. , 1993), with information on predicate-argument structures extracted from the PropBank corpus (Palmer et al. , 2005). ::: P07-1026_147:194
P17	It is shown that applying the Tree-Based Predicate Argument Recognition Algorithm (PARA) to the data as a preprocessing stage allows kNN and PML to deliver F1: 68.61 and 71.02 respectively on the WSJ23, and F1: 56.96 and 60.55 on the Brown Corpus; an increase of 8.28 in F1 measurement over the most recent published PML results for this problem (Palmer et al. , 2005). ::: W06-1622_5:165
	In (Pradhan et al. , 2005b), some experiments were conducted on SRL systems trained using different syntactic views. ::: W06-2909_200:206
	The probabilities P(r i jF i ;p) are combined with the probabilities P(fr 1::n gjp) for a set of roles appearing in a sentence given a predicate, using the following formula: P(r 1::n jF 1::n ;p) P(fr 1::n gjp) Y i P(r i jF i ;p) P(r i jp) This approach, described in more detail in Gildea and Jurafsky (2002), allows interaction between the role assignments for individual constituents while making certain independence assumptions necessary for efficient probability estimation. ::: W03-1008_68:141
R01	To some extent, function labels overlap with semantic role labels as defined in PropBank (Palmer et al. , 2005). ::: W05-1509_16:202
	In recent years tree kernels have been shown to be interesting approaches for the modeling of syntactic information in natural language tasks, e.g. syntactic parsing (Collins and Duffy, 2002), relation extraction (Zelenko et al. , 2003), Named Entity recognition (Cumby and Roth, 2003; Culotta and Sorensen, 2004) and Semantic Parsing (Moschitti, 2004). ::: E06-1015_5:212
M09,M17	In recent years, many advances have been made on SRL using singular syntactic view, such as constituent (Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Surdeanu et al., 2007), dependency (Hacioglu, 2004; Johansson and Nugues, 2008; Zhao et al., 2009), and CCG (Chen and Rambow, 2003; Boxwell et al, 2009). ::: C10-2076_23:199
	Initially, 7 features were proposed by (Gildea and Jurafsky, 2002), and all following research has used these features and some additional ones. ::: W06-1668_149:239
V01	Several systems have incorporated such dependencies, for example, (Gildea and Jurafsky, 2002; Pradhan et al. , 2004; Thompson et al. , 2003) and several systems submitted in the CoNLL-2004 shared task (Carreras and M`arquez, 2004). ::: P05-1073_11:185
R01,R05	Features and feature selection 4.1 Baseline NomBank SRL features Table 1 lists the baseline features we adapted from previous PropBank-based SRL systems (Pradhan et al. , 2005; Xue and Palmer, 2004). ::: W06-1617_58:156
	The rationale for using a simple DL learner is given in (Gildea and Jurafsky, 2002) where essentially it based on their experience with the setting of backoff weights for smoothing, it is stated that the most specific single feature matching the training data is enough to predict the SRL on test data. ::: W06-1518_53:103
	Because predicate lexical information is used for less than 5% of the branching decisions, the generated classifier scales better than the statistical method from (Gildea and Palmer, 2002) to unknown predicates. ::: P03-1002_185:187
	Though several pruning algorithms have been raised (Xue and Palmer, 2004), the policies are all in global style. ::: C08-1105_17:211
F05	Due to the sparsity of the head word feature, we also use the part-of-speech of the head word, following Surdeanu et al (2003). ::: N04-1032_123:287
	Looking at the description of the different systems, it becomes clear that the general type of features used in this edition is strongly based on previous work on the SRL task (Gildea and Jurafsky, 2002; Surdeanu et al. , 2003; Pradhan et al. , 2005a; Xue and Palmer, 2004). ::: W05-0620_231:334
M02	Regarding the design of features for predicate argument pairs, we can use the attribute-values defined in (Gildea and Jurasfky, 2002) or tree structures (Moschitti, 2004). ::: W06-2909_39:206
M09	We also compare with the CCG-based SRL presented in (Gildea and Hockenmaier, 2003)7, which has a similar motivation as this paper, except they use the Combinatory Categorial Grammar formalism and the CCGBank syntactic Treebank which was converted from the Penn Treebank. ::: W09-2601_144:186
F02,M09,M17	In order to generalize the path feature (see Table 1 in Section 3) which is probably the most salient (while being the most data sparse) feature for SRL, previous work has extracted features from other syntactic representations, such as CCG derivations (Gildea and Hockenmaier, 2003) and dependency trees (Hacioglu, 2004) or integrated features from different parsers (Pradhan et al. , 2005b). ::: D07-1062_19:201
	As previously observed (Pradhan et al. , 2004), including modifying arguments in sequence features is not helpful. ::: P05-1073_112:185
R02	Gildea and Jurafsky (2002) describe a statistical system trained on the data from the FrameNet project to automatically assign semantic roles. ::: J05-1004_360:501
	As a guideline for interpreting these results, with 8,167 observations, the threshold for statistical significance with p < .05 is a 1.0% absolute difference in performance (Gildea and Jurafsky 2002). ::: J05-1004_399:501
	Previous approaches to the SRL task have made use of a full syntactic parse of the sentence in order to define argument boundaries and to determine the role labels (Gildea and Palmer, 2002; Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Pradhan et al. , 2003; Pradhan et al. , 2004; Surdeanu et al. , 2003). ::: C04-1197_9:216
M02	On the one hand, SSTs provide learning algorithms with richer information which may be critical to capture syntactic properties of parse trees as shown, for example, in (Zelenko et al. , 2003; Moschitti, 2004). ::: E06-1015_13:212
	For example, we found that the SCF tree kernel (Moschitti, 2004) improves the AX multi classifier AX AM CX RX # train. ::: W05-0630_86:91
R01	The PropBank superimposes an annotation of semantic predicate-argument structures on top of the Penn Treebank (PTB) (Marcus et al. , 1993; Marcus et al. , 1994). ::: W03-1006_7:259
F02,M02	Instead of extracting a typical standard path feature from the derived tree, (Chen and Rambow, 2003) uses the path within the elementary tree from the predicate to the constituent argument. ::: D07-1062_179:201
P06	In addition, our system exploits a heuristic modified from that introduced by (Xue and Palmer, 2004) to filter out very unlikely constituents. ::: H05-2004_27:45
M10	Previous research (Gildea and Jurafsky, 2002; Pradhan et al. , 2004; Carreras and M`arquez, 2004) has identified many useful features for local identification and classification. ::: W05-0623_22:79
R01,R02	Two frameworks for semantic roles have found wide use in the community, PropBank (Palmer et al., 2005) and FrameNet (Fillmore et al., 2003). ::: C08-1084_8:229
	While almost all systems use the standard path of (Gildea and Jurafsky, 2002), many have explored variations of it. ::: W05-0620_261:334
	By then, "Earthquake" was trending on Twitter Search with thousands of updates 2 . However, it is a daunting task for people to find out information they are interested in from such a huge number of news tweets, thus motivating us to conduct some kind of information 2 http://blog.twitter.com/2008/07/twitter-as-news-wire.html 698 extraction such as event mining, where SRL plays a crucial role (Surdeanu et al., 2003). ::: C10-1079_16:167
R01	PropBank contains about 53,700 sentences and a xed split between training and testing which has been used in other researches e.g., (Gildea and Palmer, 2002; Surdeanu et al. , 2003; Hacioglu et al. , 2003). ::: P04-1043_145:222
	(Palmer et al. , 2005) is an annotation of one million words of the Wall Street Journal portion of the Penn Treebank II (Marcus et al. , 1994) with predicate-argument structures for verbs, using semantic role labels for each verb argument. ::: N07-1069_41:158
V01	For a fair comparison, our system was among the best at CoNLL-04, where the best system (Hacioglu et al. , 2004) achieve a 69.49 F1 score. ::: C04-1197_205:216
R02	The first work to tackle SRL as an independent task is (Gildea and Jurafsky, 2002), which presented a supervised model trained and evaluated on FrameNet. ::: P09-1004_53:272
O02,R01	Annotations similar to these have been used to create automated semantic role labeling systems (Pradhan et al. , 2005; Moschitti et al. , 2006) for use in natural language processing applications that require only shallow semantic parsing. ::: P07-1025_8:217
M02,R01	The Release 3 of the Penn Treebank contains the hand parsed syntactic trees of a subset of the Brown Corpus sections F, G, K, L, M, N, P and R. Palmer et al. , (2005) have recently PropBanked a significant portion of this Treebanked Brown corpus. ::: N07-1070_36:232
O02	In contrast with features from shallow parsing, previous work (Gildea and Palmer, 2002; Punyakanok et al. , 2005b) has shown the necessity of full syntactic parsing for SRL. ::: D07-1062_18:201
F02	While the Path feature has been identified to be very important for the argument identification task, it is one of the most sparse features and may be difficult to train or generalize (Pradhan et al. , 2004; Xue and Palmer, 2004). ::: P05-1072_53:208
*P01,P10,R01	Given the variability in the sets of roles used across the computational resources, an important issue is the extent to which different role sets affect the SRL task, as well as subsequent use of the output in other NLP applications.Gildea and Jurafsky (2002) initiated this type of investigation by exploring whether their results were dependent on the set of semantic roles they used.To this end, they mapped the FrameNet frame elements into a set of abstract thematic roles (i.e., more general roles such as Agent, Theme, Location), and concluded that their system could use these thematic roles 149 Computational Linguistics Volume 34, Number 2 without degradation.Similar questions must be investigated in the context of PropBank, where the framesets for the verbs may have signicant domain-specic meanings and arguments due to the dependence of the project on WSJ data.Given the uncertainty in the linguistic status of semantic role lists, and the lack of evidence about which types of roles would be most useful in various NLP tasks, an important ongoing focus of attention is the value of mapping between the role sets of the different resources (Swier and Stevenson 2005; Loper, Yi, and Palmer 2007; Yi, Loper, and Palmer 2007). ::: J08-2001_38:86
	Accordingly, our basic system is similar to the one proposed in (Pradhan et al. , 2005a) and it is hereby described. ::: W06-2909_29:206
R02	In recent years, the availability of large human-labeled corpora such as PropBank (Palmer et al. , 2005) and FrameNet (Baker et al. , 1998) has made possible a statistical approach of identifying and classifying the arguments of verbs in natural language texts. ::: P07-1027_7:194
	Generally speaking, these SRL approaches use a two-stage architecture: i) argument identification; ii) argument classification, to solve the task as a derivation of Gildea and Jurafskys pioneer work (Gildea and Jurafsky, 2002). ::: C08-1105_86:211
	Kernels on complete predicate argument structures The type of a target argument strongly depends on the type and number of the predicates arguments1 (Punyakanok et al. , 2005; Toutanova et al. , 2005). ::: W06-2607_88:203
	In (Punyakanok et al., 2004), several more constraints are considered. ::: D10-1030_101:273
	As this task is recognized as an important step after (or the last step of) syntactic analysis, many studies have been conducted to achieve accurate semantic role labeling (Gildea and Jurafsky, 2002; Moschitti, 2004; Hacioglu et al. , 2004; Punyakanok et al. , 2004; Pradhan et al. , 2005a; Pradhan et al. , 2005b; Toutanova et al. , 2005). ::: W06-2908_9:215
*F01,F02,F04,F05,R05,R07	For NomBank SRL fea140 Baseline Features (Pradhan et al. , 2005) b1 predicate: stemmed noun b2 subcat: grammar rule that expands the predicates parent b3 phrase type: syntactic category of the constituent b4 head word: syntactic head of the constituent b5 path: syntactic path from the constituent to the predicate b6 position: to the left or right of the predicate b11 first or last word/POS spanned by the constituent (b11FW, b11LW, b11FP, b11LP) b12 phrase type of the left or right sister (b12L, b12R) b13 left or right sisters head word/POS (b13LH, b13LP, b13RH, b13RP) b14 phrase type of parent b15 parents head word or its POS (b15H, b15P) b16 head word of the constituent if its parent has phrase type PP b17 head word or POS tag of the rightmost NP node, if the constituent is PP (b17H, b17P) b18 phrase type appended with the length of path b19 temporal keyword, e.g., Monday b20 partial path from the constituent to the lowest common ancestor with the predicate b21 projected path from the constituent to the highest NP dominating the predicate Baseline Combined Features (Xue and Palmer, 2004) b31 b1 & b3 b32 b1 & b4 b33 b1 & b5 b34 b1 & b6 Table 1: Baseline features for NomBank SRL tures, we use this set of more specific mappings to replace the morphological mappings based on WordNet. ::: W06-1617_69:156
A01,A02,A04	Automatic, accurate and wide-coverage techniques that can annotate naturally occurring text with semantic argument structure play a key role in NLP applications such as Information Extraction (Surdeanu et al. , 2003; Harabagiu et al. , 2005), Question Answering (Narayanan and Harabagiu, 2004) and Machine Translation (Boas, 2002; Chen and Fung, 2004). ::: N07-1070_10:232
M02	It includes several Wall Street Journal sections with parse-trees from both Charniaks (2000) parser and Collins (1999) parser. ::: W06-1622_60:165
E02,R01	The release of semantically annotated corpora such as FrameNet (Baker et al. , 1998) and PropBank (Palmer et al. , 2003) has made it possible to develop high-accuracy statistical models for automated semantic role labeling (Gildea and Jurafsky, 2002; Pradhan et al. , 2004; Xue and Palmer, 2004). ::: P05-1073_6:185
	Several machine learning approaches for argument identi cation and classi cation have been developed (Gildea and Jurasfky, 2002; Gildea and Palmer, 2002; Surdeanu et al. , 2003; Hacioglu et al. , 2003). ::: P04-1043_21:222
*M02,M16	Features Arg P Arg R Arg a3a5a4 Role A FS1 84.96 84.26 84.61 78.76 FS1 + hPos 92.24 84.50 88.20 79.04 FS1 + cw, cPos 92.19 84.67 88.27 80.80 FS1 + cNE 83.93 85.69 84.80 79.85 FS1 + NE flags 87.78 85.71 86.73 81.28 FS1 + pvcSum + 84.88 82.77 83.81 78.62 pvcMax FS1 + FS2 91.62 85.06 88.22 83.05 Table 1: Inductive learning results for argument identification and role assignment Model Implementation Arg a3a5a4 Role A Statistical (Gildea and Palmer) 82.8 This study 71.86 78.87 Decision Trees FS1 84.61 78.76 FS1 + FS2 88.98 83.74 Table 2: Comparison of statistical and decision tree learning models of the decision-tree-based method against the results obtained by the statistical approach reported in (Gildea and Palmer, 2002). ::: P03-1002_126:187
M13	The best performance was obtained by the SVMbased IOB tagger of (Hacioglu et al. , 2004), which almost reached the performance of 70 in F1 on the test. ::: W04-2412_266:292
M02	Its potential arguments A are extracted according to (Xue and Palmer, 2004) (3) For each pair < p,a > P A: if a covers exactly the words of semantic role of p, put minimal subtree < p,a > into positive example set (T+r ); else put it in the negative examples (Tr ) In our experiments, we set = 0.5. ::: W09-0204_101:160
	While previous programs with similar goals (Gildea and Jurafsky, 2002) were statistics-based, this tool will be based completely on hand-coded rules and lexical resources. ::: W04-2705_184:201
M02	For generating constituency trees, we used the Charniak parser (Charniak, 2000) whereas we applied LTH syntactic parser (described in (Johansson and Nugues, 2008a)) to generate dependency trees. ::: D11-1096_145:238
	As for SRL on news, most researchers used the pipelined approach, i.e., dividing the task into several phases such as argument identification, argument classification, global inference, etc., and conquering them individually (Xue and Palmer, 2004; Koomen et al., 2005; Cohn and Blunsom, 2005; Punyakanok et al., 2008; Toutanova et al., 2005; Toutanova et al., 2008). ::: C10-1079_47:167
M02	Pradhan et al., 2005), analyzing the complex input syntax trees (Moschitti, 2004; Liu and Sarkar, 2007), exploiting the complicated output the predicate-structure (Toutanova et al., 2005), as well as capturing paradigmatic relations between predicates (Gordon and Swanson, 2007). ::: C08-1105_15:211
E01	The rst step in SRL typically consists of ltering (or pruning) the set of argument candidates for a given predicate.Because arguments may be a continuous or discontinuous sequence of words, any subsequence of words in the sentence is an argument candidate.Exhaustive exploration of this space of candidates is not feasible, because it is both very large and imbalanced (i.e., the vast majority of candidates are not actual arguments of the verb).The simple heuristic rules of Xue and Palmer (2004) are commonly used to perform ltering because they greatly reduce the set of candidate arguments, while maintaining a very high recall. ::: J08-2001_43:86
	Recent releases of the Charniak parser (Charniak, 2000) have included an option to provide the top k parses of a given sentence according to the probability model of the parser. ::: W05-0623_67:79
P12	Rich annotations of corpora has allowed for the development of techniques for recovering deep linguistic structures: syntactic non-local dependencies (Johnson, 2002; Hockenmaier, 2003; Dienes, 2004; Jijkoun and de Rijke, 2004) and semantic arguments (Gildea, 2001; Pradhan et al. , 2005; Toutanovaetal. ::: W07-0208_4:151
R09, M02	We train our models with verb instances extracted from three parsed corpora: (1) the Wall Street Journal section of the Penn Treebank (PTB), which was parsed by human annotators (Marcus et al. , 1993), (2) the Brown Laboratory for Linguistic Information Processing corpus of Wall Street Journal text (BLLIP), which was parsed automatically by the Charniak parser (Charniak, 2000), and (3) the Gigaword corpus of raw newswire text (GW), which we parsed ourselves with the Stanford parser. ::: W06-1601_132:193
	Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling. ::: D08-1034_27:270
R09	In the statistical NLP community, the most widely used grammatical resource is the Penn Treebank (Marcus et al., 1993). ::: D09-1059_8:170
	Our method is similar to the Moschitti (2004)s predicate-argument feature (PAF) kernel. ::: P06-2010_118:184
R01	Consider, for example, a sentence such as The authority dropped at midnight Tuesday to $ 2.80 trillion (taken from section 00 of PropBank (Palmer et al. , 2005)). ::: N06-2026_9:89
R02	The growing interest in learning deeper information is to a large extent supported and due to the recent development of semantically annotated databases such as FrameNet (Baker et al. , 1998) or the Proposition Bank (Palmer et al. , 2005), that can be used as training resources for a number of supervised learning paradigms. ::: W06-2303_7:147
	Details of them can be found in (Koomen et al. , 2005). ::: H05-2004_24:45
F02,M02	Parse tree paths were used for semantic role labeling by Gildea and Jurafsky (2002) as descriptive features of the syntactic relationship between predicates and their arguments in the parse tree of a sentence. ::: P07-1025_19:217
M02	A careful analysis of such features reveals that most of them are syntactic tree fragments of training sentences, thus a natural way to represent them is the adoption of tree kernels as described in (Moschitti, 2004). ::: W06-2607_16:203
M09	Gildea and Hockenmaier (2003) report that using features extracted from a Combinatory Categorial Grammar (CCG) representation improves semantic labeling performance on core arguments. ::: P05-1072_55:208
E02	Regarding the classification properties, we studied the argument labeling accuracy of ST and SST kernels and their combinations with the standard features (Gildea and Jurafsky, 2002). ::: E06-1015_22:212
P12	In this paper, we find that the use of deep linguistic representations to predict these semantic labels are more effective than the generally more surface-syntax representations previously employed (Gildea and Palmer (2002)). ::: W03-1006_9:259
	We have experimentally evaluated 30 features based on the previous work in semantic role labelling (Gildea and Jurafsky, 2002; Pradhan et al. , 2004; Xue and Palmer, 2004): Lexical features (5): predicate (verb), first phrase word, last phrase word and words immediately before and after the phrase. ::: W05-0637_25:82
R09	SRL systems (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002) have extensively used features defined over Penn Treebank phrase structure trees. ::: W09-2601_11:186
M02	To avoid explicit feature engineering on trees, (Moschitti, 2004) used convolution kernels on selective portions of syntactic trees. ::: D07-1062_20:201
	These features are common to many SRL systems and are described in Xue and Palmer (2004). ::: W05-0622_51:69
	Section 2 reports on the parser that produces predicate-argument labels and compares it against the parser introduced in (Gildea and Palmer, 2002). ::: P03-1002_29:187
M02	Kernels for SRL Moschitti (2004) proposed to apply convolution tree kernels (Collins and Duffy, 2001) to SRL. ::: P06-2010_63:184
M13	The word segmenter achieves the performance of 96.1 in F1-measure while the Berkeley parser gives a performance of 82.5 and 85.5 in F1measure on golden and automatic word segmentation, respectively 2 . 1 In addition, SVMLight with the tree kernel function (Moschitti, 2004) 3 is selected as our classifier. ::: D09-1133_172:236
	However, there is not much research on combined use of different syntactic views (Pradhan et al., 2005), on the feature level of SRL. ::: C10-2076_14:199
M01	Moreover, we modeled SRL as a re-ranking task in line with (Toutanova et al. , 2005). ::: W06-2909_23:206
	In addition, while the system described here is based on pipelined classification, recent research on semantic role labeling has shown that significant performance improvements can be gained by exploiting interdependencies between arguments (Toutanova et al. , 2005). ::: W07-2048_88:92
M02	Finally, we would like to experiment with more sophisticated kernels, the tree kernels described in (Moschitti, 2004), i.e. models that have shown a lot of promise for the English SRL process. ::: W07-2026_91:93
	They are a combination of features introduced by Gildea and Jurafsky (2002), ones proposed in Pradhan et al. ::: W05-0634_58:111
M17	To address these errors, we added two additional parse representations: i) Minipar dependency parser, and ii) chunking parser (Hacioglu et al. , 2004). ::: P05-1072_121:208
	As proposition banks are semantically annotated versions of a Penn-style treebank, they provide consistent semantic role labels across different syntactic realizations of the same verb (Palmer et al. , 2005). ::: W06-3308_30:196
R01,R02	Inspired by this idea different resources were constructed, including FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005). ::: D09-1003_36:203
	Most of the following works focused on feature engineering (Xue and Palmer, 2004; Jiang et al. , 2005) and machine learning models (Nielsen and Pradhan, 2004; Pradhan et al. , 2005a). ::: P06-2010_35:184
	The concept of support verb was broadly used (Toutanova et al., 2005; Xue, 2006; Jiang and Ng, 2006)4, we here extend it to nouns and prepositions. ::: D09-1004_76:221
M13	SVM-light with linear kernel is used to train on a standard feature set (Xue and Palmer, 2004). ::: W09-2601_137:186
R01,R02	In previous work using the PropBank corpus, (Gildea and Palmer, 2002) proposed a model predicting argument roles using the same statistical method as the one employed by (Gildea and Jurafsky, 2002) for predicting semantic roles based on the FrameNet corpus (Baker et al. , 1998). ::: P03-1002_47:187
M08	Note that the form of the ILP model in this paper is different from that in (Punyakanok et al., 2004; Koomen et al., 2005) in three aspects: (1) A special label class null, which means no label is assigned, was added to the label set in (Punyakanok et al., 2004; Koomen et al., 2005). ::: C10-1153_127:255
A01,A02,A03,A04	Indeed, the analysis produced by existing semantic role labelers has been shown to benefit a wide spectrum of applications ranging from information extraction (Surdeanu et al., 2003) and question answering (Shen and Lapata, 2007), to machine translation (Wu and Fung, 2009) and summarization (Melli et al., 2005). ::: P11-1112_19:246
	These results are comparable to the results from Gildea and Palmer (2002), but only roughly because of differences in corpora. ::: W03-1006_245:259
M09,R01	Gildea and Hockenmaier (2003) present a system for labeling PropBanks semantic roles based on a statistical parser for combinatory categorial grammar (CCG) (Steedman 2000). ::: J05-1004_465:501
M02,R01	The data consists of sections of the Wall Street Journal part of the Penn TreeBank (Marcus et al. , 1993), with information on predicate-argument structures extracted from the PropBank corpus (Palmer et al. , 2005). ::: W05-0620_73:334
M02	The model of Gildea and Palmer (2002) For the Treebank-based system, we use the probability model of Gildea and Palmer (2002). ::: W03-1008_50:141
	We should note that Xue and Palmer (2004) define a similar feature template, called syntactic frame, which often captures similar information. ::: P05-1073_127:185
	(Charniak 2000) describes a different method which achieves very similar performance to (Collins 2000). ::: P02-1034_141:185
R01	In Table 4 we compare our system for semantic System P R F1 (Pradhan et al. , 2005) 80.9 76.8 78.8 Here 81.0 70.4 75.3 Table4: Evaluation of our methods for semantic role identification with Propbank (12 first iterations). ::: W07-0208_137:151
E01	As a result they show that the oracle f-score improves by over 2 points over the (Gildea and Hockenmaier, 2003) oracle results for the numbered arguments only (A0, , A5). ::: W09-2601_159:186
M02	Recent successes in statistical syntactic parsing based on supervised techniques trained on a large corpus of syntactic trees (Collins, 1999; Charniak, 2000; Henderson, 2003) have brought the hope that the same approach could be applied to the more ambitious goal of recovering the propositional content and the frame semantics of a sentence. ::: N06-2026_4:89
*P04,P05,P10	List of verbs for experiments # of Arg Freq senses number /set up 1 2 106 /emerge 1 1 80 /publish 1 2 113 /give 2 3/2 41 /build into 2 2/3 113 /enter 1 2 123 /take place 1 2 230 /pass 3 2 75 /hope 1 2 90 /increase 1 2 167 Table 2 Verb Test adverbial beneficiary(e.g. give support [to the plan]) object to be compared condition companion (e.g. talk [with you]) degree frequency location manner purpose or reason range(e.g. help you [in this aspect]) result(e.g. increase [to $100]) source(e.g. increase [from $50] to $100) temporal topic 3 Semantic Parsing 3.1 Architecture and Classifier Following the architecture of earlier semantic parsers like Gildea and Jurafsky (2002), we treat the semantic parsing task as a 1-of-N classification problem. ::: N04-1032_31:287
M13,M16	Examples are linearly interpolated relative frequency models (Gildea and Jurafsky, 2002), SVMs (Pradhan et al. , 2004), decision trees (Surdeanu et al. , 2003), and log-linear models (Xue and Palmer, 2004). ::: P05-1073_43:185
F07	Charniak (2000) shows the value his parser gains from parent annotation of nodes, suggesting that this information is at least partly complementary to information derivable from lexicalization, and Collins (1999) uses a range of linguistically motivated and carefully hand-engineered subcategorizations to break down wrong context-freedom assumptions of the naive Penn treebank covering PCFG, such as differentiating base NPs from noun phrases with phrasal modifiers, and distinguishing sentences with empty subjects from those where there is an overt subject NP. ::: P03-1054_18:233
E02	Our accuracy is most closely comparable to the 78.63% accuracy achieved on the full task by (Pradhan et al. , 2005a). ::: D07-1062_144:201
M13	Many classification techniques, SVM (Pradhan et al. , 2004b), perceptrons (Carreras and M`arquez, 2004a), Maximum Entropy (Xue and Palmer, 2004), etc. have been successfully used to solve SRL problems. ::: N06-1055_50:170
	In recent work, a number of researchers have cast this problem as a tagging problem and have applied various supervised machine learning techniques to it (Gildea and Jurafsky (2000, 2002); Blaheta and Charniak (2000); Gildea and Palmer (2002); Surdeanu et al. ::: N04-1030_9:243
E01,M13,R01,R02	We note that (a) the highest performance is reached for d = 3, (b) for PropBank our maximal accuracy (90.5%) 7f1 assigns equal importance to Precision P and Recall R, i.e. f1 = 2P RP+R . is substantially equal to the SVM performance (88%) obtained in (Hacioglu et al. , 2003) with degree 2 and (c) the accuracy on FrameNet (85.2%) is higher than the best result obtained in literature, i.e. 82.0% in (Gildea and Palmer, 2002). ::: P04-1043_170:222
V01	Most current SRL approaches can be classified in one of two classes: approaches that take advantage of complete syntactic analysis of text, pioneered by Gildea and Jurafsky (2002), and approaches that use partial syntactic analysis, championed by previous evaluations performed within the Conference on Computational Natural Language Learning (CoNLL) (Carreras and M`arquez, 2004). ::: H05-1081_9:230
R01	A0,A1,R01 stand for ARG0, ARG1 in PropBank (Palmer et al., 2005). ::: W11-0906_52:205
*F01,F02,F03,F04,F05,M02	During testing, the maxent model computes Baseline Features (Gildea and Jurafsky, 2002) pred predicate lemma path path from constituent to predicate ptype syntactic category (NP, PP, etc.) pos relative position to the predicate voice active or passive voice hw syntactic head word of the phrase sub-cat rule expanding the predicates parent Advanced Features (Pradhan et al., 2005) hw POS POS of the syntactic head word PP hw/POS head word and POS of the rightmost NP child if the phrase is a PP first/last word first/last word and POS in the constituent parent ptype syntactic category of the parent node parent hw/POS head word and POS of the parent sister ptype phrase type of left and right sister sister hw/POS head word and POS of left and right sister temporal temporal key words present partPath partial path predicate proPath projected path without directions Feature Combinations (Xue and Palmer, 2004) pred & ptype predicate and phrase type pred & hw predicate and head word pred & path predicate and path pred & pos predicate and relative position Table 1: SRL features for the baseline model the conditional probability P(a|t,p,v) of the argument label a, given the parse tree t, predicate p, and constituent node v. The classifier outputs the semantic role with the highest probability: a = argmax a P(a|t,p,v) (2) = argmax a P(a|(t,p,v)) (3) where (,,) is a feature map to an appropriate feature representation. ::: D09-1047_107:202
	Predicate argument clustering Some studies showed that verb clustering information could improve performance in semantic role labeling (Gildea and Jurafsky, 2002; Pradhan et al., 2008). ::: W11-0906_68:205
	For the initial experiments, we adopted the approach described by Gildea and Jurafsky (2002) (G&J) and evaluated a series of modifications to improve its performance. ::: N04-1030_14:243
	For our baseline SRL model, we adopt the features used in other state-of-the-art SRL systems, which include the seven baseline features from the original work of Gildea and Jurafsky (2002), additional features taken from Pradhan et al. ::: D09-1047_108:202
M02	The LTAG-spinal Treebank can be used to overcome some of the limitations of the previous work on SRL using LTAG: (Liu and Sarkar, 2007) uses LTAG-based features extracted from phrase-structure trees as an additional source of features and combined them with features from a phrase-structure based SRL framework; (Chen and Rambow, 2003) only considers those complement/adjunct semantic roles that can be localized in LTAG elementary trees, which leads to a loss of over 17% instances of semantic roles even from gold-standard trees. ::: W09-2601_17:186
*F01,F02,F03,F04,F05	Basic features from (Gildea and Jurafsky, 2002) predicate lemma and voice phrase type and head word path from phrase to predicate 1 position: phrase relative to predicate: before or after sub-cat records the immediate structure that expands from predicates parent 2 Additional features proposed by (Surdeanu et al. 2003; Pradhan et al. , 2004, 2005) predicate POS head word POS first/last word/POS POS of word immediately before/after phrase path length 1 LCA(Lowest CommonAnce stor) path from phrase to its lowest common ancestor with predicate punctuation immediately before/after phrase path trigrams: up to 9 are considered head word named entity label such as PER, ORG, LOC content word named entity label for PP parent node Additional features proposed by (Xue and Palmer, 2004) predicate phrase type predicate head word voice position syntactic frame 1 In Fig. ::: D07-1062_104:201
V01,M02,R01	We tested our model on a Semantic Role Labeling (SRL) benchmark, using PropBank annotations (Palmer et al., 2005) and automatic Charniak parse trees (Charniak, 2000) as provided for the CoNLL 2005 evaluation campaign (Carreras and M`arquez, 2005). ::: W09-1106_100:202
F02,M02	Instead of using the typical parse tree features used in typical SRL models, (Chen and Rambow, 2003) uses the path within the elementary tree from the predicate to the constituent argument. ::: W06-1518_91:103
A01,A02	It was shown that the identification of such event frames has a significant contribution for many Natural Language Processing (NLP) applications such as Information Extraction (Surdeanu et al. , 2003) and Question Answering (Narayanan and Harabagiu, 2004). ::: H05-1081_8:230
M02	Different from the widely used 127 feature functions that are based on the syntactic parse tree (Gildea and Jurafsky, 2002), we explore the use of LTAG-based features in a simple discriminative decision-list learner. ::: W06-1518_24:103
	The baseline feature set is a combination of features introduced by Gildea and Jurafsky (2002) and ones proposed in Pradhan et al. , (2004), Surdeanu et al. , (2003) and the syntactic-frame feature proposed in (Xue and Palmer, 2004). ::: P05-1072_32:208
M02	(Chen and Rambow, 2003) discuss a model for SRL that uses LTAG-based decomposition of parse trees (as is typically done for statistical LTAG parsing). ::: W06-1518_90:103
F02	All features but the 22 Split path feature are taken from existing semantic role labeling systems, see for example (Gildea and Jurafsky, 2002; Lim et al., 2004; Thompson et al., 2006). ::: D09-1003_57:203
R01,V01	We use data from the CoNLL-2004 shared taskthe PropBank (Palmer et al. , 2005) annotations of the Penn Treebank (Marcus et al. , 1993), with sections 1518 as the training set and section 20 as the development set. ::: N06-1054_102:249
M02,V01	In CoNLL-2005, full parsing trees are provided by two full parsers: the Collins parser (Collins, 1999) and the Charniak parser (Charniak, 2000). ::: W05-0638_24:90
R01	PropBank encodes propositional information by adding a layer of argument structure annotation to the syntactic structures of the Penn Treebank (Marcus et al. , 1993). ::: W06-2303_9:147
	The task of semantic role labelling(SRL),ashas been defined by previous researchers (Gildea and Jurafsky, 2002), requires collecting all the arguments that together with a verb form a predicate argument structure. ::: W08-2101_14:190
	Compared to the wealth of studies on verbal SRL (e.g., Gildea and Jurafsky (2002); Fleischman and Hovy (2003)), there is relatively little work that specifically addresses nominal SRL. ::: C08-1084_54:229
F01,F02,F03,F05	The PML method used here utilizes a modification of the backoff lattice method used by Gildea & Jurafsky (2002) to use a set of basic features specifically, the features employed for learning in this paper are Predicate (pr), Voice (vo), Phrase Type (pt), Distance (di), Head Word (hw), Path (pa), Preposition in a PP (pp), and an Actor heuristic. ::: W06-1622_10:165
	Most systems for automatic role-semantic analysis have used constituent syntax as in the Penn Treebank (Marcus et al., 1993), although there has also been much research on the use of shallow syntax (Carreras and Mrquez, 2004) in SRL. ::: D08-1008_17:220
M02	For example, Xue and Palmer (2004) reported that SRL performance dropped more than 10% when they used syntactic features from an automatic parser instead of the gold standard parsing trees. ::: C10-2083_13:87
E01	Gildea and Palmer (2002) achieve a recall of 0.50, a precision of 0.58, and an F-measure of 0.54 when using the full parser of Collins (1999). ::: W03-1006_246:259
	(Xue and Palmer, 2004)) and our experiments show that 91% of the SRL targets can be recovered despite this aggressive pruning. ::: D07-1062_51:201
F05,P03	The impact of our head-word based scoring is analyzed in Table 3, which compares results when only the head word must be correctly identified (as in Table 2) and to results when both the beginning and end of the argument must be correctly identified in the sentence (as in Gildea and Palmer (2002)). ::: W03-1008_125:141
	Charniak parser (Charniak, 2000) is used for POS tagging and full parsing. ::: P09-2064_75:110
R01,R02,V01	During the past few years, verbal SRL has dominated the research on SRL with the availability of FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005), and the consecutive CoNLL shared tasks (Carreras and Mrquez, 2004 & 2005) in English language. ::: D09-1133_14:236
M02	Some of these systems use features based on syntactic constituents produced by a Charniak parser (Pradhan et al. , 2003; Pradhan et al. , 2004) and others use only a flat syntactic representation produced by a syntactic chunker (Hacioglu et al. , 2003; Hacioglu and Ward, 2003; Hacioglu, 2004; Hacioglu et al. , 2004). ::: W05-0634_14:111
	To find a smaller set of effective features, we start with all the features considered in (Jiang and Ng, 2006), in (Xue and Palmer, 2004), and various combinations of them, for a total of 52 features. ::: P07-1027_98:194
M01	The re-ranking approach is the most promising one as suggested in (Toutanova et al. , 2005) but it does not clearly reveal if tree kernels can be used to learn the difference between correct or incorrect argument structures. ::: W06-2909_81:206
M02	Introduction Robust syntactic parsers, made possible by new statistical techniques (Ratnaparkhi 1997; Collins 1999, 2000; Bangalore and Joshi 1999; Charniak 2000) and by the availability of large, hand-annotated training corpora (Marcus, Santorini, and Marcinkiewicz 1993; Abeille 2003), have had a major impact on the field of natural language processing in recent years. ::: J05-1004_7:501
M02	Kernel Setup: We use the Constituent, Predicate, and Predicate-Constituent related features, which are reported to get the best-reported performance (Pradhan et al. , 2005a), as the baseline features. ::: P07-1026_156:194
*M12	An important consideration within this general SRL architecture is the combination of systems and input annotations.Most SRL systems include some kind of combination to increase robustness, gain coverage, and reduce effects of parse errors.One may combine: (1) the output of several independent SRL basic systems (Surdeanu et al.2007; Pradhan et al.2005b), or (2) several outputs from the same SRL system obtained by changing input annotations or other internal parameters (Koomen et al. 2005; Toutanova, Haghighi, and Manning 2005).The combination can be as simple as selecting the best among the set of complete candidate solutions, but usually consists of combining fragments of alternative solutions to construct the nal output.Finally, the combination component may involve machine learning or not.The gain in performance from the combination step is consistently between two and three F 1 points.However, a combination approach increases system complexity and penalizes efciency. ::: J08-2001_50:86
M02	It has previously been shown that SRL systems need a syntactic structure as input (Gildea and Palmer, 2002; Punyakanok et al., 2008). ::: C08-1050_14:214
	This follows the pruning heuristic of (Xue and Palmer, 2004) often used by SRL algorithms. ::: P10-1024_148:365
R01,V01	In the recent years, most works on SRL, including two CoNLL shared task in 2004 and 2005, focus on verbal predicates with the availability of PropBank (Palmer et al., 2005). ::: D09-1004_16:221
	A wide range of features have been shown to be useful in previous work on semantic role labeling for verbal predicates (Gildea and Jurafsky, 2002; Pradhan et al. , 2004b; Xue and Palmer, 2004) and our experiments show most of them are also effective for SRL of nominalized predicates. ::: N06-1055_73:170
	Perhaps the closest work to that of ours is that of (Pradhan et al. , 2004a), where they reported preliminary work for analyzing the predicate-argument structure of Chinese nominalizations, using a small data set of 630 proposition for 22 nominalizations taken from the Chinese Treebank. ::: N06-1055_153:170
R01	Bank (PropBank) (Palmer et al. , 2005) annotates the Penn TreeBank with verb argument structure. ::: W05-0620_80:334
*F01,P10,R02	FrameNet thus consists of both a computational lexicon and a role-annotated corpus.The existence of such a corpus enabled Gildea and Jurafsky (2002) to develop the rst statistical machine learning approach to SRL, using various lexical and syntactic features such as phrase type and grammatical function calculated over the annotated constituents.Although this research spurred the current wave of SRL work that has 148 M`arquez, Carreras, Litkowski, and Stevenson Semantic Role Labeling rened and extended Gildea and Jurafskys approach, the FrameNet data has not been used extensively.One issue is that the corpus is not a representative sample of the language, but rather consists of sentences chosen manually to illustrate the possible role assignments for a given lexical item.Another issue is that the semantic roles are situation-specic, rather than general roles like Agent, Theme, and Location that can be used across many situations and genres. ::: J08-2001_33:86
M02	Full parser of Charniak (2000). ::: W05-0620_111:334
M09	For instance, Gildea and Hockenmaier (2003) reported that a CCG-based parser gives improved results over the Collins parser. ::: C08-1050_18:214
M02	The key of the well-known pruning algorithm raised in (Xue and Palmer, 2004) is extracting sisters of ancestors as role candidates. ::: C08-1105_122:211
	For each argument, we started with the set of features introduced by (Gildea and Jurafsky, 2002). ::: P05-1072_91:208
M07	Methodologically, these initial results on a joint solution to parsing and semantic role labelling provide the first direct test of whether parsing is necessary for semantic role labelling (Gildea and Palmer, 2002; Punyakanok et al. , 2005a). ::: N06-2026_83:89
E02,M17	However, inmost cases they can only provide a local dependency between predicate and argument for 87% of the argument constituents (Chen and Rambow, 2003), which is too low to provide high SRL accuracy. ::: D07-1062_33:201
V02	In Senseval-3, 40 frames were selected for an SRL task with the goal of replicating Gildea and Jurafsky (2002) and improving on them (Litkowski 2004).Participants were evaluated on assigning semantic roles to given arguments, with best F 1 of 92%, and on the task of segmenting and labeling arguments, with best F 1 of 83%. ::: J08-2001_66:86
R02	Availability of linguistically annotated corpora such as the Penn Treebank (Bies et al. , 1995), Proposition Bank (Palmer et al. , 2005), and FrameNet (Johnson et al. , 2003) has stimulated much research on methods for automatic syntactic and semantic analysis of text. ::: W07-0208_3:151
F01,F02,F03,F04,F05,F07	Firstly, the 26 constituent-based features used by others are: y The seven "standard" features: predicate (c1), path (c2), phrase type (c3), position (c4), voice (c5), head word (c6) and predicate subcategorization (c7) features proposed by (Gildea and Jurafsky, 2002). ::: C10-2076_58:199
	Another approach used similarity measures either between verbs (Gordon and Swanson, 2007) or between nouns (Gildea and Jurafsky, 2002) to overcome lexical sparsity. ::: P09-1004_79:272
M02	Regarding the use of tree kernels for SRL, in (Moschitti, 2004) two main drawbacks have been 49 pointed out: Highly accurate boundary detection cannot be carried out by a tree kernel model since correct and incorrect arguments may share a large portion of the encoding trees, i.e. they may share many substructures. ::: W06-2607_19:203
R01	We use the standard training set, consisting of sections 02-21 of the Wall Street Journal (WSJ) portion of the Penn Treebank, labeled with PropBank (Palmer et al., 2005) annotations for predicates and arguments. ::: P10-1099_49:230
	This, as suggested in (Pradhan et al., 2005; Moschitti, 2004), can help stressing the differ803 ences between different argument types. ::: P08-1091_165:221
	We first used Polynomial Kernels over handcrafted, linguistically-motivated, standard SRL features (Gildea and Jurafsky, 2002; Pradhan et al., 2005; Xue and Palmer, 2004). ::: N09-2022_27:90
*A01,A02,A03,A04,V01,V02,V03	With the advent of supporting resources, SRL has become a well-dened task with a substantial body of work and comparative evaluation (see, among others, Gildea and Jurafsky [2002], Surdeanu et al.[2003], Xue and Palmer [2004], Pradhan et al.[2005a], the CoNLL Shared Task in 2004 and 2005, and Senseval-3 and SemEval-2007).The identication of event frames may potentially benet many natural language processing (NLP) applications, such as information extraction (Surdeanu et al.2003), question answering (Narayanan and Harabagiu 2004), summarization (Melli et al.2005), and machine translation (Boas 2002).Related work on classifying the semantic relations in noun phrases has also been encouraging for NLP tasks (Moldovan et al.2004; Rosario and Hearst 2004). ::: J08-2001_16:86
	In (Gildea and Jurafsky, 2002) seven different features2, which aim to capture the relation between the predicate and its arguments, were proposed. ::: E06-1015_104:212
R01	Automatic Semantic Role Labeling (SRL) systems, made possible by the availability of PropBank (Kingsbury and Palmer, 2003; Palmer et al. , 2005), and encouraged by evaluation efforts in (Carreras and Marquez, 2005; Litkowski, 2004), have been shown to accurately determine the argument structure of verb predicates. ::: W06-1617_8:156
	State-of-the-art statistical parsers trained on the Penn Treebank (PTB) (Marcus et al. , 1993) proS a8a8 a8a8a8 a72a72 a72a72a72 NP-SBJ a16a16a16 a 80 a 80 a 80 the authority VP a16a16a16 a16a16a16a16 a0 a0a0 a64 a64a64 a80a80a80 a80a80a80a80 VBD dropped PP-TMP a8a8 a72a72IN at NP NN midnight NP-TMP NNP Tuesday PP-DIR a8a8 a72a72TO to NP QP a16a16a16 a80a80a80$ 2.80 trillion Figure 1: A sample syntactic structure with function labels. ::: W05-1509_10:202
R01	The semantic roles in the examples are labeled in the style of PropBank (Palmer et al., 2005), a broad-coverage human-annotated corpus of semantic roles and their syntactic realizations. ::: P11-1112_16:246
F07,M02,R01	The Propbank (Palmer et al., 2005) is an annotated corpus of verb subcategorization and alternations which was created by adding a layer of predicate-argument annotation over the phrase structure trees in the Penn Treebank. ::: W09-2601_38:186
A01,A02,A05	Knowledge of semantic argument structure is essential for language understanding and thus important for applications such as information extraction (Moschitti et al., 2003; Surdeanu et al., 2003), question answering (Shen and Lapata, 2007), or recognizing textual entailment (Burchardt et al., 2009). ::: C10-2107_15:208
R02,V02	Noun predicates also appear in FrameNet semantic role labeling (Gildea and Jurafsky, 2002), and many FrameNet SRL systems are evaluated in Senseval-3 (Litkowski, 2004). ::: P07-1027_20:194
O02	Furthermore, corpora labelled with semantic role information can be used to train shallow semantic parsers (Gildea and Jurafsky, 2002), which could in turn benefit applications in need of broad-coverage semantic analysis. ::: P06-1146_17:209
P06	In addition, a heuristic based pruning preprocessing (Xue and Palmer, 2004) is used to filter out a lot of apparently inappropriate constituents at the beginning. ::: C10-2076_19:199
F02,M02	In standard SRL systems, these path features usually consist of a sequence of constituent parse nodes representing the shortest path through the parse tree between a word and the predicate (Gildea and Jurafsky, 2002). ::: P10-1099_104:230
R01,R02	Corpora set-up The above kernels were experimented over two corpora: PropBank (www.cis.upenn.edu/ ace) along with Penn TreeBank5 2 (Marcus et al. , 1993) and FrameNet. ::: P04-1043_144:222
	The effectiveness of the proposed additional pruning techniques may be seen as a significant improvement over the original algorithm of (Xue and Palmer, 2004). ::: D09-1004_185:221
	The usual approach (Toutanova et al. , 2005) uses a traditional boundary classifier (TBC) to select the set of potential argument nodes. ::: W06-2909_57:206
	Many of the combinations depart from the manually selected conjunctions of Xue and Palmer (2004). ::: W05-0620_270:334
R01	The semantic roles in the examples are labeled in the style of PropBank (Palmer et al., 2005), a broad coverage human-annotated corpus of semantic roles and their syntactic realizations. ::: D11-1122_15:233
O02	However, the performance decrease shows that negation scope finding is not as sensitive to automatic syntactic parsing as common shallow semantic parsing, whose performance might decrease by about ~10 in F1-measure (Toutanova et al., 2005). ::: C10-1076_159:201
	As regards i), recently there has been an increase in the number of papers dealing with nominalized predicates (Pradhan et al., 2004) (Jiang and Ng, 2006) (Xue, 2006) (Liu and Ng, 2007). ::: D08-1055_11:129
V01	On the WSJ data, our results surpass with almost 6% the results obtained by the best SRL system that used partial syntax in the CoNLL 2004 shared task evaluation (Hacioglu et al. , 2004). ::: W05-0635_66:78
R01	See (Palmer et al. , 2005) for a detailed discussion of PropBank semantic roles labels. ::: W06-2303_16:147
	In the following decade, great success in terms of parse disambiguation and even language modeling was achieved by various lexicalized PCFG models (Magerman, 1995; Charniak, 1997; Collins, 1999; Charniak, 2000; Charniak, 2001). ::: P03-1054_8:233
M02	(Chen and Rambow, 2003) use LTAG-based decomposition of parse trees (as is typically done for statistical LTAG parsing) for SRL. ::: D07-1062_178:201
M07	Previous research has shown the benefit of jointly learning semantic roles of multiple constituents (Toutanova et al., 2008; Koomen et al., 2005). ::: D09-1047_25:202
	Note also that the transformations which are taken into account are a superset of the transformations taken into account by Gildea and Palmer (2002). ::: W03-1006_174:259
M02,E01	The 50-best parser is a probabilistic parser that on its own produces high quality parses; the maximum probability parse trees (according to the parsers model) have an f-score of 0.897 on section 23 of the Penn Treebank (Charniak, 2000), which is still state-of-the-art. ::: P05-1022_8:180
R01	The inter-annotator agreement for PropBank reported in (Palmer et al. , 2005) is above 0.9 in terms of the Kappa statistic (Sidney and Castellan Jr. , 1988). ::: W06-1617_138:156
R01,M09	Using the CCGbased parser, Gildea and Hockenmaier (2003) find a 2% absolute improvement over the Collins parser in identifying core or numbered PropBank arguments. ::: J05-1004_469:501
M13,V01	Train Devel tWSJ tBrown Sentences 39,832 1,346 2,416 426 Tokens 950,028 32,853 56,684 7,159 Propositions 90,750 3,248 5,267 804 Arguments 239,858 8,346 14,077 2,177 Table 2: Counts on the data set The preprocessing modules used in CONLL2005 include an SVM based POS tagger (Gimenez and M`arquez, 2003), Charniak (2000)s full syntactic parser, and Chieu and Ng (2003)s Named Entity recognizer. ::: P06-2010_127:184
M07,M10	The improvement achieved by the joint model relative to the local model is about 2 points absolute in F-Measure, similar to the improvement when gold-standard syntactic parses are used (Toutanova et al. , 2005). ::: W05-0623_77:79
M02,M09,M17	Other syntactic representations such as CCG derivations (Gildea and Hockenmaier, 2003) and dependency trees (Hacioglu, 2004; Surdeanu et al., 2008) have also been explored. ::: W09-2601_12:186
	Punyakanok et al. , (2004) further showed that constituent-by-constituent (Cby-C) tagging is better than P-by-P. ::: W06-3308_61:196
	In our experiments we employ the features listed in Table 1, defined in (Gildea and Jurafsky, 2002; Pradhan et al., 2005; Xue and Palmer, 2004). ::: P08-1091_98:221
	The alternative approach is to combine heuristic and machine-learning approaches (Xue and Palmer, 2004). ::: N06-1055_62:170
	Hacioglu has previously described a chunk based semantic labeling method (Hacioglu et al. , 2004). ::: P05-1072_153:208
M02	In this category, it is also noticeable the use of the syntactic frame feature, proposed by Xue and Palmer (2004). ::: W05-0620_265:334
M02,R01	We present performance results on the February 2004 version of PropBank on gold-standard parse trees as well as results on automatic parses generated by Charniaks parser (Charniak, 2000). ::: P05-1073_15:185
	It is interesting to note that the semantic frames are a helpful way of generalizing between predicates; words in the same frame have been found frequently to share the same syntactic argument structure (Gildea and Jurafsky 2002). ::: J05-1004_276:501
M02	Accordingly, we do not maximize the probability of the entire labeled parse tree as in (Toutanova et al. , 2005). ::: P07-1027_85:194
R01,R02	Detecting and classifying the arguments of predicates has been an active area of research in recent years, driven by the availability of large-scale semantically annotated corpora such as the FrameNet (Baker et al. , 1998) and the Propbank (Palmer et al. , 2005). ::: N06-1055_8:170
	(Gildea and Palmer, 2002) report the results listed on the first line of Table 2. ::: P03-1002_128:187
	In spite of the above difficulties, there are proposition banks in the newswire domain that are adequate for training SRL systems (Xue and Palmer, 2004; Palmer et al. , 2005). ::: W06-0602_27:182
