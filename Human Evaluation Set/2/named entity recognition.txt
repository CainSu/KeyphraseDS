You should select [10] representative sentences! 

[]	Moreover, the experimental results shown in Tables 3 and F=1 additional resources ASO-semi 89.31 unlabeled data (27M words) (Ando and Zhang, 2005) (Florian et al. , 2003) 88.76 their own large gazetteers, 2M-word labeled data (Chieu and Ng, 2003) 88.31 their own large gazetteers, very elaborated features HySOL 88.14 unlabeled data (17M words) supplied gazetters HySOL 87.20 unlabeled data (17M words) Table 5: Previous top systems in NER (CoNLL2003) experiments F=1 additional resources ASO-semi 94.39 unlabeled data (Ando and Zhang, 2005) (15M words: WSJ) HySOL 94.30 unlabeled data (17M words: Reuters) (Zhang et al. , 2002) 94.17 full parser output (Kudo and Matsumoto, 2001) 93.91 Table 6: Previous top systems in Chunking (CoNLL-2000) experiments 4 indicate that HySOL is rather robust with respect to the hyper-parameter since we can obtain fairly good performance without a prior distribution. 
[]	NER proves to be a knowledge intensive task, and it was reassuring to observe that System Resources Used F1 + LBJ-NER Wikipedia, Nonlocal Features, Word-class Model 90.80 (Suzuki and Isozaki, 2008) Semi-supervised on 1Gword unlabeled data 89.92 (Ando and Zhang, 2005) Semi-supervised on 27Mword unlabeled data 89.31 (Kazama and Torisawa, 2007a) Wikipedia 88.02 (Krishnan and Manning, 2006) Non-local Features 87.24 (Kazama and Torisawa, 2007b) Non-local Features 87.17 + (Finkel et al., 2005) Non-local Features 86.86 Table 7: Results for CoNLL03 data reported in the literature.
[]	Table 2: Tags for positions in a word Tag Description S one-character word B first character in a multi-character word E last character in a multi-character word I intermediate character in a multi-character word (only for words longer than 2 chars) Table 3: Tags for character types Tag Description ZSPACE Space ZDIGIT Digit ZLLET Lowercase alphabetical letter ZULET Uppercase alphabetical letter HIRAG Hiragana KATAK Katakana OTHER Others (Kanji etc.) 3.3 Support Vector Machine-based Chunking We used the chunker yamcha (Kudo and Matsumoto, 2001), which is based on support vector machines (Vapnik, 1998). 
[1]	Supervised learning methods can effectively solve NER problem by learning a model from manually labeled data (Borthwick, 1999; Sang and Meulder, 2003; Gao et al., 2005; Florian et al., 2003).
[]	Since MUC-6, BBN's Hidden Markov Model (HMM) based IdentiFinder (Bikel et al. , 1997) has achieved remarkably good performance.
[]	ME tagger is based on Ratnaparkhi (1996)s POS tagger and is described in Curran and Clark (2003). 
[1]	Our system is a Maximum Entropy Markov Model, which further develops a system earlier used for the CoNLL 2003 shared task (Klein et al. , 2003) and the 2004 BioCreative critical assessment of information extraction systems, a task that involved identifying gene and protein name mentions but not distinguishing between them (Dingare et al. , 2004). 
[]	(Bikel et al. , 1997) and (Bikel et al. , 1999) are other examples of the use of HMMs. 
[]	Approaches to NER There has been a considerable amount of work on NER in English (Isozaki and Kazawa, 2002; Zhang and Johnson, 2003; Petasis et al., 2001; Mikheev et al., 1999). 
[]	Comparison with previous best results: KM01 (Kudoh and Matsumoto, 2001), CM03 (Carreras and Marquez, 2003), SP03 (Sha and Pereira, 2003), ZDJ02 (Zhang et al. , 2002). 
[]	Table 1 presents the results of our system using three learning algorithms, the uneven margins SVM, the standard SVM and the PAUM on the CONLL2003 test set, together with the results of three participating systems in the CoNLL-2003 shared task: the best system (Florian et al. , 2003), the SVM-based system (May eld et al. , 2003) and the Perceptron-based system (Carreras et al. , 2003).
[]	For the Penn Treebank, (Ratnaparkhi, 1996) reports an accuracy of 96.6% using the Maximum Entropy approach, our much simpler and therefore faster HMM approach delivers 96.7%.
[]	Both of these beliefs are questionable, as the top MUC 7 performance of 93.39% Entity Precision Recall F-Score Fully Correct protein 77.40% 68.48% 72.67% DNA 66.19% 69.62% 67.86% RNA 72.03% 65.89% 68.83% cell line 59.00% 47.12% 52.40% cell type 62.62% 76.97% 69.06% Overall 71.62% 68.56% 70.06% Left Boundary Correct protein 82.89% 73.34% 77.82% DNA 68.47% 72.01% 70.19% RNA 75.42% 68.99% 72.06% cell line 63.80% 50.96% 56.66% cell type 63.93% 78.57% 70.49% Overall 75.72% 72.48% 74.07% Right Boundary Correct protein 84.70% 74.96% 79.53% DNA 74.43% 78.29% 76.31% RNA 78.81% 72.09% 75.30% cell line 70.2% 56.07% 62.34% cell type 71.68% 88.10% 79.05% Overall 79.65% 76.24% 77.91% Table 2: Results on the evaluation data (Mikheev et al. , 1998) in the domain of newswire text used an easier performance metric where incorrect boundaries were given partial credit, while both the biomedical NER shared tasks to date have used an exact match criterion where one is doubly penalized (both as a FP and as a FN) for incorrect boundaries. 
[1]	When building probabilistic models for tag sequences, we often decompose the global probability of sequences using a directed graphical model (e.g. , an HMM (Brants, 2000) or a conditional Markov model (CMM) (Ratnaparkhi, 1996)). 
[]	There have been several efforts to apply machine learning techniques to the same task (Cowie, 1995) (Bikel et al, 1997) (Gallippi, 1996) (Bennett et al, 1997) (Borthwick et al, 1997).
[1]	Prefix and suffix tries were also used previously (Cucerzan and Yarowsky, 1999). 
[1]	At any rate, regularized conditional loglinear models have not previously been applied to the problem of producing a high quality part-of-speech tagger: Ratnaparkhi (1996), Toutanova and Manning (2000), and Collins (2002) all present unregularized models. 
[]	Since in previous work, many NER systems have been applied successfully in newswire domain (Zhou and Su 2002; Bikel et al. 1999; Borthwich et al. 1999), more and more explorations have been done to port existing NER system into biomedical domain (Kazama et al. 2002; Takeuchi et al. 2002; Nobata et al. 1999 and 2000; Collier et al. 2000; Gaizauskas et al. 2000; Fukuda et al. 1998; Proux et al. 1998).
[1]	We use a support vector machine (SVM)based chunker yamcha (Kudo and Matsumoto, 2001) for the chunking process. 
[1]	There is a similar brief class (BWC) (Settles 2004) which collapses consecutive characters into one. 
[1]	Most existing work to capture label consistency, has attempted to create all parenleftbign2parenrightbig pairwise dependencies between the different occurrences of an entity, (Finkel et al. , 2005; Sutton and McCallum, 2004), where n is the number of occurrences of the given entity.
[]	The Stanford Tagger is based on Conditional Random Fields (Finkel et al., 2005). 
[]	See (Florian et al. , 2003) for such a study. 
[]	The results we obtained on the CoNLL03 test set were consistent with what was reported in (Finkel et al., 2005). 
[]	We are also focusing on other potential applications, including chunking (Sha and Pereira, 2003), named entity recognition (Florian et al. , 2004; Ando and Zhang, 2005b; Daume III and Marcu, 2006), and speaker adaptation (Kuhn et al. , 1998).
[1]	SVM has given high performance in various classification tasks (Joachims, 1998; Kudo and Matsumoto, 2001). 
[]	SVMs have given high performance in various classification tasks (Joachims, 1998; Kudo and Matsumoto, 2001). 
[1]	In this paper, we present an approach for extracting the named entities (NE) of natural language inputs which uses the maximum entropy (ME) framework (Berger et al. , 1996). 
[]	For example, the average F1 of the Stanford NER (Finkel et al., 2005) , which is trained on the CoNLL03 shared task data set and achieves state-of-the-artperformanceonthattask, dropsfrom 90.8%(RatinovandRoth,2009)to45.8%ontweets.
[]	(Bikel et al. , 1997) report on Nymble, an HMM-based name tagging system operating in English and Spanish. 
[]	For example, the average F1 of the Stanford NER (Finkel et al., 2005) , which is trained on the CoNLL03 shared task data set and achieves state-of-the-artperformanceonthattask, dropsfrom 90.8%(RatinovandRoth,2009)to45.8%ontweets. 
[]	Statistical methods such as HMM (Bikel et al., 1997; Zhou and Su, 2001), Decision tree model (Baluja et al., 2000; Isozaki, 2001), and conditional random fields (McCallum, 2003) have been used. 
[]	NERC has been investigated using supervised(McCallum and Li, 2003), unsupervised (Etzioni et al., 2005) and semi-supervised (Pasca et al., 2006b) learning methods. 
[]	We use the maximum entropy tagging method described in (Kazama et al. , 2001) for the experiments, which is a variant of (Ratnaparkhi, 1996) modified to use HMM state features.
