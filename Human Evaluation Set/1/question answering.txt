You should select [10] representative sentences! 

[1]	This probability is computed using IBMs Model 1 (Brown et al., 1993): P(Q|A) = product display qQ P(q|A) (3) P(q|A) = (1)Pml(q|A)+Pml(q|C) (4) Pml(q|A) = summation display aA (T(q|a)Pml(a|A)) (5) where the probability that the question term q is generated from answer A, P(q|A), is smoothed using the prior probability that the term q is generated from the entire collection of answers C, Pml(q|C). 
[1]	ANSWER SELECTION modules typically work either by trying to prove the answer is correct (Moldovan & Rus, 2001) or by giving them a weight produced by summing a collection of heuristic features (Radev et al. , 2000); in the latter case candidates having a larger number of matching query terms, even if they do not exactly match the context in the question, might generate a larger score than a correct passage with fewer matching terms. 
[]	To evaluate our learning approach, we trained AQUAREA$ on the same development set of stories and tested it on the same test set of stories as those used in all past work on the reading comprehension task (Hirschman et al. , 1999; Charniak et al. , 2000; Riloffand Thelen, 2000; Wang et al. , 2000).
[]	In prior work (Hirschman et al. , 1999; Charniak et al. , 2000; Riloffand Thelen, 2000) the number and type of information sources used for computation is specific to and rlifFerent for each question type.
[]	In this field as well, the use of pre-defined sets of relation patterns has proved fairly reliable, particularly in the case of factoid type queries (Brill et al. , 2002; Ravichandran and Hovy, 2002; Hovy et al. , 2002; Soubbotin and Soubbotin, 2002).
[]	Recently, some pioneering studies have investigated approaches to automatically construct QA components from scratch by applying machine learning techniques to training data (Ittycheriah et al. , 2001a)(Ittycheriah et al. , 2001b)(Ng et al. , 2001) (Pasca and Harabagiu)(Suzuki et al. , 2002)(Suzuki 215 Table 1: Number of Questions in Question Types of CRL QA Data # of Questions # of Question Types Example 1-9 74 AWARD, CRIME, OFFENSE 10-50 32 PERCENT, N PRODUCT, YEAR PERIOD 51-100 6 COUNTRY, COMPANY, GROUP 100-300 3 PERSON, DATE, MONEY Total 115 et al. , 2003) (Zukerman and Horvitz, 2001)(Sasaki et al. , 2004). 
[]	This method was first described in Ravichandran and Hovy (2002). 
[]	This is referred to as the 10-90 rule (Moldovan and Rus 2001). 
[]	Open-domain question answering (Lehnert, 1986; Harabagiu et al. , 2001; Light et al. , 2001) and story comprehension (Hirschman et al. , 1999) have become important directions in natural language processing. 
[]	To help our model learn that it is desirable to copy answer words into the question, we add to each corpus a list of identical dictionary word pairs w iw i . For each corpus, we use GIZA (Al-Onaizan et al. , 1999), a publicly available SMT package that implements the IBM models (Brown et al. , 1993), to train a QA noisy-channel model that maps flattened answer parse trees, obtained using the cut procedure described in Section 3.1, into questions. 
[]	(Berger et al. , 1996) gave a good description of ME model. 
[]	Recent years have witnessed significant progress in developing methods for the automatic identification and labeling of semantic roles conveyed by sentential constituents.1 The success of these methods, often referred to collectively as shallow semantic parsing (Gildea and Jurafsky, 2002), is largely due to the availability of resources like FrameNet (Fillmore et al. , 2003) and PropBank (Palmer et al. , 2005), which document the surface realization of semantic roles in real world corpora. 
[1]	While probabilistic or web-based methods for answer validation have been previously explored in the literature (Magnini et al. , 2002), these approaches have modeled the relationship between a question and a (correct) answer in terms of relevance and have not tried to approximate the deeper semantic phenomena that are involved in determining answerhood. 
[1]	Many studies on QA have focused on discriminative models to predict a function of matching features between each question and candidate passage (set of sentences), namely q/a pairs, e.g., (Ng et al., 2001; Echihabi and Marcu, 2003; Harabagiu and Hickl, 2006; Shen and Klakow, 2006; Celikyilmaz et al., 2009).
[]	Ravichandran and Hovy (2002) present an alternative ontology for type preference and describe a method for using this alternative ontology to extract particular answers using surface text patterns. 
[]	We also used the answer keys created by the DeepRead developers (Hirschman et al. , 1999). 
[]	For example, syntactic information has been deployed to reformul+ate questions (Hermjakob et al. , 2002) or to replace questions by syntactically similar ones (Lin and Pantel, 2001); lexical ontologies such as Wordnet1 have been used to find synonyms for question words (Burke et al. , 1997; Hovy et al. , 2000; Prager et al. , 2001; Harabagiu et al. , 2001), and statistical machine translation (SMT) models trained on question-answer pairs have been used to rank candidate answers according to their translation probabilities (Berger et al. , 2000; Echihabi and Marcu, 2003; Soricut and Brill, 2006). 
[]	Data are collected from four sources: 4,500 English questions published by USC (Hovy et al. , 2001), about 500 manually constructed questions for a few rare classes, 894 TREC 8 and TREC 9 questions, and also 500 questions from TREC 10 which serves as our test set 3 . These questions were manually labeled according to our question hierarchy. 
[]	We convert each text into logic form (Moldovan and Rus, 2001). 
[1]	The use of lexical patterns to identify answers in corpus-based QA received lots of attention after a team taking part in one of the earlier QA Tracks at TREC showed that the approach was competitive at that stage (Soubbotin and Soubbotin, 2002; Ravichandran and Hovy, 2002). 
[]	Similarly, Murdock and Croft (2005) adopted a simple translation model from IBM model 1 (Brown et al. , 1990; Brown et al. , 1993) and applied it to QA. 
[]	Echihabi and Marcu (2003) presented a noisy channel approach in which they adapted the IBM model 4 from statistical machine translation (Brown etal. 
[1]	To help our model learn that it is desirable to copy answer words into the question, we add to each corpus a list of identical dictionary word pairs w iw i . For each corpus, we use GIZA (Al-Onaizan et al. , 1999), a publicly available SMT package that implements the IBM models (Brown et al. , 1993), to train a QA noisy-channel model that maps flattened answer parse trees, obtained using the cut procedure described in Section 3.1, into questions.
[1]	In a related but independent effort, a group at MITRE has investigated question answering in the context of the reading comprehension task (Hirschman et al. , 1999). 
[]	For example, syntactic information has been deployed to reformul+ate questions (Hermjakob et al. , 2002) or to replace questions by syntactically similar ones (Lin and Pantel, 2001); lexical ontologies such as Wordnet1 have been used to find synonyms for question words (Burke et al. , 1997; Hovy et al. , 2000; Prager et al. , 2001; Harabagiu et al. , 2001), and statistical machine translation (SMT) models trained on question-answer pairs have been used to rank candidate answers according to their translation probabilities (Berger et al. , 2000; Echihabi and Marcu, 2003; Soricut and Brill, 2006).
[]	Capturing question or answer dependencies can be cast as a straightforward process of mapping syntactic trees to sets of binary head modifier relationships, as first noted in (Collins, 1996).
[1]	In (Echihabi and Marcu, 2003) another form of combining strategies for advanced QA is proposed: (1) a knowledge-based Q/A implementation based on syntactic/semantic processing is combined using a maximum-entropy framework with (2) a statistical noisy-channel algorithm for Q/A and (3) a pattern-based approach that learn from Web data.
[1]	Several QA systems have investigated the use of text patterns for QA (Soubbotin and Soubbotin, 2001), (Soubbotin and Soubbotin, 2002), (Ravichandran and Hovy, 2002). 
[1]	Wordnets and ontologies are very common resources and are employed in a wide variety of direct and indirect QA tasks, such as reasoning based on axioms extracted from WordNet (Moldovan et al. , 2003), probabilistic inference using lexical relations for passage scoring (Paranjpe et al. , 2003), and answer filtering via WordNet constraints (Leidner et al. , 2003).
[]	IBMs Statistical QA (Ittycheriah et al. , 2001a) system uses a probabilistic model trainable from Question-Answer sentence pairs.
