You should select [10] representative sentences! 

[]	Table 1: Sentence scoring metrics Name Description Source POS F Closeness to the beginning of the document: 1i (Edmundson, 1969) POS L Closeness to the end of the document: i (Baxendale, 1958) POS B Closeness to the borders of the document: max(1i, 1ni+1) (Lin and Hovy, 1997) LEN W Number of words in the sentence (Satoshi et al., 2001) LEN CH Number of characters in the sentence5 LUHN maxi{clusters(S)}{CSi}, CSi = W2iNi (Luhn, 1958) KEY Sum of the keywords frequencies: summation textt{Keywords(S)} tf(t) (Edmundson, 1969) COV Ratio of keywords number (Coverage): |Keywords(S)||Keywords(D)| (Liu et al., 2006a) TF Average term frequency for all sentence words: summationtext tS tf(t) N (Vanderwende et al., 2007) TFISF summationtexttS tf(t) isf(t), isf(t) = 1 log(n(t))log(n) , (Neto et al., 2000) n(t) is the number of sentences containing t SVD Length of a sentence vector in 2 V T after computing Singular Value (Steinberger and Jezek, 2004) Decomposition of a term by sentences matrix A = UV T TITLE O Overlap similarity6 to the title: sim(S,T) = |ST|min{|S|,|T|} (Edmundson, 1969) TITLE J Jaccard similarity to the title: sim(S,T) = |ST||ST| TITLE C Cosine similarity to the title: sim(vectorS, vectorT) = cos(vectorS, vectorT) = vectorSvectorT|vectorS||vectorT| D COV O Overlap similarity to the document complement new sim(S,D S) = |ST|min{|S|,|DS|} D COV J Jaccard similarity to the document complement sim(S,D S) = |ST||SDS| D COV C Cosine similarity to the document complement cos(vectorS, vectorD S) = vectorS vectorDS|vectorS|| vectorDS| LUHN DEG Graph-based extensions of LUHN, KEY and COV measures respectively.
[]	This method of evaluation has already been used in other summarization evaluations such as Edmundson (1969) and Marcu (1997). 
[]	Lin and Hovy (2000) first introduced topic signatures which are topic relevant terms for summarization. 
[]	Barzilay and Elhadad (1997) constructed lexical chains and extracted strong chains in summaries. 
[]	Utility (RU) (Radev et al. , 2000) is tested on a large corpus for the first time in this project. 
[]	Lapata (2003) proposed most of these features. 
[1]	Pyramid approach was introduced by Nenkova and Passonneau (2004) as a method for evaluating machine-generated summaries based on a set of human model summaries. 
[]	We use the pyramid evaluation method (Nenkova and Passonneau, 2004) at the sentence level to evaluate the summary created for each set. 
[]	In addition, this is the same even if we use the SummBank corpus (Radev et al. , 2003). 
[]	ROUGE (Lin, 2004), a recall-oriented evaluation package for automatic summarization. 
[]	The authors address a number of shortcomings of manual and automatic summary evaluation methods such as ROUGE (Lin and Hovy, 2003), and argue that the Pyramid method is reliable, diagnostic and predictive. 
[]	We provided more in-depth discussion of this issue in other papers (Lin and Hovy, 2002; Lin and Hovy 2003b). 
[1]	Automatic evaluation was performed with ROUGE (Lin, 2004), a widely used and recognized automated summarization evaluation method. 
[1]	The summary sentences can also be selected by using machine learning methods (Kupiec et al., 1995; Amini and Gallinari, 2002) or graph-based methods (ErKan and Radev, 2004; Mihalcea and Tarau, 2004). 
[]	These methods usually represent the documents as term-sentence matrices (where each row represents a sentence and each column represents a term) or graphs (where each node is a sentence and each edge represents the pairwise relationship among corresponding sentences), and ranks the sentences according to their scores calculated by a set of predefined features, such as term frequency inverse sentence frequency (TF-ISF) (Radev et al., 2004; Lin and Hovy, 2002), sentence or term position (Yih et al., 2007), and number of keywords (Yih et al., 2007).
[]	Thus, we use MEAD (Radev et al., 2000) as our baseline. 
[]	In recent years, graph-based methods have been proposed for sentence ranking (Erkan and Radev, 2004; Mihalcea and Tarau, 2004). 
[1]	Many of these documents are likely to repeat much the same information, while differing in certain i Most of these were based on statistical techniques applied to various document entities; examples include frait, 1983; Kupiec et al. , 1995; Paice, 1990, Klavans and Shaw, 1995; MeKeown et al. , 1995; Shaw, 1995; Aon et al. , 1997; Boguraev and Kennedy, 1997; Hovy and Lin, 1997; Mitra et al. , 1997; Teufel and Moens, 1997; Barzilay and Elhadad, 1997; Carbonell and Goldstein, 1998; Baldwin and Mortbn, 1998; Radev and McKeown, 1998; Strzalkowski et al. , 1998).
[]	These n-gram key concepts are called topic signatures (Lin and Hovy 2000). 
[]	Erkan and Radev (2004) and Yoshioka (2004) evaluate the relevance (similarity) between any two sentences first. 
[]	For details see (Radev et al. , 2000). 
[1]	ROUGE evaluation: NIST also evaluated the summaries automatically using ROUGE (Lin, 2004; Lin and Hovy, 2003). 
[]	This is similar to Relative Utility (Radev et al. , 2003). 
[]	This is an extension of Lins method (Lin and Hovy, 2000). 
[1]	General text summarization, including single document summarization (Luhn, 1958; Goldstein et al., 1999) and multi-document summarization (Kraaij et al., 2001; Radev et al., 2003) has been well studied; our work is under the framework of extractive summarization (Luhn, 1958; McKeown and Radev, 1995; Goldstein et al., 1999; Kraaij et al., 2001), but our problem formulation differs from any existing formulation of the summarization problem.
[1]	Pyramid (Nenkova and Passonneau, 2004) is a manually evaluated measure of recall on facts or Semantic Content Units appearing in the reference summaries. 
[]	This result is presented as 0.053 with the official ROUGE scorer (Lin, 2004). 
[]	Introduction In the last decade, automatic text summarization has become a popular research topic with a curiously restricted scope of applications.A few innovative research directions have emerged, including headline generation (Soricut and Marcu 2007), summarization of books (Mihalcea and Ceylan 2007), personalized summarization (Daz and Gervas 2007), generation of tables-of-contents (Branavan, Deshpande, and Barzilay 2007), summarization of speech (Fuentes et al.2005), dialogues (Zechner 2002), evaluative text (Carenini, Ng, and Pauls 2006), and biomedical documents (Reeve, Han, and Brooks 2007).In addition, more researchers have been venturing past purely extractive summarization (Krahmer, Marsi, and van Pelt 2008; Nomoto 2007; McDonald 2006).By and large, however, most research in text summarization still revolves around texts characterized by rigid structure.The better explored among such texts are news articles (Barzilay and McKeown 2005), medical documents (Elhadad et al.2005), legal documents (Moens 2007), and papers in the area of computer science (Teufel and Moens 2002; Mei and Zhai 2008).Although summarizing these genres is a formidable challenge in itself, it excludes a continually increasing number of informal documents available electronically.Such documents, ranging from novels to personal Web pages, offer a wealth of information that merits the attention of the text summarization community. 
[1]	We used the ROUGE evaluation approach (Lin and Hovy, 2003), which is based on n-gram cooccurrence between machine summaries and ideal human summaries. 
[1]	TextRank (Mihalcea and Tarau, 2005) and LexPageRank (Erkan and Radev, 2004) use algorithms similar to PageRank and HITS to compute sentence importance. 
[]	We tested several measures, such as ROUGE (Lin, 2004) and the cosine distance. 
[]	Table 1: Sentence scoring metrics Name Description Source POS F Closeness to the beginning of the document: 1i (Edmundson, 1969) POS L Closeness to the end of the document: i (Baxendale, 1958) POS B Closeness to the borders of the document: max(1i, 1ni+1) (Lin and Hovy, 1997) LEN W Number of words in the sentence (Satoshi et al., 2001) LEN CH Number of characters in the sentence5 LUHN maxi{clusters(S)}{CSi}, CSi = W2iNi (Luhn, 1958) KEY Sum of the keywords frequencies: summation textt{Keywords(S)} tf(t) (Edmundson, 1969) COV Ratio of keywords number (Coverage): |Keywords(S)||Keywords(D)| (Liu et al., 2006a) TF Average term frequency for all sentence words: summationtext tS tf(t) N (Vanderwende et al., 2007) TFISF summationtexttS tf(t) isf(t), isf(t) = 1 log(n(t))log(n) , (Neto et al., 2000) n(t) is the number of sentences containing t SVD Length of a sentence vector in 2 V T after computing Singular Value (Steinberger and Jezek, 2004) Decomposition of a term by sentences matrix A = UV T TITLE O Overlap similarity6 to the title: sim(S,T) = |ST|min{|S|,|T|} (Edmundson, 1969) TITLE J Jaccard similarity to the title: sim(S,T) = |ST||ST| TITLE C Cosine similarity to the title: sim(vectorS, vectorT) = cos(vectorS, vectorT) = vectorSvectorT|vectorS||vectorT| D COV O Overlap similarity to the document complement new sim(S,D S) = |ST|min{|S|,|DS|} D COV J Jaccard similarity to the document complement sim(S,D S) = |ST||SDS| D COV C Cosine similarity to the document complement cos(vectorS, vectorD S) = vectorS vectorDS|vectorS|| vectorDS| LUHN DEG Graph-based extensions of LUHN, KEY and COV measures respectively. 
[1]	Some summarization systems assume that the importance of a sentence is derivable from a rhetorical representation of the source text (Marcu, 1997). 
[]	Evaluation We evaluated the quality of the headlines using ROUGE (Lin and Hovy, 2003). 
[]	The idea has been formalized in the construct of lexical chains (Barzilay and Elhadad, 1997). 
