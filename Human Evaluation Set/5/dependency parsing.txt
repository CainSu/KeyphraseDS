You should select [10] representative sentences! 

[1]	However, to make parsing tractable, these models are forced to restrict features over a very limited history of parsing decisions (McDonald and Pereira, 2006; McDonald and Nivre, 2007).
[]	In conditional log-linear models for dependency parsing (which are closely related to conditional random fields (Lafferty et al., 2001)), a distribution over dependency structures for a sentence x is defined as follows: p(y|x) = 1Z(x) exp{g(x,y)}, (1) where Z(x) is the partition function, w is a parameter vector, and g(x,y) = summation display (h,m,l)y wf(x,h,m,l) Here f(x,h,m,l) is a feature vector representing the dependency (h,m,l) in the context of the sentence x (see for example (McDonald et al., 2005a)).
[1]	Eisner (1996) gave a generative model with a cubic parsing algorithm based on an edge factorization of trees. 
[]	In which case, the parsing problem reduces to a18a37a36 a1a39a38a41a40a43a42a45a44a46a38a48a47 a49a22a50a41a51a53a52a55a54a57a56 a58 a52a60a59a62a61a5a63a64a59a66a65a43a56a67a50a26a49 sa2a5a4a29a19a22a21a23a4a25a24a28a16 (1) where the score sa2a5a4 a19 a21 a4 a24 a16 can depend on any measurable property of a4a20a19 and a4a25a24 within the tree a18 . This formulation is suf ciently general to capture most dependency parsing models, including probabilistic dependency models (Wang et al. , 2005; Eisner, 1996) as well as non-probabilistic models (McDonald et al. , 2005; Wang et al. , 2006).
[]	We conducted experiments with two data-driven parsers, MaltParser (Nivre et al., 2007b) and MSTParser (McDonald et al., 2006). 
[]	The constraints are chosen based on the two criteria: (1) adding them to the base constraints (those added in advance) would result in an extremely large program, and (2) it must be efcient to detect whether the constraint is violated in y. No Cycles (T2) For every possible cycle c for the sentence x we have a constraint which forbids the case where all edges in c are active simultaneously: summation display (i,j)c di,j |c|1 Comma Coordination (C3) For each symmetric conjunction token i which forms a symmetric coordination and each set of tokens A in x to the left of i with no comma between each pair of successive tokens we add: summation display aA di,a |A|1 which forbids con gurations where i has the argument tokens A. Compatible Coordination Arguments (C4) For each conjunction token i and each set of tokens A in x with incompatible POS tags, we add a constraint to forbid con gurations where i has the argument tokens A. summation display aA di,a |A|1 Selective Projective Parsing (P1) For each pair of triplets (i,j,l1) and (m,n,l2) we add the constraint: ei,j,l1 + em,n,l2 1 if l1 or l2 is in P. 3.2 Training For training we use single-best MIRA (McDonald et al. , 2005a). 
[]	Local models without global constraints are therefore mislead into deadend interpretations from which they cannot recover (McDonald and Nivre, 2007). 
[]	Thus, transition-based parsers normally run in linear or quadratic time, using greedy deterministic search or fixed-width beam search (Nivre et al., 2004; Attardi, 2006; Johansson and Nugues, 2007; Titov and Henderson, 2007), and graph-based models support exact inference in at most cubic time, which is efficient enough to make global discriminative training practically feasible (McDonald et al., 2005a; McDonald et al., 2005b).
[1]	We adopted the second order MST parsing algorithm as outlined by Eisner (1996). 
[]	This allows us to ef ciently use ILP for dependency parsing and add constraints which provide a signi cant improvement over the current stateof-the-art parser (McDonald et al. , 2005b) on the Dutch Alpino corpus (see bl row in Table 1). 
[1]	We can reduce the time complexity to O(n3q3) by strictly adopting the DP structures in the parsing algorithm of Eisner (1996). 
[1]	Nivre and McDonald (2008) and Zhang and Clark (2008) proposed stacking methods to combine graph-based parsers with transition-based parsers. 
[1]	The work of Eisner (1996) showed that the argmax problem for digraphs could be solved in O(n3) using a bottom up dynamic programming algorithmsimilartoCKY. 
[1]	Edge-factored models have many computational benefits, most notably that inference for nonprojective dependency graphs can be achieved in polynomial time (McDonald et al. , 2005b).
[]	Sen T/S Lem CPoS PoS MSF Dep NPT NPS Danish 94 5.2 18.2 no 10 24 47 52 1.0 15.6 Dutch 195 13.3 14.6 yes 13 302 81 26 5.4 36.4 German 700 39.2 17.8 no 52 52 0 46 2.3 27.8 Portuguese 207 9.1 22.8 yes 15 21 146 55 1.3 18.9 Slovene 29 1.5 18.7 yes 11 28 51 25 1.9 22.2 Table 1: Data sets; Tok = number of tokens (*1000); Sen = number of sentences (*1000); T/S = tokens per sentence (mean); Lem = lemmatization present; CPoS = number of coarse-grained part-of-speech tags; PoS = number of (fine-grained) part-of-speech tags; MSF = number of morpho syntactic features (split into atoms); Dep = number of dependency types; NPT = proportion of non-projective dependencies/tokens (%); NPS = proportion of non-projective dependency graphs/sentences (%) The history-based classifier can be trained with any of the available supervised methods for function approximation, butin the experiments below we will rely on SVM, which has previously shown good performance for this kind of task (Kudo and Matsumoto, 2002; Yamada and Matsumoto, 2003). 
[]	Nivre and McDonald (2008) explore a parser stacking approach in which the output of one parser is fed as an input to a different kind of parser. 
[1]	Nivre and Nilsson (2005) showed how the restriction to projective dependency graphs could be lifted by using graph transformation techniques to preprocess training data and post-process parser output, so-called pseudo-projective parsing.
[]	However, they make different types of errors, which can be seen as a reflection of their theoretical differences (McDonald and Nivre, 2007). 
[]	In the multilingual track of the CoNLL 2007 shared task on dependency parsing, a single parser must be trained to handle data from ten different languages: Arabic (Hajic et al. , 2004), Basque (Aduriz et al. , 2003), Catalan, (Mart et al. , 2007), Chinese (Chen et al. , 2003), Czech (Bohmova et al. , 2003), English (Marcus et al. , 1993; Johansson and Nugues, 2007), Greek (Prokopidis et al. , 2005), Hungarian (Csendes et al. , 2005), Italian (Montemagni et al. , 2003), and Turkish (Oflazer et al. , 2003).1 Our contribution is a study in multilingual parser optimization using the freely available MaltParser system, which performs 1For more information about the task and the data sets, see Nivre et al.
[]	The results reported here for English and Czech are comparable to the previous best published numbers in (McDonald et al. , 2005a), as Table 3 shows. 
[1]	This technique is similar to the parser voting methods used by Sagae and Lavie (2006). 
[]	The latest state-of-the-art statistical dependency parsers are discriminative, meaning that they are based on classifiers trained to score trees, given a sentence, either via factored whole-structure scores (McDonald et al. , 2005a) or local parsing decision scores (Hall et al. , 2006).
[1]	Deterministic dependency parsers which run in linear time have also been developed (Nivre & Scholz, 2004; Attardi, 2006). 
[]	Dependency treebanks are becoming available in many languages, and several approaches to dependency parsing on multiple languages have been evaluated in the CoNLL 2006 and 2007 shared tasks (Buchholz & Marsi, 2006; Nivre et al. , 2007). 
[]	Following standard practice for higher-order dependency parsing (McDonald and Pereira, 2006; Carreras, 2007), Models 1 and 2 evaluate not only the relevant third-order parts, but also the lower-order parts that are implicit in their third-order factorizations. 
[]	Another use of bottom-up is due to Eisner (1996), who introduced the notion of a span. 
[]	Ax = {(i,j,l) | i,j Vx and l L} Let D(Gx) represent the subgraphs of graph Gx that are valid dependency graphs for the sentence x. Since Gx contains all possible labeled arcs, the set D(Gx) must necessarily contain all valid dependency graphs for x. Assume that there exists a dependency arc scoring function, s : V V L R. Furthermore, define the score of a graph as the sum of its arc scores, s(G = (V,A)) = summation display (i,j,l)A s(i,j,l) The score of a dependency arc, s(i,j,l) represents the likelihood of creating a dependency from word wi to word wj with the label l. If the arc score function is known a priori, then the parsing problem can be stated as, 123 G = argmax GD(Gx) s(G) = argmax GD(Gx) summation display (i,j,l)A s(i,j,l) This problem is equivalent to finding the highest scoring directed spanning tree in the graph Gx originatingoutoftherootnode0, which can be solved for both the labeled and unlabeled case in O(n2) time (McDonald et al. , 2005b). 
[]	For example, the parser of McDonald and Pereira (2006) defines parts for sibling interactions, such as the trio plays, Elianti, and . in Figure 1. 
